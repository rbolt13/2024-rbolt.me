[
  {
    "objectID": "blog/Technical-Blog/posts/20240402-TidyTuesday-dubois/index.html",
    "href": "blog/Technical-Blog/posts/20240402-TidyTuesday-dubois/index.html",
    "title": "Week 14 Tidy Tuesday: Du Bois Visualization Challenge",
    "section": "",
    "text": "For this week’s TidyTuesday, I’ve recreated W.E.B. Du Bois’s ‘A Series of Statistical Charts, Illustrating the Conditions of Descendants of Former African Slaves Now Resident in the United States of America,’ utilizing R and the Tidyverse for a modern analysis.\n\nDu Bois Visualization Challenge 2024\n\n\n\n\n\n\n\n\n\n\n\nRandi’s Chart (Rendition)\n\n\nDu Bois’s Chart (Original)\n\n\n\nThe challenge this year pays homage to the profound data visualization legacy of W.E.B. Du Bois, inviting participants to reinterpret the groundbreaking visualizations he presented at the 1900 Paris Exposition with contemporary tools. Anchored by the vibrant colors of the Pan-African Flag, the challenge provides a unique opportunity to explore the intersection of history, culture, and data science.\nParticipants are encouraged to channel the essence of Plate 37 from Du Bois’s series, ‘A Series Of Statistical Charts Illustrating The Conditions Of Descendants Of Former African Slaves Now Resident In The United States,’ using the Pan-African Flag’s colors as a palette. This endeavor not only celebrates Du Bois’s legacy but also deepens our understanding of the historical and current conditions of African American communities.\n\nThe goal of the challenge is to celebrate the data visualization legacy of W.E.B Du Bois by recreating the visualizations from the 1900 Paris Exposition using modern tools.\n\nFor comprehensive details and to join the challenge, visit the GitHub repository!\n\n\nCode\nThis code follows a streamlined structure organized into four distinct phases: Set-Up, Clean, Graph, and Save.\n\nSet-UpCleanGraphSaveLinks\n\n\n\n\nShow Code\n#### Packages ####\n# tidyverse: A collection of data-related packages.\n# showtext: Use various fonts. \n# forcats: Add factors to categorical variables. \n# ggtext: Used markdown on graph text. \n# ggrepel: Overlapping labels on graph. \nbase::library(tidyverse)\nbase::library(showtext)\nbase::library(forcats)\nbase::library(ggtext)\nbase::library(ggrepel)\n\n#### Data ####\n# dubois: TidyTuesday data of occupation percentages\ndubois &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-04-02/dubois_week10.csv')\n\n#### Fonts ####\n# font_add_google(): Search Google Fonts. \n# showtext_auto(): Turn showtext on for graphics. \nsysfonts::font_add_google(\"Courier Prime\", \"font\")\nshowtext::showtext_auto()\n\n#### Colors ####\n# col1: Text \n# col2: Background\n# colors: Colors for Pie Chart\ncol1 &lt;- \"green\"\ncol2 &lt;- \"black\"\ncolors &lt;- c(\"#ce1d40\", \"#a59faa\", \"#e4bdb0\", \"#d1bea6\", \"#9b8f7d\", \"#ecb95b\")\n\n#### Text ####\n# title_text\n# subtitle_text\n# caption_text\ntitle_text &lt;- \"A SERIES OF STATISTICAL CHARTS. ILLUSTRA-&lt;br&gt;TING THE CONDITION OF THE DESCENDANTS OF FOR-&lt;br&gt;MER AFRICAN SLAVES NOW RESIDENT IN THE UNITED&lt;br&gt;STATES OF AMERICA.\"\nsubtitle_text &lt;- \"&lt;br&gt; &lt;span style='color:red;'&gt;UNE SÉRIE DE CARTES ET DIAGRAMMES STATISTIQUES MONTRANT LA&lt;br&gt;CONDITION PRÉSENTE DES DESCENDANTS DES ANCIENS ESCLAVES AFRI-&lt;br&gt;CANS ACTUELLMENT ÉTABLIS DANS LES ETATS UNIS D´ AMÉRIQUE.&lt;/span&gt; &lt;br&gt; &lt;br&gt; THE UNIVERSITY WAS FOUNDED IN 1867. &lt;br&gt; IT HAS INSTRUCED 6000 NEGROS STUDENTS. &lt;br&gt; &lt;br&gt; &lt;span style='color:red;'&gt;L´UNIVERSITÉ A ÉTÉ FONDÉE EN 1867. &lt;br&gt; ELLE A DONNÉ L´ INSTRUCTION A'6000 ÉTUDIANTS NEGRES.&lt;/span&gt;&lt;br&gt; &lt;br&gt; IT HAS GRADUATED 330 NEGROES AMOUNG WHOM ARE: &lt;br&gt; &lt;br&gt; &lt;span style='color:red;'&gt; ELLE A DÉLIVRÉ DES DIPLOMES A 330 NÉGRES DONT:&lt;/span&gt;\"\ncaption_text &lt;- \"Randi Bolt \\nApril 2024 \\n#TidyTuesday \\nDu Bois\"\n\n\nSet-Up\n\nPackages\nData\nFonts\nColors\nText\n\n\n\n\n\nShow Code\n#### Clean Data ####\n# Add Columns\n# 1. Calculate the cumulative sum of 'Percentage' column in reverse order.\n# 2. Calculate the position for labels. \n# 3. Replace NA values in 'pos' with half of the 'Percentage' value.\ndubois &lt;- dubois |&gt;\n  dplyr::mutate(\n    # Calculate the cumulative sum of 'Percentage' column in reverse order.\n    csum = base::rev(base::cumsum(base::rev(Percentage))),\n    # Calculate the position for labels.\n    pos = Percentage/2 + dplyr::lead(csum, 1),\n    # Replace NA values in 'pos' with half of the 'Percentage' value.\n    pos = dplyr::if_else(base::is.na(pos), Percentage/2, pos))\n\n\nClean \n\nCalculate the cumulative sum of ‘Percentage’ column in reverse order.\nCalculate the position for labels.\nReplace NA values in ‘pos’ with half of the ‘Percentage’ value.\n\n\n\n\n\nShow Code\n#### Pie Chart ###\n# Data: dubois (modified with position column)\n# Aesthetics: x = blank, y = Percentage, fill = fct_inorder(Occupation)\n# Geometry: width = 1\n# Polarize Coordinate: theta = y, start = 1.5, directional = 1 (clockwise)\n# Define fill color and order\n# Add labels outside of pie chart. \n# Legend title\n# Define labels\n# Remove Grid from Pie Chart\n# Theme: title, subtitle, caption, legend, background, plot margins\npie &lt;- ggplot2::ggplot(\n  # Data\n  dubois, \n  # Aesthetics\n  aes(\n    x = \"\" , \n    y = Percentage, \n    fill = fct_inorder(Occupation))\n  ) +\n  # Geometry\n  geom_col(\n    width = 1) +\n  # Polarize Coordinates\n  coord_polar(\n    theta = \"y\", \n    start = 1.5, \n    direction = 1) +\n  # Define fill color and order\n  scale_fill_manual(\n    values = colors,\n    breaks = c(\n      \"Teachers\", \"Ministers\", \"Government Service\", \n      \"Business\", \"Other Professions\", \"House Wives\"\n      )\n  ) +\n  # Add Labels Outside Pie Chart \n  ggrepel::geom_label_repel(\n    # Data\n    data = dubois,\n    # Aesthetics\n    aes(\n      y = pos, \n      label = paste0(Percentage, \"%\")\n      ),\n    size = 4.5, \n    nudge_x = 1, \n    show.legend = FALSE,\n    colour = \"black\"\n    ) +\n  # Legend Title\n  guides(\n    fill = guide_legend(title = \"\")\n    ) +\n  # Labels\n  labs(\n    title = title_text,\n    subtitle = subtitle_text,\n    caption = caption_text\n  ) +\n  # Remove Grid from Pie Chart\n  theme_void(\n  ) +\n  # Theme\n  theme(\n    # Title\n    plot.title = element_markdown(\n      size = 11, \n      hjust = 0.5,\n      lineheight = 0.9,\n      family = \"font\",\n      face = \"bold\",\n      color = col1),\n    plot.title.position = \"plot\",\n    # Subtitle\n    plot.subtitle = element_markdown(\n      size = 9, \n      hjust = 0.5,\n      lineheight = 0.9,\n      family = \"font\", \n      color = col1\n    ),\n    # Caption\n    plot.caption = element_text(\n      size = 6,\n      family = \"font\",\n      color = col1,\n      hjust = 1\n    ),\n    # Legend\n    legend.position = \"left\",\n    legend.title = element_blank(),\n    legend.text = element_text(\n      size = 8,\n      family = \"font\",\n      color = col1),\n    legend.key = element_blank(),\n    legend.background = element_blank(),\n    # Background\n    plot.background = element_rect(fill = col2, color = NA),\n    # Plot Margins\n    plot.margin = margin(t = 20, r = 20, b = 20, l = 20))\n\n\nGraph\n\nData: dubois (modified with position column)\nAesthetics: x = blank, y = Percentage, fill = fct_inorder(Occupation)\nGeometry: width = 1\nPolarize Coordinate: theta = y, start = 1.5, directional = 1 (clockwise)\nDefine fill color and order\nAdd labels outside of pie chart.\nLegend title\nDefine labels\nRemove Grid from Pie Chart\nTheme: title, subtitle, caption, legend, background, plot margins\n\n\n\n\n\nShow Code\n#### Save ####\nggplot2::ggsave(\n  \"plot.png\", \n  width = 5.25, \n  height = 6, \n  units = \"in\",\n  dpi = 100)\n\n\nSave plot as plot.png!\n\n\n\nR for Data Science’s #TidyTuesday Repo\nRandi Bolt’s #TidyTuesday Repo\n\n\n\n\n\nQuick Notes\n\nThe ggrepel() package includes the geom_label_repel() function, which is utilized to place chart labels outside of pie chart segments, ensuring that values are displayed clearly without overlapping the chart itself.\nThe pos variable, defined when cleaning, is calculated as a position for placing labels by first determining the midpoint of each item’s Percentage value and then adjusting this position based on the cumulative sum of percentages below it. This adjustment is made by adding half of an item’s own percentage to the cumulative sum of percentages of all items below it, shifted upwards by one position (dplyr::lead(csum, 1)), ensuring labels are centered appropriately on or near their respective segments. In cases where this calculation results in NA (specifically for the last item due to the lead function shifting values), pos is set to half of the item’s own percentage, ensuring every item has a defined position for its label.\nThe coord_polar() function customization options include:\n\nSpecifying the \\(\\theta\\) component based on either the x or y aesthetic.\nAdjusting the starting offset from the default 12 o’clock position, expressed in radians.\nSetting the direction parameter to 1 for clockwise orientation or -1 for counterclockwise orientation.\n\nThe plot.title.position() argument aligns the plot’s title centrally over the entire plot area, rather than just centering it above the pie chart, for improved layout balance and title visibility.\nDue to the integrated design of geom_label_repel() within the ggrepel package, altering the color of the connecting lines between the chart and the percentage values would concurrently change the text color of the percentage values. Opting to maintain these lines in black provided a visually appealing contrast, deemed superior to alternative colors such as white. This choice also streamlined the visualization process, circumventing the more time-consuming task of manually positioning percentage labels."
  },
  {
    "objectID": "blog/Technical-Blog/blog.html",
    "href": "blog/Technical-Blog/blog.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "Week 14 Tidy Tuesday: Du Bois Visualization Challenge\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13 Tidy Tuesday: NCAA Men’s March Madness\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nReady4R: Crosstables\n\n\n\n\n\n\nR\n\n\nReady4R\n\n\nExploratory Data Analysis\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nReady4R: skimr Package\n\n\n\n\n\n\nReady4R\n\n\nR\n\n\nExploratory Data Analysis\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 25 Tidy Tuesday: UFO Sightings Redux\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 24 Tidy Tuesday: SAFI Teaching Data\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Python Commands for Virtual Environments and Package Management\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 22 Tidy Tuesday: Centenarians Data\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGun Law Scorecard (Article Review)\n\n\n\n\n\n\nArticle Review\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 17 Tidy Tuesday: London Marathon\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 Tidy Tuesday: Bob Ross Paintings\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nFlashcards\n\n\n\n\n\n\nStatistics\n\n\nMachine Learning\n\n\nMath\n\n\n\n\n\n\n\n\n\nJan 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR and SQLite\n\n\n\n\n\n\nR\n\n\nSQL\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNBA Salaries - Part 1: Web-Scraping\n\n\n\n\n\n\nR\n\n\nWeb-Scraping\n\n\nNBA\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Links\n\n\n\n\n\n\nQuarto\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMidterm Prep. Group Theory\n\n\n\n\n\n\nMath\n\n\n\n\n\n\n\n\n\nMar 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMidterm Prep. Modern College Geometry\n\n\n\n\n\n\nGeometry\n\n\n\n\n\n\n\n\n\nMar 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio: Building a Blog with R\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLatex\n\n\n\n\n\n\nMath\n\n\n\n\n\n\n\n\n\nJan 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nProve H and I-H are Idempotent\n\n\n\n\n\n\nProof\n\n\nStatistics\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\nNov 29, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "Empowering businesses through actionable intelligence and data innovation.\nAs a dedicated data analyst with a relentless passion for exploring diverse topics and a commitment to continual improvement, I bring a dynamic data-driven approach to a diverse set of industries in the technical space. Explore my hands-on projects to witness firsthand my expertise, delve into insightful content on my various blogs, or connect with me through any of the provided links. I eagerly await the opportunity to engage with you. 🌹"
  },
  {
    "objectID": "blog/Book-Reviews/blog.html",
    "href": "blog/Book-Reviews/blog.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "Book Review: The Only Good Indians\n\n\n\n\n\n\n10/10\n\n\nHorror\n\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBook Review: How to Win Friends & Influence People\n\n\n\n\n\n\n03/10\n\n\nSelf-help\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBook Review: Carmageddon\n\n\n\n\n\n\n09/10\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Personal-Blog/blog.html",
    "href": "blog/Personal-Blog/blog.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "2024 January Winter Storm\n\n\n\n\n\n\nStorm Preparedness\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240301-BookReview-Carmageddon/2024_03_Book_Review_Carmageddon.html",
    "href": "blog/Book-Reviews/posts/20240301-BookReview-Carmageddon/2024_03_Book_Review_Carmageddon.html",
    "title": "Book Review: Carmageddon",
    "section": "",
    "text": "Rating: 9/10  Overview: This thought-provoking book critically examines the pervasive influence of automobiles on contemporary society. Through detailed analysis and persuasive arguments, it showcases a variety of examples to illustrate the detrimental effects of car culture on our daily lives, challenging the supremacy of the automobile in modern civilization.\n\n\nIntroduction\nIn his compelling and thought-provoking book, Carmageddon: How Cars Make Life Worse and What to Do About It, Daniel Knowles invites readers to reconsider the pervasive dominance of automobiles in modern society. Through sharp analysis and persuasive arguments, Knowles explores the multifaceted ways in which car culture adversely affects our daily lives. Below, I share some of the insights and reflections that resonated with me long after turning the last page.\n\n\nCivil Rights and Public Transportation\n\nThe automotive industry’s exponential growth coincided with the era when people of color finally gained the right to sit anywhere on public transportation. This pivotal moment in civil rights history marks a complex intersection with the rise of car culture.\nIn the United States, the expansion of roads and highways was strategically planned in ways that often had deleterious effects on certain communities. These construction projects were intentionally routed to displace specific groups or sever their access to vital city resources, exacerbating social and economic divides.\nDisplacement due to these infrastructure projects frequently forced communities into areas lacking in essential public transportation services. This not only marginalized these groups further but also entrenched car dependency as a primary mode of mobility, despite its broader societal impacts.\nThe Albina Vision Trust in Portland is an exemplary organization actively working to address and rectify transportation policies, among other institutional decisions, that have historically led to the displacement of hundreds of families. Their efforts highlight a crucial movement towards reconciling past injustices with forward-thinking urban and transportation planning.\n\n\n\nCars and Classism\n\nCars since their origination have been a sign of the upper class. Even today you are seen as “less” if you do not own a car.\nA lot of jobs require people to have a drivers license or “valid form of transportation”, which means driving. Even though there are plenty of valid forms of transportation that are not driving.\nOn Class, Capitalism and Urban Planning in Who Framed Roger Rabbit , delves into an intriguing analysis of the film, which is based on the true story of the dismantling of trolley systems across the United States. It offers a compelling look at how the movie intertwines the narratives of class, capitalism, and urban planning, highlighting the profound impact of the automobile industry’s rise on public transit and, by extension, society at large.\nIn Kenneth Grahame’s classic novel, The Wind in the Willows, the character of Mr. Toad epitomizes the reckless and inconsiderate motorist, reflecting critiques of upper-class driving behaviors. For an intriguing dive into the real-world inspiration behind this memorable character, explore the article: Meet the Real-Life Inspiration Behind “Mr.Toad”.\n\n\n\nParking\n\nSociety’s preference for free parking over affordable housing is starkly evident in the increasing number of individuals living out of RVs and cars. This phenomenon highlights a troubling reality: in many areas, it’s more financially feasible to secure a parking spot for a vehicle than to afford living accommodations.\nUrban planning regulations often mandate a disproportionate allocation of parking spaces relative to the square footage of buildings, irrespective of the practical need for such extensive parking. This is particularly illogical for certain types of establishments, such as bars, where the excessive requirement for parking does not align with the nature of the business.\nThe prioritization of free parking spaces over the availability of public bathrooms underscores a misalignment in urban development priorities, where the convenience of car owners is often placed above basic public amenities.\nIdealized representations of car-centric cities frequently omit the extensive infrastructure devoted to parking. These depictions fail to acknowledge the visual, spatial, and environmental impact of parking lots and garages, presenting an unrealistically clean and uncluttered urban landscape.\nThe mandated quantity of parking in urban areas contributes to sprawling city layouts, with destinations spaced too far apart for practical walking. This sprawling effect contradicts one of the fundamental advantages of city living: the ability to navigate the urban environment on foot. By encouraging car dependency, current parking requirements undermine the walkability and, ultimately, the very essence of city life.\n\n\n\nTraffic\n\nTraffic congestion is inherently linked to the prevalence of cars. The solution to reducing traffic lies not in expanding road networks but in decreasing car usage.\nA shift towards public transportation, coupled with investments in its infrastructure, could significantly alleviate traffic congestion. Prioritizing transit development over the construction of more roads offers a sustainable path to reducing urban traffic.\nThe average driver spends a considerable amount of time stuck in traffic, a reality conspicuously absent from car advertisements. These commercials often portray an idealized driving experience, sidestepping the common frustration of traffic jams that many drivers face daily.\n\n\n\nThe Lethal Impact of Some Vehicles\n\nCars are usually designed to keep you safe in crashes. They have special features like engines that drop down and fronts that crumple to absorb crash impacts, protecting the people inside. But, there’s a big problem with large, lifted trucks and vans. Their height means they often skip right over those safety features in smaller cars. Instead of the front crumpling and absorbing the crash, these big vehicles can crash directly into where passengers sit, causing much worse injuries, and fatalities.\nCar manufacturers are aware that these big trucks and vans are more dangerous in accidents. Still, they keep selling them because they’re profitable, not just the vehicles themselves but also all the extra accessories they can sell.\nChoosing one of these big vehicles might feel safer because they’re so sturdy and high off the ground. But the truth is, they turn accidents into much more dangerous situations.\n\n\n\nCharity Car Clubs: A Strategic Image Shift\n\nAware of their controversial impact, car manufacturers have sought to align themselves with the emblematic “American Dream.” To cultivate a positive public image, they’ve initiated charity car clubs. These initiatives gather car owners to engage in charitable activities, all under the auspices of the car company’s brand.\nThis strategy is designed to forge a strong association between car ownership and membership in a commendable community. It intentionally crafts the narrative that owning a car from a particular brand elevates one’s social standing, embedding the vehicle not just as a means of transport but as a symbol of belonging to a virtuous circle.\n\n\n\nVehicles in Developing Nations\n\nThe United States, along with other developed nations, enforces rigorous environmental and safety standards for vehicles. Consequently, cars that no longer comply with these stringent regulations in their home countries are often exported to developing nations.\nThe implementation of import regulations on vehicles by developing countries can inadvertently widen the socioeconomic divide. Such policies may restrict access to affordable transportation for lower-income segments, exacerbating the gap between different social classes.\n\n\n\nCar Corporations and Public Health\n\nThe Volkswagen emissions scandal revealed the company cheated on emissions tests, significantly impacting environmental and public health standards.\nStudies indicate lead from gasoline significantly impaired the IQ of nearly half the U.S. population. The elimination of leaded gasoline has been a critical step in removing a major public health threat, marking a pivotal victory against environmental pollution caused by automotive fuels.\n\n\n\nLimitations of Electric Cars\n\nIn cities where the electrical grid relies on coal for power, the environmental benefits of electric cars are negated. The reliance on fossil fuels for electricity generation means that even zero-emission vehicles indirectly contribute to pollution.\nThe production of electric car batteries involves extensive mining operations due to their large size compared to smartphone batteries. This increased demand for raw materials significantly exacerbates the environmental impact associated with their extraction and processing.\nInvestments in infrastructure projects like the Las Vegas Convention Center Loop a one-way, one-lane tunnel that permits only a single car at a time, represents a questionable allocation of resources. This system pales in comparison to traditional subway systems, which offer higher capacity, efficiency, and speed at lower costs.\n\n\n\nGender Bias in Transporation\n\nPublic transportation systems often overlook the needs of women, despite their higher usage rates compared to men.\nMothers, in particular, face challenges when navigating buses or trains with children, highlighting a lack of family-friendly amenities.\nThe design and routing of many public transit options cater predominantly to the conventional nine-to-five work commute, neglecting the complex daily routines of individuals like stay-at-home moms who juggle school drop-offs, grocery shopping, and other errands.\nHistorically, crash test dummies were based on male body types, leading to vehicle safety features that are more optimized for men than for women.\nChild safety in vehicles largely falls to parents, as standard car seat designs cater to adults, not children, necessitating additional child-specific safety equipment.\n\n\n\nWalkable Cities\n\nWalkable cities emphasize pedestrian access, fostering healthier lifestyles by minimizing dependence on cars.\nShifting towards these urban designs offers notable environmental advantages, such as diminished air pollution and reduced greenhouse gas emissions.\nNotable examples include quaint college towns and places like Disney World’s Epcot, designed for easy pedestrian navigation across areas. Unlike the sprawling urban developments of today, these examples showcase the practicality and appeal of compact, walkable communities, underscoring the need for a return to more human-scale urban planning.\n\n\n\nConlusion\nIn “Carmageddon: How Cars Make Life Worse and What to Do About It,” Daniel Knowles navigates the intricate ways in which automobile dominance shapes modern society, spotlighting its profound impact on everything from civil rights and classism to urban planning and public health."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230123-Original-RandSQLite/index.html",
    "href": "blog/Technical-Blog/posts/20230123-Original-RandSQLite/index.html",
    "title": "R and SQLite",
    "section": "",
    "text": "Using a R package called SQLite, this post demonstrates how to answer data related questions with both R and SQL (Structured Query Language).\n\n\n\nNaruto with SQL logo, and Sauske with the R Studio logo, standing back to back.\n\n\n\n0. Introduction\nThere are a handful of programming languages that data scientists use when querying, analyzing, and manipulating data. What I have found is that while R and Python have a lot more power in what they are capable of producing, SQL is used by a wider variety of roles to access and query data. So to get more practice using both SQL and R I pulled 10 questions and some data off a website called Learn SQL, and will be answering the following questions in both languages.\n\n0.0 Set-Up0.1 Packages0.2 Data0.3 Database\n\n\nContents:\n\n0.1 Packages\n0.2 Data\n0.3 Data base\n\n\n\nFor all my data queries and manipulation in R I will be using base R, dplyr, and magrittr.\n\nlibrary(dplyr)\nlibrary(magrittr)\n\nTo create a SQL database, and run SQL queries in R chunks I will be using a package called RSQLite.\n\n# install.packages(\"RSQLite\")\nlibrary(RSQLite)\n\n\n\nThis post will use three data sets that I copied from Learn SQL:\n\npatients: Which includes patient_id, first_name, last_name, gender, birth_date, city, province_id, allergies, height, and weight. Note I only copied the first 1000 entries.\n\n\npatients &lt;- utils::read.csv('data/patients.csv')\n\n\nprovince_names: Which includes province_id, and province_name.\n\n\nprovince_names &lt;- utils::read.csv(\"data/province_names.csv\")\n\n\nadmissions: which includes patient_id, admission_date, discharge_date, diagnosis, attending_doctor_id\n\n\nadmissions &lt;- utils::read.csv(\"data/admissions.csv\")\n\n\n\nTo create a database use:\n\ndbConnect() to connect to a SQL data base called Hospital.db in the 00_data folder.\nSQLite() to connect to a SQLite database file.\n\n\nhosp &lt;- RSQLite::dbConnect(RSQLite::SQLite(),\n                           \"data/Hospital.db\")\n\nTo define data within the database use:\n\ndbWriteTable() to create a data set within the hospital database first call the data base (hosp), define a name, and then define the data.\n\n\nRSQLite::dbWriteTable(hosp,\n                      \"patients\",\n                      patients)\nRSQLite::dbWriteTable(hosp,\n                      \"province_names\",\n                      province_names)\nRSQLite::dbWriteTable(hosp,\n                      \"admissions\",\n                      admissions)\n\nVerify the three data sets are in the database using:\n\ndbListTables() to list the tables within the hosp database.\n\n\nRSQLite::dbListTables(hosp)\n\n[1] \"admissions\"     \"patients\"       \"province_names\"\n\n\n\n\n\n\n\n1. Show the first ten rows of patients data.\n\n1.01.1 R1.2 SQL\n\n\nContents\n\n1.1 Solution in R\n1.2 Solution in SQL\n\n\n\nIn R use:\n\nhead() to view the first 10 rows of the patients data.\n\n\nutils::head(patients, 10)\n\n   patient_id first_name  last_name gender birth_date            city\n1           1     Donald Waterfield      M 1963-02-12          Barrie\n2           2     Mickey     Baasha      M 1981-05-28          Dundas\n3           3       Jiji     Sharma      M 1957-09-05        Hamilton\n4           4      Blair       Diaz      M 1967-01-07        Hamilton\n5           5    Charles      Wolfe      M 2017-11-19         Orillia\n6           6        Sue     Falcon      F 2017-09-30            Ajax\n7           7     Thomas     ONeill      M 1993-01-31      Burlington\n8           8      Sonny    Beckett      M 1952-12-11 Port Hawkesbury\n9           9     Sister    Spitzer      F 1966-10-15         Toronto\n10         10     Cedric   Coltrane      M 1961-11-10         Toronto\n   province_id  allergies height weight\n1           ON       NULL    156     65\n2           ON      Sulfa    185     76\n3           ON Penicillin    194    106\n4           ON       NULL    191    104\n5           ON Penicillin     47     10\n6           ON Penicillin     43      5\n7           ON       NULL    180    117\n8           NS       NULL    174    105\n9           ON Penicillin    173     95\n10          ON       NULL    157     61\n\n\n\n\nIn R use:\n\ndbGetQuery() to run SQL commands from a given data base.\n\nIn SQL use:\n\nSELECT to select.\n* to include all columns.\nFROM to define the patients data for select to include all columns from.\nLIMIT to only show the top ten rows.\n\n\nRSQLite::dbGetQuery(hosp, \n                    \"SELECT * \n                     FROM patients \n                     LIMIT 10\"\n                    )\n\n   patient_id first_name  last_name gender birth_date            city\n1           1     Donald Waterfield      M 1963-02-12          Barrie\n2           2     Mickey     Baasha      M 1981-05-28          Dundas\n3           3       Jiji     Sharma      M 1957-09-05        Hamilton\n4           4      Blair       Diaz      M 1967-01-07        Hamilton\n5           5    Charles      Wolfe      M 2017-11-19         Orillia\n6           6        Sue     Falcon      F 2017-09-30            Ajax\n7           7     Thomas     ONeill      M 1993-01-31      Burlington\n8           8      Sonny    Beckett      M 1952-12-11 Port Hawkesbury\n9           9     Sister    Spitzer      F 1966-10-15         Toronto\n10         10     Cedric   Coltrane      M 1961-11-10         Toronto\n   province_id  allergies height weight\n1           ON       NULL    156     65\n2           ON      Sulfa    185     76\n3           ON Penicillin    194    106\n4           ON       NULL    191    104\n5           ON Penicillin     47     10\n6           ON Penicillin     43      5\n7           ON       NULL    180    117\n8           NS       NULL    174    105\n9           ON Penicillin    173     95\n10          ON       NULL    157     61\n\n\n\n\n\n\n\n2. Show total patients admitted.\n\n2.02.1 R2.2 SQL\n\n\nContents\n\n2.1 Solution in R\n2.2 Solution in SQL\n\n\n\nIn R use:\n\ndata.frame() to define a column for total_admissions.\nnrow() to count the rows in the patients data which will equal the total_admissions.\n\n\nbase::data.frame(\"total_admissions\" = base::nrow(patients))\n\n  total_admissions\n1             1000\n\n\n\n\nIn SQL use:\n\nSELECT to select.\nCOUNT(*) to count the total number of rows.\nAS to define that count as a new variable, total_admissions.\nFROM to define the patients data for select to count the total numbers of rows for, and define as total_admissions.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT COUNT(*) AS total_admissions \n                     FROM patients\"\n                    )\n\n  total_admissions\n1             1000\n\n\n\n\n\n\n\n3. Show first and last name as full_name.\n\n3.03.1 R3.2 SQL\n\n\nContents\n\n3.1 Solution in R\n3.2 Solution in SQL\n\n\n\nIn R use:\n\nhead() to show the first 10 rows of data.\ndata.frame() to define a data frame that includes full_name.\npaste0() to paste together the first_name, a space, and the last_name. This will equal the full_name.\n\n\nutils::head(\n  base::data.frame(\n    full_name = base::paste0(patients$first_name, \n                             \" \", \n                             patients$last_name)), \n  10)\n\n           full_name\n1  Donald Waterfield\n2      Mickey Baasha\n3        Jiji Sharma\n4         Blair Diaz\n5      Charles Wolfe\n6         Sue Falcon\n7      Thomas ONeill\n8      Sonny Beckett\n9     Sister Spitzer\n10   Cedric Coltrane\n\n\n\n\nIn SQL use:\n\nSELECT to select.\n|| to concatenate first_name, space, and last name.\nAS to define the concatination as full_name.\nFROM to define the patients data for select to concatenate data from.\nLIMIT to show the first 10 rows of data.\n\n\nRSQLite::dbGetQuery(hosp,\n  \"SELECT first_name || ' ' || last_name AS full_name \n   FROM patients \n   LIMIT 10\"\n   )\n\n           full_name\n1  Donald Waterfield\n2      Mickey Baasha\n3        Jiji Sharma\n4         Blair Diaz\n5      Charles Wolfe\n6         Sue Falcon\n7      Thomas ONeill\n8      Sonny Beckett\n9     Sister Spitzer\n10   Cedric Coltrane\n\n\n\n\n\n\n\n4. Show unique cities that are in province_id ‘NS’?\n\n4.04.1 R4.2 SQL\n\n\nContents\n\n4.1 Solution in R\n4.2 Solution in SQL\n\n\n\nIn R define the patients data then use:\n\n%&gt;% to pipe data.\nfilter() to filter for all province_id that is equal to “NS”.\nsummerise() to define unique_cites.\nunique() to remove duplicate elements of the city column which will be defined as unique_cites.\n\n\npatients %&gt;% \n  filter(province_id == \"NS\") %&gt;%\n  summarise(unique_cities = unique(city)) \n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n    unique_cities\n1 Port Hawkesbury\n2         Halifax\n\n\n\n\nIn SQL:\n\nSELECT to select.\nDISTINCT() to define city as the column to remove duplicates from.\nAS to define those cities as unique_cites.\nFROM to define the patients data for select to get unique_cites from.\nWHERE specifies a condition.\nIS is the condition that ‘NS’ is equal to province_id.\n\n\ndbGetQuery(hosp,\n           \"SELECT DISTINCT(city) AS unique_cities \n            FROM patients \n            WHERE province_id IS 'NS'\"\n           )\n\n    unique_cities\n1 Port Hawkesbury\n2         Halifax\n\n\n\n\n\n\n\n5. Show the total number of male patients and the total number of female patients.\nDisplay the two results in the same row.\n\n5.05.1 R5.2 SQL\n\n\nContents\n\n5.1 Solution in R\n5.2 Solution in SQL\n\n\n\nIn R use:\n\ndata.frame() to create a data frame with two columns: male_count, and female_count.\nlength() to count the length of the input.\nwhich() to indicate which gender equals “M” or “F”, and counts that length accordingly.\n\n\nbase::data.frame(\n  male_count = base::length(base::which(patients$gender == 'M')),\n  female_count = base::length(base::which(patients$gender == 'F'))\n  )\n\n  male_count female_count\n1        543          457\n\n\n\n\nIn SQL use:\n\nSELECT to select.\nCOUNT(*) to count all input values.\nFROM to define the patients data for select to count on.\nWHERE is a condition defined as gender = “M” or “F”.\nAS is defining the count as male_count, or female_count respectively.\n\n\ndbGetQuery(hosp,\n           \"SELECT \n           (SELECT COUNT(*) FROM patients WHERE gender = 'M') AS male_count, \n           (SELECT COUNT(*) FROM patients WHERE gender = 'F') AS female_count\")\n\n  male_count female_count\n1        543          457\n\n\n\n\n\n\n\n6. Show all allergies ordered by popularity. Remove NULL values from the query.\n\n6.06.1 R6.2 SQL\n\n\nContents\n\n6.1 Solution in R\n6.2 Solution in SQL\n\n\n\nIn R define the patients data then use:\n\n%&gt;% pipe the data.\nfilter() to subset data to all allergies that aren’t “NULL”.\ngroup_by() to convert the table into one that is grouped by allergies.\nsummarise() to define total_diagnosis.\nn() to count the size of each group.\narrange() to define how the data is arranged.\ndesc() to define that the data is arranged in descending order by total_diagnosis.\n\n\npatients %&gt;% \n  dplyr::filter(allergies != \"NULL\") %&gt;%\n  dplyr::group_by(allergies) %&gt;%\n  dplyr::summarise(total_diagnosis = dplyr::n()) %&gt;%\n  dplyr::arrange(dplyr::desc(total_diagnosis)) %&gt;%\n  utils::head(10)\n\n# A tibble: 10 × 2\n   allergies   total_diagnosis\n   &lt;chr&gt;                 &lt;int&gt;\n 1 Penicillin              230\n 2 Codeine                  58\n 3 Sulfa                    35\n 4 ASA                      16\n 5 Sulfa Drugs              13\n 6 Tylenol                  11\n 7 Wheat                    11\n 8 Peanuts                  10\n 9 Bee Stings                9\n10 Iodine                    9\n\n\n\n\nIn SQL:\n\nSELECT the allergies column.\nCOUNT(*) count all input.\nAS to define the count as total_diagnosis.\nFROM to define the patients data to select the allergies column from.\nWHERE to define a condition.\nIS NOT is the condition that says allergies cannot be ‘NULL’.\nGROUP BY groups the data by allergies.\nOrder BY to define how the data order is output.\nDESC is the given condition that the data is ordered in descending order by total_diagnosis.\nLIMIT to limit output to the first 10 rows.\n\n\ndbGetQuery(hosp,\n           \"SELECT allergies,\n            COUNT(*) AS total_diagnosis\n            FROM patients\n            WHERE allergies IS NOT 'NULL'\n            GROUP BY allergies\n            ORDER BY total_diagnosis DESC\n            LIMIT 10\"\n           )\n\n     allergies total_diagnosis\n1   Penicillin             230\n2      Codeine              58\n3        Sulfa              35\n4          ASA              16\n5  Sulfa Drugs              13\n6        Wheat              11\n7      Tylenol              11\n8      Peanuts              10\n9       Iodine               9\n10  Bee Stings               9\n\n\n\n\n\n\n\n7. Display the total number of patients for each province. Order by descending.\n\n7.07.1 R7.1 SQL\n\n\nContents\n\n7.1 Solution in R\n7.2 Solution in SQL\n\n\n\nIn R use:\n\nmerge() to join the data sets province_names, and patients by “province_id”.\n%&gt;% to pipe the data.\ngroup_by() to group by province name.\nsummarise() to define the patient count.\nn() to count the number of patients in each province. This will equal patient_count.\narrange() to define how the data is arranged.\ndesc() to define that the data is arranged in descending order by patient_count.\n\n\nbase::merge(province_names, patients, by = \"province_id\") %&gt;%\n  dplyr::group_by(province_name) %&gt;%\n  dplyr::summarise(patient_count = dplyr::n()) %&gt;%\n  dplyr::arrange(dplyr::desc(patient_count))\n\n# A tibble: 8 × 2\n  province_name             patient_count\n  &lt;chr&gt;                             &lt;int&gt;\n1 Ontario                             954\n2 Alberta                              14\n3 British Columbia                     11\n4 Nova Scotia                           9\n5 Manitoba                              7\n6 Newfoundland and Labrador             2\n7 Quebec                                2\n8 Saskatchewan                          1\n\n\n\n\nIn SQL use:\n\nSELECT select the province_name column.\nCOUNT(*) to count all input.\nAS to define count as patient_count.\nFROM to define the patients data AS pa to select province_name to count the number of patients from.\nJOIN to join the province_names data AS pr.\nON is the clause to join data based on pr.province_id = pa.province_id.\nGROUP BY to group the data by pr.province_id.\nORDER BY to define the order of the data output.\nDESC defines that the output data be arranged in order of descending patient_count.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT province_name,\n                     COUNT(*) AS patient_count\n                     FROM patients pa\n                     JOIN province_names pr ON pr.province_id = pa.province_id\n                     GROUP BY pr.province_id\n                     ORDER BY patient_count DESC\")\n\n              province_name patient_count\n1                   Ontario           954\n2                   Alberta            14\n3          British Columbia            11\n4               Nova Scotia             9\n5                  Manitoba             7\n6                    Quebec             2\n7 Newfoundland and Labrador             2\n8              Saskatchewan             1\n\n\n\n\n\n\n\n8. Show the provinces that have more patients identified as ‘M’ than ‘F’. Must only show full province_name.\n\n8.08.1 R8.2 SQL\n\n\nContents\n\n8.1 Solution in R\n8.2 Solution in SQL\n\n\n\nIn R use,\n\nmerge() to join the data sets province_names, and patients by “province_id”.\ngroup_by() to group by province name.\ncount() to count the number of “M” and “F” patients for each province.\nslice() to remove certain rows based on a given criteria.\nwhich.max() to determine which province has a greater number of male patients.\nsummarise() to define only province_name in the output.\n\n\nbase::merge(province_names, patients, by = \"province_id\") %&gt;%\n  dplyr::group_by(province_name) %&gt;%\n  dplyr::count(gender == \"M\", gender == \"F\") %&gt;%\n  dplyr::slice(base::which.max(n)) %&gt;%\n  dplyr::summarise(province_name = province_name)\n\n# A tibble: 8 × 1\n  province_name            \n  &lt;chr&gt;                    \n1 Alberta                  \n2 British Columbia         \n3 Manitoba                 \n4 Newfoundland and Labrador\n5 Nova Scotia              \n6 Ontario                  \n7 Quebec                   \n8 Saskatchewan             \n\n\n\n\nIn SQL use,\n\nSELECT to select pr.province_name column.\nFROM to define the patients data to select pr.province_name from.\nAS to define patients data as pa.\nJOIN to join pa.province data with pr.province_name data.\nAS to define province_name data as pr.\nON to join pa.patients and pr.province_names data by province_id.\nGROUP BY to group data by province_id.\nHAVING to define a clause to filter the data where the number of “M” patients is greater than “F” patients.\nCOUNT() to count the given input.\nCASE to go through the condition of gender = “M” or when gender = “F”\nWHEN to preform the count when the condition is true.\nTHEN 1 END to add a 1 to the count when the case is met.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT pr.province_name\n                     FROM patients AS pa\n                     JOIN province_names AS pr ON pa.province_id = pr.province_id\n                     GROUP BY pr.province_id\n                     HAVING\n                     COUNT( CASE WHEN gender = 'M' THEN 1 END) &gt;\n                     COUNT( CASE WHEN gender = 'F' THEN 1 END)\")\n\n              province_name\n1                   Alberta\n2          British Columbia\n3                  Manitoba\n4 Newfoundland and Labrador\n5               Nova Scotia\n6                   Ontario\n7                    Quebec\n8              Saskatchewan\n\n\n\n\n\n\n\n9. Each admission costs $50 for patients without insurance, and $10 for patients with insurance. All patients with an even patient_id have insurance.\nGive each patient a ‘Yes’ if they have insurance, and a ‘No’ if they don’t have insurance. Add up the admission_total cost for each has_insurance group.\n\n9.09.1 R9.2 SQL\n\n\nContents\n\n9.1 Solution in R\n9.2 Solution in SQL\n\n\n\nIn R use,\n\n%&gt;% to pipe the data.\nmutate() to mutate the data to include has_insurance, and cost_after_insurance information.\ncase_when() to define a case where if the patient id is odd then they don’t have insurance, and if they are even they do have insurance.\ngroup_by() to group the data by has_insurance.\nsummarize() to define cost_after_insurance.\nsum() to add up the cost for all those with and without insurance.\n\n\npatients %&gt;%\n  dplyr::mutate(\n    has_insurance = dplyr::case_when(\n    patient_id %%2==1 ~ \"Yes\",\n    patient_id %%2!=1 ~ \"No\"\n  ),cost_after_insurance = dplyr::case_when(\n    has_insurance == \"Yes\" ~ 10,\n    has_insurance == \"No\" ~ 50\n  )) %&gt;%\n  group_by(has_insurance)  %&gt;%\n  summarise(cost_after_insurance = base::sum(cost_after_insurance))\n\n# A tibble: 2 × 2\n  has_insurance cost_after_insurance\n  &lt;chr&gt;                        &lt;dbl&gt;\n1 No                           25000\n2 Yes                           5000\n\n\n\n\nIn SQL use,\n\nSELECT to select.\nCASE to define a case.\nWHEN to select when a patient_id is even.\nTHEN to set insurance to “Yes” if patient_id is even.\nELSE to set has_insurance to “No” when patient_id is not even.\nEND to end the case.\nAS to define the first case values as has_insurance.\nSUM() to add up input.\nCASE to define another case.\nWHEN to select when a patient has an even id.\nTHEN to set cost_after_insurance to 10 if the patient id is even.\nELSE to set the cost_after_insurance to 50 if the patient id is not even.\nEND to end the case.\nAS to define the second case values as cost_after_insurance.\nFROM to define the patients data to select and figure out insurance costs for.\nGROUP BY to group data by has_insurance.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT \n                        CASE WHEN patient_id % 2 = 0 Then 'Yes'\n                        ELSE 'No' \n                        END as has_insurance,\n                     SUM(\n                        CASE WHEN patient_id % 2 = 0 Then 10\n                        ELSE 50 \n                        END\n                         ) AS cost_after_insurance\n                     FROM patients \n                     GROUP BY has_insurance;\")\n\n  has_insurance cost_after_insurance\n1            No                25000\n2           Yes                 5000\n\n\n\n\n\n\n\n10. We are looking for a specific patient. Pull all columns for the patient who matches the following criteria:\n\nFirst_name contains a ‘b’ after the first two letters.\nIdentifies their gender as ‘F’\nTheir weight would be between 50 kg and 70 kg\nTheir patient_id is an odd number\nThey are from the city ‘Burlington’\n\n\n10.010.1 R10.2 SQL\n\n\nContents\n\n10.1 Solution in R\n10.2 Solution in SQL\n\n\n\nIn R define the patients data then use,\n\n%&gt;% to pipe the data.\nfilter to filter the database on first name, gender, weight, patient_id, and city.\ngrepl to select first_names where the 3rd letter is b.\n\n\npatients %&gt;%\n  dplyr::filter(base::grepl(\"^.{2}[b]\", first_name),\n         gender == \"F\",\n         weight &gt; 50 & weight &lt; 80,\n         patient_id %%2==1,\n         city == \"Burlington\")\n\n  patient_id first_name last_name gender birth_date       city province_id\n1        695    Sabrina    Hettie      F 2000-11-25 Burlington          ON\n   allergies height weight\n1 Penicillin    160     51\n\n\n\n\nIn SQL use:\n\nSELECT to select all * columns.\nFROM to select all columns from the patients data.\nWhere defines multiple conditions.\nLIKE is a condition where first_name has the third letter equal to a lower case b.\nAND to define multiple conditions such as gender, patient_id, and city.\nBETWEEN to define weight is greater than 50, but less than 70.\n\n\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT *\n                     FROM patients\n                     WHERE\n                      first_name LIKE '__b%'\n                      AND gender = 'F'\n                      AND weight BETWEEN 50 AND 70\n                      AND patient_id % 2 = 1\n                      AND city = 'Burlington';\")\n\n  patient_id first_name last_name gender birth_date       city province_id\n1        695    Sabrina    Hettie      F 2000-11-25 Burlington          ON\n   allergies height weight\n1 Penicillin    160     51"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230620-TidyTuesday-UFOSightingsRedux/index.html",
    "href": "blog/Technical-Blog/posts/20230620-TidyTuesday-UFOSightingsRedux/index.html",
    "title": "Week 25 Tidy Tuesday: UFO Sightings Redux",
    "section": "",
    "text": "Explore the mysterious realm of unidentified flying objects as I dive into my Week 25 #TidyTuesday submission. In this otherworldly data visual I uncover intriguing patterns about sightings lasting less than 5 minutes in Oregon, all classified according to their reported shapes.\n\nUFO Sightings Redux\nThe data this week comes from the National UFO Reporting Center, cleaned and enriched with data from sunrise-sunset.org by Jon Harmon.\nIf this dataset looks familiar, that’s because we used a version of it back in 2019. The new version adds the last several years of data, adds information about time-of-day, and cleans up some errors in the original dataset. We’d love to see visualizations describing the differences between the 2019 dataset and this new dataset!\n\nThe National UFO Reporting Center was founded in 1974 by noted UFO investigator Robert J. Gribble. The Center’s primary function over the past five decades has been to receive, record, and to the greatest degree possible, corroborate and document reports from individuals who have been witness to unusual, possibly UFO-related events. Throughout its history, the Center has processed over 170,000 reports, and has distributed its information to thousands of individuals.\n\n\n\n\nCode\nThis weeks code retains its core components of cleaning, visualizing, and styling, while the index.R file handles data loading, file execution, and the generation of a data visualization.\n\nCleaningVisualizeStyleindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\n# magrittr: %&gt;% pipe function. \n# dplyr: data cleaning functions.\n# tidyr: data manipulation functions.\nbase::library(dplyr)\nbase::library(magrittr)\nbase::library(tidyr)\n\n#### Cleaning Function ####\nclean &lt;- function(df){\n  # Filter date for Oregon\n  or_df &lt;- df %&gt;%\n    dplyr::filter(state == c(\"OR\")) %&gt;%\n    dplyr::reframe(\n      shape = shape, \n      duration_seconds = duration_seconds,\n      duration_minutes = duration_seconds/60)\n  # remove nas \n  or_df$duration_seconds &lt;- stats::na.omit(or_df$duration_seconds)\n  # Subset for all data 5 minutes or less\n  clean_df &lt;- \n    base::subset(or_df, \n                 duration_seconds &lt; 301) %&gt;%\n    tidyr::replace_na(list(shape = c(\"not available\")))\n    \n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(ggplot2)\n\n#### Visual Function ####\nvis &lt;- function(clean_df){\n  vis &lt;- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(x = shape,\n                 y = duration_minutes,\n                 fill = shape,\n                 color = shape)) +\n    ggplot2::geom_boxplot(outlier.shape = NA) +\n    ggplot2::geom_jitter(\n      na.rm = TRUE, \n      color = \"#d2ff46\", \n      alpha = 0.7)\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# ggplot2: graphing functions.\n# showtext: font functions. \nbase::library(\"ggplot2\")\nbase::library(\"showtext\")\n\n#### Load Fonts ####\nsysfonts::font_add_google(\"Orbitron\", \"font\")\nshowtext::showtext_auto()\n\n#### Load Colors ####\ncol1 &lt;- \"#d2ff46\"\ncol2 &lt;- \"#0d0d0d\"\ncol3 &lt;- \"white\"\n\n#### Axis Labels ####\naxis_labs &lt;- c(\"changing\", \"orb\", \"formation\", \"oval\", \"unknown\",\n               \"cigar\", \"rectangle\", \"circle\", \"egg\", \"sphere\", \n               \"other\", \"light\", \"triangle\", \"teardrop\", \"cross\",\n               \"diamond\", \"cylinder\", \"disk\", \"fireball\", \"not available\", \n               \"flash\", \"chevron\", \"cone\")\n\n#### Style Function ####\nsty &lt;- function(vis){\n  sty &lt;- vis +\n    # labs\n    ggplot2::labs(\n      title = \"Shapes in Oregon: Short Sightings\",\n      subtitle = \"\\nThe following box plot displays the distribution of UFO sightings lasting less than 5 minutes in Oregon, categorized by \\ntheir reported shape. Among this subset of sightings, Cylinder, Disk, Fireball, Not Available, Flash, Chevron, and Cone \\nwere more frequently reported with durations of less than two minutes. In contrast, the Changing UFO shape was \\nobserved for a broader duration range, spanning from 2 to 5 minutes. \\n\\nThe asterisk (*) denotes the mean duration in minutes.\\n \",\n      caption = \"Randi Bolt - June 2023 \\n#TidyTuesday: UFO Sightings Redux\",\n      x = \"Shape\",\n      y = \"Duration in Minutes\")  +\n    # x and y axis\n    ggplot2::scale_y_continuous(limits = c(0,5)) + \n    ggplot2::scale_x_discrete(limits = axis_labs) +\n    # variable color and fill colors\n    ggplot2::scale_color_viridis_d() +\n    ggplot2::scale_fill_viridis_d(alpha = 0.6) +\n    # mean point\n    ggplot2::stat_summary(\n      fun = mean, \n      geom = \"point\", \n      shape = 8, \n      size = 4) +\n    # theme\n    ggplot2::theme(\n      # labs \n      plot.title = element_text(\n        size = 30,\n        family = \"font\",\n        face = \"bold\",\n        hjust = 0,\n        color = col1),\n      plot.subtitle = element_text(\n        size = 18,\n        family = \"font\",\n        hjust = 0,\n        color = col1),\n      plot.caption = element_text(\n        size = 12,\n        family = \"font\",\n        hjust = 1,\n        color = col1),\n      axis.title = element_text(\n        size = 24, \n        family = \"font\",\n        color = col1),\n      axis.text = element_text(\n        size = 18, \n        family = \"font\",\n        color = col1),\n      axis.text.x = element_text(\n        angle = 55,\n        vjust = .7), \n      plot.background = element_rect(fill = col2),\n      panel.background = element_rect(fill = col2),\n      panel.grid.major.x = element_blank(),\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      legend.position = \"none\") \n  return(sty)\n}\n\n\n\n\n\n\nShow Code\n#### Week ####\nweek &lt;- c(\"25\")\n\n#### Load Packages ####\n# tidyverse: A collection of data-related packages.\nbase::library(tidyverse)\n\n#### Load Data ####\n# tt_data: ufo_sightings\ntt_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-20/ufo_sightings.csv')\n\n#### Clean Data ####\nclean_data &lt;- clean(tt_data)\n\n#### Create Data Visual ####\nvis_data &lt;- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis &lt;- sty(vis_data)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\",\n  dpi = 150)\n\n\n\n\nWeek 25 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nI utilized tidyr::replace_na() in the cleaning function to replace missing values (NA) with the string ‘not available’.\nIn the visualizing function, I opted to use ggplot2::geom_jitter() instead of ggplot2::geom_point() due to the nature of the data. Since the data points represent estimates of sighting durations, and they are mostly in minutes rather than specific seconds, using geom_point() would result in straight lines of dots across the box plots. Therefore, I decided to apply jittering using geom_jitter() to add some randomness to the point positions, which I deemed acceptable for visualizing the estimated sighting durations.\nIn the styling function, I utilized ggplot2::scale_x_discrete(limits = axis_labs) to define the order of the x-axis labels. Note that if you misspell a word, the data will not appear on your graph.\nAs the styling function grows longer, I find myself considering the idea of splitting it up. One approach I might explore is separating the labels into their own distinct function. This way, I can assess whether this change significantly improves the readability of the styling function.\nThis visual raises some concerns, and I acknowledge the need to allocate more time for statistical analysis in the future. The following issues can be identified with this visual.\n\n\nLack of statistical significance in the arrangement of UFO shapes along the x-axis: The order was chosen based on aesthetic and readability considerations, rather than any statistical criteria.\nAbsence of statistical analysis for outlier elimination: Since the data was limited to sightings lasting 5 minutes or less, no statistical analysis was conducted to identify and eliminate outliers."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230613-TidyTuesday-SAFITeachingData/index.html",
    "href": "blog/Technical-Blog/posts/20230613-TidyTuesday-SAFITeachingData/index.html",
    "title": "Week 24 Tidy Tuesday: SAFI Teaching Data",
    "section": "",
    "text": "Explore a rich data set focused on understanding the role of farmer-led irrigation in Africa with my week 24 #TidyTuesday submission. In this post my visual delves into the relationship between livestock ownership and the number of rooms in a household.\n\nSAFI Data\nThe data this week comes from the SAFI (Studying African Farmer-Led Irrigation) survey, a subset of the data used in the Data Carpentry Social Sciences workshop. So, if you’re looking how to learn how to work with this data, lessons are already available! Data is available through Figshare.\nCITATION: Woodhouse, Philip; Veldwisch, Gert Jan; Brockington, Daniel; Komakech, Hans C.; Manjichi, Angela; Venot, Jean-Philippe (2018): SAFI Survey Results. doi:10.6084/m9.figshare.6262019.v1\n\nSAFI (Studying African Farmer-Led Irrigation) is a currently running project which is looking at farming and irrigation methods. This is survey data relating to households and agriculture in Tanzania and Mozambique. The survey data was collected through interviews conducted between November 2016 and June 2017 using forms downloaded to Android Smartphones. The survey forms were created using the ODK (Open Data Kit) software via an Excel spreadsheet. The collected data is then sent back to a central server. The server can be used to download the collected data in both JSON and CSV formats. This is a teaching version of the collected data that we will be using. It is not the full dataset.\n\n\nThe survey covered such things as; household features (e.g. construction materials used, number of household members), agricultural practices (e.g. water usage), assets (e.g. number and types of livestock) and details about the household members.\n\n\nThe basic teaching dataset used in these lessons is a subset of the JSON dataset that has been converted into CSV format.\n\n\n\n\nCode\nThis week in the index.R file I tried using the tidytuesdayR package and the variable week &lt;- c(24) in attempt to streamline updating the index file for future weeks. This project retains its core components of cleaning, visualizing, and styling, while the index.R file handles data loading, file execution, and the generation of a data visualization.\n\nCleaningVisualizeStyleindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(dplyr)\nbase::library(magrittr)\n\n#### Cleaning Function ####\nclean &lt;- function(df){\n  # extract data from list\n  extracted_df &lt;- df[[1]]\n  clean_df &lt;- extracted_df %&gt;% \n    dplyr::count(rooms, liv_count)\n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(ggplot2)\n\n#### Visual Function ####\nvis &lt;- function(clean_df){\n  vis &lt;- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(x = rooms, y = liv_count, fill = n)) +\n    geom_tile()\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(\"ggplot2\")\nbase::library(\"showtext\")\n\n#### Load Fonts ####\nsysfonts::font_add_google(\"Judson\", \"font\")\nshowtext::showtext_auto()\n\n#### Load Colors ####\ncol1 &lt;- \"white\"\ncol2 &lt;- \"#3E3D53\"\ncol3 &lt;- \"#2C6E63\"\ncol4 &lt;- \"#FCE100\"\n\n#### Style Function ####\nsty &lt;- function(vis){\n  sty &lt;- vis +\n    # labs\n    ggplot2::labs(\n      title = \"Relationship Between Live Stock Ownership and Number of Rooms in Household\",\n      subtitle = \"This subset of the SAFI (Studying African Farmer-Led Irrigation) data shows that of the 131 people surveyed most lived in a house with 1 room,\\n and had 1 cattle. There is no obvious evidence to indicate that the number of rooms in a home is related to the number of live stock owned.\",\n      caption = \"Randi Bolt - June 2023 \\n#TidyTuesday: SAFI Data - June 2017\",\n      x = \"Rooms\",\n      y = \"Live Stock\",\n      fill = \"Surveyed\")  +\n    # add numbers in boxes\n    ggplot2::geom_text(\n      ggplot2::aes(label = n),\n      color = col1,\n      size = 12,\n      family = \"font\") + \n    # scale color\n    ggplot2::scale_fill_gradient(low = col3, high = col4) +\n    # Axis Breaks \n    ggplot2::scale_x_continuous(\n      breaks = seq(1,8,1)) + \n    ggplot2::scale_y_continuous(\n      breaks = seq(1,5,1)) + \n # theme\n    ggplot2::theme(\n      plot.title = element_text(\n        size = 30,\n        family = \"font\",\n        face = \"bold\",\n        hjust = 0,\n        color = col1),\n      plot.subtitle = element_text(\n        size = 18,\n        family = \"font\",\n        hjust = 0,\n        color = col1),\n      plot.caption = element_text(\n        size = 12,\n        family = \"font\",\n        hjust = 1,\n        color = col1),\n      axis.title = element_text(\n        size = 24, \n        family = \"font\",\n        color = col1),\n      axis.text = element_text(\n        size = 18, \n        family = \"font\",\n        color = col1),\n      legend.title = element_text(\n        size = 24,\n        family = \"font\",\n        color = col1),\n      legend.text = element_text(\n        size = 16,\n        family = \"font\",\n        color = col1),\n      plot.background = element_rect(fill = col2),\n      panel.background = element_rect(fill = col2),\n      panel.grid.major.x = element_blank(),\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      legend.background = element_rect(fill = col2))\n  return(sty)\n}\n\n\n\n\n\n\nShow Code\nweek &lt;- c(\"24\")\n#### Load Packages ####\nbase::library(tidyverse)\nbase::library(tidytuesdayR)\n\n#### Load Data ####\ntt_data &lt;- tidytuesdayR::tt_load(2023, week = base::as.integer(week))\n\n\n\n    Downloading file 1 of 1: `safi_data.csv`\n\n\nShow Code\n#### Clean Data ####\nclean_data &lt;- clean(tt_data)\n\n#### Create Data Visual ####\nvis_data &lt;- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis &lt;- sty(vis_data)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\",\n  dpi = 150)\n\n\n\n\nWeek 24 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nI used ggplot2::geom_tile() to create the heat map graph.\nI used ggplot2::scale_fill_gradient() to assign a “high” color and “low” color, but it might make the graph easier to read if I picked 5 colors instead.\nI would have liked white horizontal lines on this graph, but was only getting the vertical lines to show. Will need to do more research on this in future weeks.\nWhen I use the showtext package to assign fonts to the text on my graphs and go to save them as a .png file, if I don’t update the dpi = value (dots per inch) then I have huge spaces in between lines of text. I found that the larger the dpi value the more space there is between lines of text. 150 dpi seems to be a good dpi for this graph.\nI am still debating using the TidyTuesdayR package. It is nice to have the link to the .csv file avaialble in the code."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html",
    "title": "Midterm Prep. Group Theory",
    "section": "",
    "text": "Notes consist of Sets, Subsets, Operations, Group, Abelian Group, Subgroups, and Homework 1 and 2 questions."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#examples",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#examples",
    "title": "Midterm Prep. Group Theory",
    "section": "Examples",
    "text": "Examples\n\n\\(\\mathbb{N}=\\{1,2,3,...\\}\\) : they exist naturally\n\\(\\mathbb{Z}=\\{...,-3,-2,-1,0,1,2,3,...\\}\\) : includes zero and negatives\n\\(\\mathbb{Q}=\\{\\frac{m}{n}|m,n\\in \\mathbb{Z}\\text{ and }n\\ne 0\\}\\) : integer fractions\n\\(\\mathbb{R}\\) : includes square roots and pie, real analysis starts with \\(\\sqrt{2}\\)\n\\(\\mathbb{C}\\) : includes imaginary numbers\nAsterisk in the superscript means delete zero\nPlus sign in the superscript means only positive values (&gt;0)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#properties",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#properties",
    "title": "Midterm Prep. Group Theory",
    "section": "Properties",
    "text": "Properties\n\n* is commutative if \\(a\\ne b\\), \\(a*b=b*a\\) \\(\\forall\\) a,b \\(\\in A\\).\n\n+ and \\(\\cdot\\) are commutative\n- , \\(\\div\\) , funtion composition and matrix multiplication are not commutative\n\n* is associative if \\((a*b)*c=a*(b*c)\\) \\(\\forall\\) a,b,c, \\(\\in A\\).\n\naddition is associative, subtraction is not associative\n\nIf \\(\\exists\\) \\(e\\in A\\) \\(\\Rightarrow\\) \\(e*a=a*e=a\\) \\(\\forall\\) \\(a\\in A\\), the we call e the identity element in A w.r.t. *.\n\n0=e w.r.t. addition\n1=e w.r.t. multiplication\n\nIf \\(e\\in A\\) is the identity w.r.t. * and \\(a,b\\in A\\Rightarrow\\) \\(a*b=b*a=e\\) we call a and b inverses of one another.\n\nthe inverse of \\(a\\in \\mathbb{R}\\) w.r.t. addition is -a since \\(a+(-a)=(-a)+a=0\\)\nthe inverse of \\(a\\in\\mathbb{R}^*\\) w.r.t. multiplication is \\(\\frac{1}{a}\\) since \\(a(\\frac{1}{a})=(\\frac{1}{a})a=1\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#proof-outlines",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#proof-outlines",
    "title": "Midterm Prep. Group Theory",
    "section": "Proof Outlines",
    "text": "Proof Outlines\n\nCommutative:\n\\(\\underline{\\text{No}}\\): Give an example, “Let a = 1, and b =2”, and then show \\(a*b\\ne b*a\\).\n\\(\\underline{\\text{Yes}}\\): “For any a,b in the set” and then show \\(a*b=b*a\\).\nAssociative\n\\(\\underline{\\text{No}}\\): Give an example, “Let a=1, b=2, c=3” and then show \\(a*(b*c)\\ne (a*b)*c\\).\n\\(\\underline{\\text{Yes}}\\): “For any a,b,c in the set” and then show \\(a*(b*c)=(a*b)*c\\).\nIdentity\n\\(\\underline{\\text{No}}\\): Suppose that \\(e\\in\\) the given set \\(\\Rightarrow\\) \\(a*e=a\\) \\(\\forall a\\in\\) the given set. Then show \\(a*e=a\\) by plugging e in for b and solving for e. “Since the identity element must be a constant then there is no identiy w.r.t.” the given set. (can’t involve variables)\n\\(\\underline{\\text{Yes}}\\): State what the identity element is w.r.t. the orperation and show that \\(a*e=a\\) and \\(e*a=a\\).\nInverses\n\\(\\underline{\\text{No}}\\): Given an example of an element who doesn’t have an inverse.\nNote: If there is not identity element then there is no inverse.\n\\(\\underline{\\text{Yes}}\\): “Suppose b=\\(a^{-1}\\). Then \\(a*b=e\\)” (where e is the identity found in 3), and then try to solve the eqation for b. Then check \\(b*a=e\\) as well.\nNote: Do not need to check \\(b*a=e\\) if we know * is commutative."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#proposition-1",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#proposition-1",
    "title": "Midterm Prep. Group Theory",
    "section": "Proposition 1",
    "text": "Proposition 1\nLet G be a group, then G has exactly one identity element."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#proposition-2",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#proposition-2",
    "title": "Midterm Prep. Group Theory",
    "section": "Proposition 2",
    "text": "Proposition 2\nEvery element of G has exactly one inverse."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#theorem-1-cancellation-law",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#theorem-1-cancellation-law",
    "title": "Midterm Prep. Group Theory",
    "section": "Theorem 1 (Cancellation Law)",
    "text": "Theorem 1 (Cancellation Law)\nLet G be a group and let \\(a,b,c\\in G\\), then \\(ab=ac\\Rightarrow b=c\\) and \\(ba=ca\\Rightarrow b=c\\)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#theorem-2",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#theorem-2",
    "title": "Midterm Prep. Group Theory",
    "section": "Theorem 2",
    "text": "Theorem 2\nLet G be a group and let \\(a,b\\in G\\). If \\(ab=e\\), then a and b are inverses, i.e. \\(a=b^{-1}\\) and \\(b=a^{-1}\\)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#theorem-3",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#theorem-3",
    "title": "Midterm Prep. Group Theory",
    "section": "Theorem 3",
    "text": "Theorem 3\nLet G be a group and let \\(a,b\\in G\\) then \\((ab)^{-1}=b^{-1}a^{-1}\\) and \\((a^{-1})^{-1}=a\\).\n\nto show a and b are inverses, show their product is e."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#klein-4-group",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#klein-4-group",
    "title": "Midterm Prep. Group Theory",
    "section": "Klein 4 Group",
    "text": "Klein 4 Group\n(for fintie groups) : \\(a^2=b^2=c^2=e\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#example",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#example",
    "title": "Midterm Prep. Group Theory",
    "section": "Example",
    "text": "Example\nAddition:\n\n\\(0\\in H\\).\n\\(\\forall\\) a,b \\(\\in H\\) \\(a+b=H\\).\n\\(\\forall\\) \\(a\\in H\\) , \\(-a\\in H\\)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#proof-outlines-two-step-subgroup-test",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#proof-outlines-two-step-subgroup-test",
    "title": "Midterm Prep. Group Theory",
    "section": "Proof Outlines (Two-Step Subgroup Test)",
    "text": "Proof Outlines (Two-Step Subgroup Test)\n\n\\(e\\in H\\)\nfor all a,b \\(\\in H\\), \\(ab^{-1}\\in H\\).\n\nProve something is a subgroup of G.\n\n“Suppose” then show e is 0 or 1 for the subgroup in a short series of equalities, “the additive or multiplicative element 0 or 1 is in” the subgroup.\n“Now take any a,b \\(\\in\\) the subgroup.” Then define a and b potentially for some other integers. Then show \\(ab^{-1}=\\) something identifiable in the subgroup.\n\n“Therefore we’ve shown that” our subgroup “contains the identity element and for all a,b \\(\\in H\\) , \\(ab^{-1}\\in\\)” our subgroup. Thus our subgroup is a subgroup of G."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#example-1",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#example-1",
    "title": "Midterm Prep. Group Theory",
    "section": "Example",
    "text": "Example\nAddition:\n\n\\(0\\in H\\).\nfor all \\(a,b\\in H\\), \\(a+(-b)=a-b\\in H\\)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#added-notes",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#added-notes",
    "title": "Midterm Prep. Group Theory",
    "section": "Added Notes",
    "text": "Added Notes\n\nIf G is abelian, then \\((ab)^n=a^nb^n\\) for all \\(n\\in \\mathbb{N}\\).\nIn any group \\((a^{-1})^n=(a^n)^{-1}\\) for all \\(n\\in \\mathbb{N}\\).\nIn any group \\(e^{-1}=e\\)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#two-step-subgroup-test",
    "href": "blog/Technical-Blog/posts/20220328-MidtermPrepGroupTheory/index.html#two-step-subgroup-test",
    "title": "Midterm Prep. Group Theory",
    "section": "Two-step subgroup test",
    "text": "Two-step subgroup test\nLet G be a group. A subset H \\(\\subseteq G\\) is a subgroup of G if\n\n\\(e\\in H\\)\n\n\naddition: show 0 \\(\\in H\\)\n\n\n\\(\\forall a,b,\\in H\\), \\(ab^{-1}\\in H\\).\n\n\naddition: \\(\\forall\\) a,b \\(\\in H\\), \\(a+(-b)=a-b\\in H\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "",
    "text": "In this post, I’ll be sharing my review of the data visuals and analysis presented in the article Gun Law Scorecard."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#introduction",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#introduction",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "Introduction",
    "text": "Introduction\nGun Law Scorecard grades each U.S. state on gun safety, highlighting the correlation between gun regulation and lower gun-related deaths. The article features 9 interactive data visuals that tell the story of gun laws and deaths in the country. In this review, I will describe each data visual’s features and provide three bullet points summarizing my thoughts."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#grading-the-states-map",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#grading-the-states-map",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "1. Grading the States (Map)",
    "text": "1. Grading the States (Map)\n\n\n\n\n\nThe data visualization is a choropleth map that displays the relationship between a state’s gun law strength, gun death rank, gun death rate, and grade for gun safety. Each state is represented with a color-coded gradient that corresponds to its value for each of these factors.\n\n\n\n\n\n\nThe title of each section in this article corresponds to the content depicted in the visual presented within in.\nThe visual explanation consists of four sentences. The first two describe the grading system used in the article.The third presents the thesis that “strong gun laws save lives”. The last sentence explains how the visual works.\nThe interactive features of the visual include: (1) a drop-down menu located beside the “GRADE” title that allows users to switch between “GUN LAW STRENGTH,” “GUN DEATH RANK,” and “GUN DEATH RATE”; (2) the ability to hover over each state with a mouse, revealing its grade, strength, or rank; and (3) pop-up windows that provide a more detailed analysis of each state."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#grading-the-states-table",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#grading-the-states-table",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "2. Grading the States (Table)",
    "text": "2. Grading the States (Table)\n\n\n\n\n\nA table showing Grade, State, Gun Law Strength, Gun Death Rank, and Fun Death Rate.\n\n\n\n\n\n\nGun Law Strength (Ranked) is the left most column. This follows our intuition to have a ranked row as the leftmost column of the table. It is also the more optimistic choice between Law Strength and Death Rate.\nPop-out darkens the rest of the screen, and gives state specific information of what has changed this year and how each state can improve.\n\n\n\n\nThe table shows all 50 states (no option to show less), so it is helpful to be able to switch back to the map to keep scrolling."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#gun-laws-vs.-gun-deaths",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#gun-laws-vs.-gun-deaths",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "3. Gun Laws Vs. Gun Deaths",
    "text": "3. Gun Laws Vs. Gun Deaths\n\n\n\n\n\nA linear regression dot plot comparing Gun Deaths Per 100,000 to Gun Law Strength.\n\n\n\n\n\n\nTitle of the plot tells the viewer the conclusion of the the visual which is, As Gun Laws Weaken, Gun Deaths Rise.\nI’m curious about some of the outlier states such as New Mexico, and New Hampshire.\nIt would be interesting to include more data and run a more in depth linear analysis."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#compare-states",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#compare-states",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "4. Compare States",
    "text": "4. Compare States\n\n\n\n\n\nComparison Table to compare two states on Grade, Gun Law Strength Rank, Gun Death Rate Rank, Fun Deaths Per 100k, and % Difference from National Average.\n\n\n\n\n\n\nStarting comparison is the top ranking state California, and the bottom ranking state Arkansas.\nThe interactive table above was helpful in finding other comparisons such as Massachusetts and Mississippi.\nThis visual and the table are the only two with the option to share to twitter or facebook on the footer of the visual."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#federal-process",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#federal-process",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "5. Federal Process",
    "text": "5. Federal Process\n\n\n\n\n\nTimeline of Federal Progress from 1934 to 2022.\n\n\n\n\n\n\nBold text in left tells what the visual is about which is that in the past 100 years there has only been 6 national gun laws passed. The idea of more national regulation is a theme throughout this article.\nBoth the word “Progress” and the years are written in red. Showing that progress happened these years.\nSimple, effective, and informative. By clicking on the different hears the reader can learn a little bit about the law. There is also more information about each law when clicking on the “Learn More” button under the text on the left."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#best-worst",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#best-worst",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "6. Best & Worst",
    "text": "6. Best & Worst\n\n\n\n\n\nBest and Worst for Strongest Laws, Most Improved, Safest State, Weakest Laws, Biggest Drop, and Deadliest State.\n\n\n\n\n\n\nGood visual to compare the good and the bad. Each has two interactive buttons. One to show a states entire score card, and the other to input that state into the comparison table.\nMetrics for plaques were Strongest/Weakest Laws, Most Improved/ Biggest Drop, and Safest/Deadliest State.\nEach plaque includes an image of a state, but the image (unlike the other buttons below the line) does not pop the user up to the interactive map. Meaning it is purely decorative. Something to consider."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#state-progress-heat-map-chart",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#state-progress-heat-map-chart",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "7. State Progress (Heat Map Chart)",
    "text": "7. State Progress (Heat Map Chart)\n\n\n\n\n\nHeat Map Chart of New Gun Safety Laws in 2022.\n\n\n\n\n\n\nInteresting choice of visual, but helpful in giving a broad understanding of what the state laws related to guns look like.\nWhen hovering over the category name the rectangle changes from blue to red, and a dark blue box pops up to tell the reader the exact number of laws.\nAnother visual I may have considered using for this is a horizontal bar chart. This visual style would make it easier for the reader to notice that both “Access to Guns” and “Other Gun Safety Laws” have 14 new laws in 2022. Similarly the “Domestic Violence” and “Gun Dealer Regulations” both have 6 new gun laws in 2022."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#state-progress-dot-plot",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#state-progress-dot-plot",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "8. State Progress (Dot Plot)",
    "text": "8. State Progress (Dot Plot)\n\n\n\n\n\nDot Plot of New Gun Safety Laws from 2013 to 2022.\n\n\n\n\n\n\nThis visual works well with the one above it, because after reading about the new laws in 2022 one would be curious what the progression over time has been.\nInteractivity is fun, but I would have preferred the amount actively visible above the plot since there are only 10 points.\nI am curious if there exists data that shows which state gun laws may have been created because of a mass shooting event."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#hate-crimes-guns-bar-graph",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#hate-crimes-guns-bar-graph",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "9. Hate Crimes & Guns (Bar Graph)",
    "text": "9. Hate Crimes & Guns (Bar Graph)\n\n\n\n\n\nBar Graph of Hate Crimes Reported to the FBI from 2012 to 2021.\n\n\n\n\n\n\nImportant visual to add that is related to the main thesis of the article.\nSince the hate crimes seem to be on the rise, it may be interesting to view the number of hate crimes by state, or see if there is any relevence to included that data into the scorecard.\nIt may be interesting to see this same visual, but broken down by hate crime, or comparing other categories of gun violence such as suicide, domestic violence, and police violence."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#final-thoughts",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#final-thoughts",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere are some of my main take away from this analysis:\n\nA consistent color scheme of Red, White, and Blue was present across the article, and data visuals. Red is more prominently used with guns, gun deaths, worst states, and as a highlight, accent, or call to action color. The blue seems more neutral or positive. Something to consider when deciding on color palletes for visuals.\nA “new” metric was created (the scorecard) to tell a consistent story through multiple data visuals.\nInteractive visuals are a great way to engage the reader to investigate their own state, and familiarize themselves with other states.\nOther data that might be interesting to view, gun sales, suicide by gun, mass shootings, state regulations, and gun related hate crimes."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221121-Original-QuartroLinks/index.html",
    "href": "blog/Technical-Blog/posts/20221121-Original-QuartroLinks/index.html",
    "title": "Quarto Links",
    "section": "",
    "text": "Here is a list of links I used to help create this Quarto blog. As well as links for web accessibility, examples of other quarto blogs, and examples of non-quarto blogs.\n\n\nQuarto LinksAccessibilityQuarto BlogsOther Blogs\n\n\nA Quarto tip a day\nCreating a blog with Quarto in 10 steps - Bea Milz\nCreate your Data Science portfolio with Quarto: Slides\nFrom R Markdown to Quarto\nHow to style your Quarto blog without knowing a lot of HTML/CSS\nInstalling and Configuring Python with RStudio\nManaging Execution\nQuarto: Creating a Blog (Documentation)\nQuarto/RMarkdown - What’s Different: Slides Repo\nquarto-trio (Using Python, R, and Apache Arrow)\n\n\na11Y - digital accessibility\nWAVE - Web Accessibility Evaluation Tool\n\n\nAbhirup Moitra\nAlbert Rapp\nBea Milz: Repo\nEmy Tamika\nDaniel Tran: Repo\nDanielle Navarro - Notes from a data witch: Repo\nDeesha menghani: Repo\nJesse Adler\nMeghan Harris: Repo\nRober Mitchell: Repo\nSam Csik: Repo\nTed Landeras, PhD: repo\n\n\nAlicia Johnson (Made with blogdown))\nAllison Hill, PhD\nCrystal Lewis\nGreg Wilson\nDavid Neuzerling: Repo\nJenny Bryan (Made with Blogdown and Hugo): Repo\nJon Harmon\nJulia Silge (Made with Blogdown and Hugo): Repo\nMachine Learning Mastery!\nNan Xiao: Repo\nNicola Rennie: Repo\nTanner Heffner (Made with Svelte): Repo\nYanina Bellini Saibene\nYihui Xie"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230221-TidyTuesday-BobRoss/index.html",
    "href": "blog/Technical-Blog/posts/20230221-TidyTuesday-BobRoss/index.html",
    "title": "Week 8 Tidy Tuesday: Bob Ross Paintings",
    "section": "",
    "text": "Step into the world of happy little data with my Week 8 #TidyTuesday submission which reveals the number of colors Bob Ross used each season of “The Joy of Painting”.\n\nBob Ross Paintings\nThe data this week comes from Jared Wilber’s data on Bob Ross Paintings via @frankiethull Bob Ross Colors data package.\n\n\n\nCode\nThis week I wanted to keep the visual fairly simple so that I could focus my efforts on designing the project in a way that felt intuitive, was easy to read, and had a manageable workflow. To achieve these objectives I broke the project into three key functions: cleaning, visualizing, and styling - organized within the ‘Functions’ folder for that weeks submission. The index.R file integrates data loading, function execution, and the generation of of a data visualization.\n\nCleaning FunctionVisual FunctionStyle FunctionIndex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\n# dplyr: data cleaning functions.\nbase::library(dplyr)\n#### Cleaning Function ####\nclean &lt;- function(df){\n  # clean function\n  clean_df &lt;- df %&gt;%\n    dplyr::group_by(season) %&gt;%\n    dplyr::summarise(\n      total_num_colors = base::sum(num_colors))\n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# ggplot2: graphing functions. \nbase::library(ggplot2)\n#### Visual Function ####\nvis &lt;- function(clean_df){\n  vis &lt;- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(\n      x = season,\n      y = total_num_colors)) +\n    ggplot2::geom_point() + \n    ggplot2::geom_segment(\n      ggplot2::aes(\n        x = season,\n        xend = season,\n        y = 0,\n        yend = total_num_colors)) + \n    ggplot2::geom_hline(\n      yintercept=137.871, \n      linetype=\"dashed\", \n      color = \"red\") +\n    ggplot2::geom_text(\n      ggplot2::aes(label = total_num_colors,\n                   vjust = -1)\n    )\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# ggplot2: graphing functions. \nbase::library(ggplot2)\n#### Style Function ####\nsty &lt;- function(vis){\n  sty &lt;- vis +\n    # labels \n    ggplot2::labs(\n      title = \"Colors Used Each Season\",\n      subtitle = \"The dashed red line shows the average number of colors used each season, 137.871.\",\n      caption = \"Graph by Randi Bolt \\n Data from #TidyTuesday\",\n      x = \"Season\",\n      y = \"Colors Used\"\n    ) + \n    # themes\n    ggplot2::theme_classic() + \n    ggplot2::theme(\n      plot.title = element_text(\n        face = \"bold\",\n        hjust = .5),\n      plot.subtitle = element_text(\n        hjust = .5)\n    ) \n  return(sty)\n  }\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# tidyverse: A collection of data-related packages.\nbase::library(tidyverse)\n\n#### Load Data ####\n# bob_ross: data about and from \"The Joy of Painting\".\ntt_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-21/bob_ross.csv')\n\n#### Clean Data ####\nclean_data &lt;- clean(tt_data)\n\n#### Create Data Visual ####\nvis_data &lt;- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis &lt;- sty(vis_data)\n\n#### Save Plot ####\nggplot2::ggsave(\n  base::paste0(\"plot.png\"), \n  width = 30, \n  height = 20, \n  units = \"cm\")\n\n\n\n\nWeek 8 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nTo make a lollipop graph using ggplot2 you need to assign geom_point() and geom_segment().\ngoem_hline() was used to created the dashed horizontal red line.\ngeom_text() is used to add the values above the lollipops.\nUsing the TidyTuesdayR package can be problematic.\nIt looks nicer to keep all labels in the labs() function."
  },
  {
    "objectID": "projects/Educational-Insights.html",
    "href": "projects/Educational-Insights.html",
    "title": "Educational Insights",
    "section": "",
    "text": "Notes from Ge Shao’s Applied Regression Analysis Fall 2021 class. Topics include simple linear regression, least-square estimates, hypothesis testing, t-stat, p-stat, sum of squares estimates, mean square error, confidence interval, R squared, multiple linear models, hat matrix, multilinearity, model adequacy checking, residual analysis, PRESS statistic, detecting outliers, correcting model inadequacies, stabilizing variance, DIFFTS, DFBETAS, COVRATIO,… Continue reading.\n\n\n\nNotes from Nadee Jayasena’s Introduction ot Mathematical Statistics II Winter 2022 class. Topics include marginal PDF’s, joint PDF’s and CDF’s, independence of random variables, bivariate probability, conditional distribution, conditional covariance, correlation, univariate transformations, bivariate transformations, Jacobians, method of moment-generating functions, sampling distributions, central limit theorem, … Continue reading\n\n\n\nNotes from Dorcas Ofori-Boaten’s Introduction to Mathematical Statistics I Fall 2021 class. Topics include mean, median, mode, variance, standard deviation, Empirical Rule, set theory, DeMorgans Law, Distributive Law, counting rules, statistical independence, mutual independence, total law of probability, Bayes Rule, discrete and continuous random variables, probability distribution, expected value, CDF, PMF, PDF, … Continue reading\n\n\n\nNotes from Subash Kochar’s Applied Statistics for Engineers and Scientists Summer 2021 class. Topics include discrete and continous random various, bernoulli random variables, pmf, pdf, expeced value, variance, mean, standard deviation, probablity distributions: Poisson, binomial, uniform, normal, exponential, gamma, chi-squared, Weibull, lognormal, …Continue reading"
  },
  {
    "objectID": "projects/Educational-Insights.html#stat-464-applied-regression-analysis",
    "href": "projects/Educational-Insights.html#stat-464-applied-regression-analysis",
    "title": "Educational Insights",
    "section": "",
    "text": "Notes from Ge Shao’s Applied Regression Analysis Fall 2021 class. Topics include simple linear regression, least-square estimates, hypothesis testing, t-stat, p-stat, sum of squares estimates, mean square error, confidence interval, R squared, multiple linear models, hat matrix, multilinearity, model adequacy checking, residual analysis, PRESS statistic, detecting outliers, correcting model inadequacies, stabilizing variance, DIFFTS, DFBETAS, COVRATIO,… Continue reading."
  },
  {
    "objectID": "projects/Educational-Insights.html#stat-462-introduction-to-mathematical-statistics-ii",
    "href": "projects/Educational-Insights.html#stat-462-introduction-to-mathematical-statistics-ii",
    "title": "Educational Insights",
    "section": "",
    "text": "Notes from Nadee Jayasena’s Introduction ot Mathematical Statistics II Winter 2022 class. Topics include marginal PDF’s, joint PDF’s and CDF’s, independence of random variables, bivariate probability, conditional distribution, conditional covariance, correlation, univariate transformations, bivariate transformations, Jacobians, method of moment-generating functions, sampling distributions, central limit theorem, … Continue reading"
  },
  {
    "objectID": "projects/Educational-Insights.html#stat-461-introduction-to-mathematical-statistics-i",
    "href": "projects/Educational-Insights.html#stat-461-introduction-to-mathematical-statistics-i",
    "title": "Educational Insights",
    "section": "",
    "text": "Notes from Dorcas Ofori-Boaten’s Introduction to Mathematical Statistics I Fall 2021 class. Topics include mean, median, mode, variance, standard deviation, Empirical Rule, set theory, DeMorgans Law, Distributive Law, counting rules, statistical independence, mutual independence, total law of probability, Bayes Rule, discrete and continuous random variables, probability distribution, expected value, CDF, PMF, PDF, … Continue reading"
  },
  {
    "objectID": "projects/Educational-Insights.html#stat-451-applied-statistics-for-engineers-and-scientists",
    "href": "projects/Educational-Insights.html#stat-451-applied-statistics-for-engineers-and-scientists",
    "title": "Educational Insights",
    "section": "",
    "text": "Notes from Subash Kochar’s Applied Statistics for Engineers and Scientists Summer 2021 class. Topics include discrete and continous random various, bernoulli random variables, pmf, pdf, expeced value, variance, mean, standard deviation, probablity distributions: Poisson, binomial, uniform, normal, exponential, gamma, chi-squared, Weibull, lognormal, …Continue reading"
  },
  {
    "objectID": "projects/Educational-Insights.html#mth-388-modern-college-geometry",
    "href": "projects/Educational-Insights.html#mth-388-modern-college-geometry",
    "title": "Educational Insights",
    "section": "MTH 388: Modern College Geometry",
    "text": "MTH 388: Modern College Geometry\nNotes from Rebecca Tramel’s Modern College Geometry Winter 2022 class. Topics include axioms, proof writing, Euclid’s Elements, congruence, isometries, triangles, area proofs, circles, group theory for symmetries, Euclidean distance, taxi-cab geometry, spherical geometry, hyperbolic geometry, … Continue reading"
  },
  {
    "objectID": "projects/Educational-Insights.html#mth-344-introduction-to-group-theory",
    "href": "projects/Educational-Insights.html#mth-344-introduction-to-group-theory",
    "title": "Educational Insights",
    "section": "MTH 344: Introduction to Group Theory",
    "text": "MTH 344: Introduction to Group Theory\nNotes from Julie Bracken’s Introduction to Group Theory Winter 2022 class. Topics include set theory, cancellation law, subgroups, cyclic subgroups, injecteive, surjective, bijective, function composition, permutation groups, symmetric groups, dihedral group, cycle decomposition, parity of transposistions, alternating groups, isomorphism, partitions, Lagrange’s Theorem, homomorphism, normal subgroups, kernal, … Continue reading"
  },
  {
    "objectID": "projects/Technical-Blogs.html",
    "href": "projects/Technical-Blogs.html",
    "title": "Technical Blogs",
    "section": "",
    "text": "Current iteration of my blog. Made using Quarto and deployed with Netlify. Technical posts include a lot of data analysis and visualization in R, and tutorials."
  },
  {
    "objectID": "projects/Technical-Blogs.html#rbolt.me-2024",
    "href": "projects/Technical-Blogs.html#rbolt.me-2024",
    "title": "Technical Blogs",
    "section": "",
    "text": "Current iteration of my blog. Made using Quarto and deployed with Netlify. Technical posts include a lot of data analysis and visualization in R, and tutorials."
  },
  {
    "objectID": "projects/Technical-Blogs.html#rbolt22-2023",
    "href": "projects/Technical-Blogs.html#rbolt22-2023",
    "title": "Technical Blogs",
    "section": "rbolt22 (2023)",
    "text": "rbolt22 (2023)\nMade using Quarto and deployed with Netlify. Topics include collaborating with Posit Cloud and Google Drive, using R, SQL, and Python, creating a package in R, web scraping, data visualization, machine learning, statistics and probability, and more."
  },
  {
    "objectID": "projects/Technical-Blogs.html#rbolt2-2022",
    "href": "projects/Technical-Blogs.html#rbolt2-2022",
    "title": "Technical Blogs",
    "section": "rbolt2 (2022)",
    "text": "rbolt2 (2022)\nMade with R using the blogdown package and deployed with Netlify. Topics include reproducible research for data science, docker for reproducible research, project organization for data science, Bayes statistics, predicting stock prices with R packages, statistics and mathematics notes, and more."
  },
  {
    "objectID": "projects/Technical-Blogs.html#rbolt-2021",
    "href": "projects/Technical-Blogs.html#rbolt-2021",
    "title": "Technical Blogs",
    "section": "rbolt (2021)",
    "text": "rbolt (2021)\nMade with R using the blogdown package and deployed with Netlify. Topics include using API’s, analyzing and modifying linear regression models, and more."
  },
  {
    "objectID": "projects/Data-Visuals-and-Analysis.html",
    "href": "projects/Data-Visuals-and-Analysis.html",
    "title": "Data Visuals and Analysis",
    "section": "",
    "text": "TidyTuesday is a weekly data analysis and visualization challenge that provides a structured format to practice data wrangling and visualization skills using R programming language. Each week, a new dataset is posted to the TidyTuesday Github repository and participants are encouraged to explore and visualize the data using the principles of tidy data. Continue reading …"
  },
  {
    "objectID": "projects/Data-Visuals-and-Analysis.html#tidy-tuesday",
    "href": "projects/Data-Visuals-and-Analysis.html#tidy-tuesday",
    "title": "Data Visuals and Analysis",
    "section": "",
    "text": "TidyTuesday is a weekly data analysis and visualization challenge that provides a structured format to practice data wrangling and visualization skills using R programming language. Each week, a new dataset is posted to the TidyTuesday Github repository and participants are encouraged to explore and visualize the data using the principles of tidy data. Continue reading …"
  },
  {
    "objectID": "projects/Data-Visuals-and-Analysis.html#rsite",
    "href": "projects/Data-Visuals-and-Analysis.html#rsite",
    "title": "Data Visuals and Analysis",
    "section": "#Rsite",
    "text": "#Rsite\nFour different projects from Statistics 363: Introduction to R, which includes analysis of NYC Flight Data, investigating if Kobe Bryant had “hot hands”, comparing Gas and Hydro power using base R and ggplot2, and exploring states with the highest and lowest numbers of Covid cases. Continue reading …"
  },
  {
    "objectID": "projects/Data-Visuals-and-Analysis.html#oregon-grown",
    "href": "projects/Data-Visuals-and-Analysis.html#oregon-grown",
    "title": "Data Visuals and Analysis",
    "section": "Oregon Grown",
    "text": "Oregon Grown\nA state known for growing all varieties of trees and greenery, it’s no wonder that Oregon was one of the first states to legalize recreational cannabis in 2014. Measure 91 changed the lives of both those affiliated with cannabis and those who benefit from the tax revenue it brings in. While many businesses and industries struggled in 2020 due to COVID-19 the cannabis industry kept growing. Continue reading …"
  },
  {
    "objectID": "projects/Talks.html",
    "href": "projects/Talks.html",
    "title": "Talks",
    "section": "",
    "text": "Code to Content (2024)\n\nUsed 4 years of hands-on experience creating technical blogs to compile a short 6 minute presentation to guide and encourage R users to create and share their expertise and journeys online.\nCreated accessible slides with Quarto, and hosted on Github.\nPresented as a lightning talk at PDX R User Group, February 2024.\nAccepted as a presenter for a lightning talk at Cascadia R Conference, June 2024.\n\n\n\nOregon Grown (2021)\n\nUsed spatial references from Oregon Spatial Library, ARLIS, and PCC Geography Department to apply mapping techniques such as select by attribute, spatial join, clip, and XY Table to Point to investigate dispensary diversity in Oregon November 2020.\nFormatted, organized, added classifications, and geocoded a data table of approximately 700 rows of data from the Oregon Liquor Control Commission’s active marijuanna retail license.\nDiscovered more than half of dispensaries in Oregon are owned by chains, and about 10% of dispensaries in Portland’s Urban Growth Boundary are owned by one chain, Groundworks.\nCreated an abstract that was accepted for GIS in Action 2021, and presented virtually April 2021."
  },
  {
    "objectID": "projects/Dashboards.html",
    "href": "projects/Dashboards.html",
    "title": "Dashboards",
    "section": "",
    "text": "2024 Oscar Prediction Challenge Dashboard is an interactive shiny app that presents the results of a survey where participants predicted the winners of various Oscars categories."
  },
  {
    "objectID": "projects/Dashboards.html#oscar-prediction-challenge",
    "href": "projects/Dashboards.html#oscar-prediction-challenge",
    "title": "Dashboards",
    "section": "",
    "text": "2024 Oscar Prediction Challenge Dashboard is an interactive shiny app that presents the results of a survey where participants predicted the winners of various Oscars categories."
  },
  {
    "objectID": "projects/Dashboards.html#covid-19",
    "href": "projects/Dashboards.html#covid-19",
    "title": "Dashboards",
    "section": "Covid-19",
    "text": "Covid-19\nCovid-19 Dashboard is an interactive analysis tool that specifically examines Covid-19 cases and fatalities in both the United States and my home state of Oregon. What initially began as a class project at the onset of the pandemic, has undergone multiple iterations and refinements to evolve into a fully functional and informative dashboard."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html",
    "href": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html",
    "title": "NBA Salaries - Part 1: Web-Scraping",
    "section": "",
    "text": "Scraping and cleaning NBA Salary data from ESPN - NBA Players Salaries, with some simple statistic and data visuals."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#change-salaray-from-character-to-numeric",
    "href": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#change-salaray-from-character-to-numeric",
    "title": "NBA Salaries - Part 1: Web-Scraping",
    "section": "4.1 Change Salaray from Character to Numeric",
    "text": "4.1 Change Salaray from Character to Numeric\nNotice that the SALARY column is a character value. This will not be helpful when trying to do math, or make graphs with this numerical data. To change this 3 things must be addressed:\n\nRemoving the dollar sign.\nRemoving the commas.\nChange character type to numeric.\n\n\nnba_salaries_2023$SALARY &lt;- str_remove_all(nba_salaries_2023$SALARY,\n                    \"\\\\$\")\nnba_salaries_2023$SALARY &lt;- str_remove_all(nba_salaries_2023$SALARY,\n                    \",\")\nnba_salaries_2023$SALARY &lt;- as.numeric(nba_salaries_2023$SALARY)\n\nNow we are able to do math, make graphs, and arrange the data by salary."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#basic-statistics",
    "href": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#basic-statistics",
    "title": "NBA Salaries - Part 1: Web-Scraping",
    "section": "4.2 Basic Statistics",
    "text": "4.2 Basic Statistics\n\nHighest paid value : 51,915,615\nLowest paid value : 289,542\nMedian : 5e+06\nMean : 9,925,143\nStandard Deviation : 11,295,000"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#box-plots",
    "href": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#box-plots",
    "title": "NBA Salaries - Part 1: Web-Scraping",
    "section": "4.3 Box Plots",
    "text": "4.3 Box Plots\n\n4.3.1 2022 - 2023 Yearly Salary by Postion\n\nggplot2::ggplot(data = nba_salaries_2023,\n                mapping = ggplot2::aes(x = SALARY,\n                                       y = POSITION)) + \n  ggplot2::geom_boxplot()\n\n\n\n\n\n\n\n\nFrom this visual we can see that Point Guards (PG) appear to be paid the most, while Guards (G) and Forwards (F) on average are paid the least.\n\n\n4.4 2022-2023 Yearly Salary by Team\n\nggplot2::ggplot(data = nba_salaries_2023,\n                mapping = ggplot2::aes(x = SALARY,\n                                       y = TEAM)) + \n  ggplot2::geom_boxplot()\n\n\n\n\n\n\n\n\nThis plot is not the easiest to read, and might be worth sub-setting the information further. However eye-balling this visual we can see most teams pay between $2,000,000 and $15,000,000 per player with a few outliers. These outliers of course being superstar players."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#footnotes",
    "href": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#footnotes",
    "title": "NBA Salaries - Part 1: Web-Scraping",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nForbes - The Worlds 10 Highest-Paid Athletes of 2022↩︎"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240326-TidyTuesday-marchmadness/index.html",
    "href": "blog/Technical-Blog/posts/20240326-TidyTuesday-marchmadness/index.html",
    "title": "Week 13 Tidy Tuesday: NCAA Men’s March Madness",
    "section": "",
    "text": "This week’s TidyTuesday slam dunks into March Madness, highlighting the discrepancy between actual round winners and fan forecasts. My chart focuses on the bracket busters, where red celebrates teams that surpassed expectations, and blue revealing those that couldn’t shake the pressure.\n\nNCAA’s Men’s March Madness\n\nThis week’s data is NCAA Men’s March Madness data from Nishaan Amin’s Kaggle dataset and analysis Bracketology: predicting March Madness.\n\n\nCode\nThis code follows the established #TidyTuesday tradition of cleaning, visualizing, and styling data. However, it introduces a streamlined structure organized into four distinct phases: Set-Up, Clean, Graph, and Save.\n\nSet-UpCleanGraphSaveLinks\n\n\n\n\nShow Code\n#### Packages ####\n# tidyverse: A collection of data-related packages.\n# googlesheets4: Read data from a google sheet URL. \n# forcats: Working with Categorical Variables (Factors)\n# showtext: Use various fonts. \n# ggtext: Use markdown in subtitle. \nbase::library(tidyverse)\nbase::library(googlesheets4)\nbase::library(forcats)\nbase::library(showtext)\nbase::library(ggtext)\n\n#### Data ####\n# public_picks: TidyTuesday Data of public picks. \n# team_results: My data set of team results for 2024.\npublic_picks &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-03-26/public-picks.csv')\nteam_results &lt;- googlesheets4::read_sheet(\n  \"https://docs.google.com/spreadsheets/d/1rxHKeP0RVZZ2ujjqRPq3AhK6R8a4aI7LMRyli543gTs/edit#gid=0\",\n  sheet = \"2024\",\n  na=\"TBD\"\n)\n\n#### Fonts ####\n# font_add_google(): Search Google Fonts. \n# showtext_auto(): Turn showtext on for graphics. \nsysfonts::font_add_google(\"Merriweather\", \"font\")\nshowtext::showtext_auto()\n\n#### Colors ####\n# col1: Text \n# col2: Background\ncol1 &lt;- \"#002A5C\"\ncol2 &lt;- \"#e0a761\"\n\n### Text ####\n# title_text\n# subtitle_text\n# caption_text\n# xlab_text\n# ylab_text\n# fill_text\ntitle_text &lt;- \"Team Performance vs. Public Expectation\"\nsubtitle_text &lt;- \"March Madness is a knockout college basketball tournament with 68 teams &lt;br&gt;battling in six rounds for the title. This chart, covering men's teams for &lt;br&gt; 2024, shows when teams didn't meet or beat fan expectations. &lt;span style='color:red;'&gt;Red &lt;/span&gt; means &lt;br&gt;a team did &lt;span style='color:red;'&gt;better than expected&lt;/span&gt;, &lt;span style='color:blue;'&gt;blue means worse&lt;/span&gt;, and gray indicates &lt;br&gt;that the data isn't currently available. The values represent the percentage &lt;br&gt;point difference between actual wins and public forecasts.\"\ncaption_text &lt;- \"Randi Bolt \\nApril 2024 \\n#TidyTuesday \\nNCAA March Madness\"\nxlab_text &lt;- \"NCAA Tournament Rounds\"\nylab_text &lt;- \"Team (Seed)\"\nfill_text &lt;- \"Difference\"\n\n\nSet-Up\n\nPackages\nData\nFonts\nColors\nText\n\n\n\n\n\nShow Code\n### Clean Data\n# 1. Create win columns for each round. \n# 2. Convert char percentage columns into doubles.\n# 3. Join team_results and public picks by TEAM.\n# 4. Calculate the difference b/t team results and public picks.\n# 5. Pivot heatmap data longer so that rounds are in one column.\n# 6. Rename rounds to extend names for labels.\n# 7. Re-assign factor levels for rounds column.\n\n# 1. Creates new columns that will have a 1 value if the team made it to that round at least once, otherwise it will be 0.\nteam_results &lt;- team_results |&gt;\n  mutate(R64WIN = ifelse(R64 &gt; 0, 1, 0),\n         R32WIN = ifelse(R32 &gt; 0, 1, 0),\n         S16WIN = ifelse(S16 &gt; 0, 1, 0),\n         E8WIN = ifelse(E8 &gt; 0, 1, 0),\n         F4WIN = ifelse(F4 &gt; 0, 1, 0),\n         F2WIN = ifelse(F2 &gt; 0, 1, 0),\n         CHAMPWIN = ifelse(CHAMP &gt; 0, 1, 0))\n\n# 2. Convert character percentage columns into doubles.\npublic_picks &lt;- public_picks |&gt;\n  filter(YEAR == 2024) |&gt;\n  mutate(\n    across(\n      R64:FINALS, \n      ~readr::parse_number(as.character(.))\n    )\n  )\n\n# 3. Join team_results and public picks by TEAM\ndata_for_heatmap &lt;- team_results |&gt;\n  inner_join(public_picks, by = \"TEAM\") |&gt;\n  # 4. Calculate the difference b/t team results and public picks.\n  mutate(TEAM_SEED = paste(TEAM, \" (\", SEED, \")\", sep = \"\"),\n         DIFF_R64 = R64WIN * 100 - R64.y,\n         DIFF_R32 = R32WIN * 100 - R32.y,\n         DIFF_S16 = S16WIN * 100 - S16.y,\n         DIFF_E8 = E8WIN * 100 - E8.y,\n         DIFF_F4 = F4WIN * 100 - F4.y,\n         DIFF_FINALS = F2WIN * 100 - FINALS)\n\n# 5. Pivot heatmap data longer so that rounds are in one column.\ndata_for_heatmap_long &lt;- data_for_heatmap |&gt;\n  select(TEAM_SEED, REGION, DIFF_R64, DIFF_R32, \n         DIFF_S16, DIFF_E8, DIFF_F4, DIFF_FINALS) |&gt;\n  pivot_longer(cols = starts_with(\"DIFF\"), \n               names_to = \"ROUND\", \n               values_to = \"DIFFERENCE\") |&gt;\n  mutate(ROUND = sub(\"DIFF_\", \"\", ROUND)) \n\n# 6. Rename rounds to extend names for labels.\ndata_for_heatmap_long$ROUND &lt;- \n  factor(\n    data_for_heatmap_long$ROUND,\n    levels = c(\"R64\", \"R32\", \"S16\", \n               \"E8\", \"F4\", \"FINALS\"),\n    labels = c(\"Round of 64\", \"Round of 32\", \n               \"Sweet 16\", \"Elite 8\", \n               \"Final Four\", \"Finals\")\n  )\n\n# 7. Re-assign factor levels for rounds column.\ndata_for_heatmap_long$ROUND &lt;- fct_relevel(\n  data_for_heatmap_long$ROUND, \n  \"Round of 64\", \"Round of 32\",\n  \"Sweet 16\", \"Elite 8\",\n  \"Final Four\", \"Finals\")\n\n\nClean\n\nCreate win columns for each round.\nConvert character percentage columns into doubles.\nJoin team_results and public_picks by TEAM.\nCalculate the difference between team results and public picks.\nPivot heatmap data longer so that rounds are in one column.\nRename rounds to extend names for labels.\nRe-assign factor levels for rounds column.\n\n\n\n\n\nShow Code\n#### Heatmap ####\n# Data: data_for_heatmap_long\n# Aesthetics: x = Round, y = Team (Seed), fill = difference\n# Graph Colors: low = blue, high = red, midpoint = 0\n# Labels: defined under Set-Up &gt; Text. \n# Add Values: If the difference is NA define as TBD, else\n# round the difference to 2 decimals and add a % sign. \n# Facet Wrap: by region. \n# Apply minimal theme. \n# Theme: title, subtitle, caption, axis titles, axis text,\n# background colors, and legend position. \n\n# heatmap\nheatmap &lt;- ggplot2::ggplot(\n  # Data\n  data_for_heatmap_long, \n  # Aesthetics\n  aes(x = ROUND, \n      y = TEAM_SEED, \n      fill = DIFFERENCE)\n  ) +\n  geom_tile() + \n  # Graph Colors\n  ggplot2::scale_fill_gradient2(\n    low = \"blue\", \n    high = \"red\", \n    midpoint = 0\n  ) +\n  # Labels\n  ggplot2::labs(\n    title = title_text,\n    subtitle = subtitle_text,\n    caption = caption_text,\n    x = xlab_text,\n    y = ylab_text,\n    fill = fill_text\n  ) +\n  # Percentage Point Difference Values\n  ggplot2::geom_text(\n    ggplot2::aes(\n      label = ifelse(\n        is.na(DIFFERENCE), \"TBD\",\n        paste0(round(DIFFERENCE, \n                     digits = 2),\"%\")\n        ),\n      hjust = 0\n    )\n  ) +\n  # Facet Wrap\n  facet_wrap(~ REGION, ncol = 1, scales = \"free_y\") +\n  # Apply minimal theme\n  ggplot2::theme_minimal() +\n  # Theme\n  ggplot2::theme(\n      # Title\n      plot.title = element_text(\n        size = 30,\n        family = \"font\",\n        face = \"bold\",\n        hjust = 0,\n        color = col1),\n      # Subtitle\n      plot.subtitle = element_markdown(\n        size = 16,\n        family = \"font\",\n        hjust = 0,\n        color = col1),\n      # Caption\n      plot.caption = element_text(\n        size = 12,\n        family = \"font\",\n        hjust = 1,\n        color = col1),\n      # Axis Titles\n      axis.title = element_text(\n        size = 24,\n        family = \"font\",\n        color = col1),\n      # X Axis Title\n      axis.title.x = element_text(vjust = -1.5),\n      # Axis Text\n      axis.text = element_text(\n        size = 12,\n        family = \"font\",\n        color = col1),\n      # Background colors\n      plot.background = element_rect(fill = col2),\n      # Adjust Axis text\n      axis.text.x = element_text(vjust = -1),\n      # Position Legend\n      legend.position = \"bottom\") \n\n\nGraph\n\nData: data_for_heatmap_long\nAesthetics: x = Round, y = Team (Seed), fill = difference\nGraph Colors: low = blue, high = red, midpoint = 0\nLabels: defined under Set-Up &gt; Text.\nAdd Values: If the difference is NA define as TBD, else round the difference to 2 decimals and add a % sign.\nFacet Wrap: by region.\nApply minimal theme.\nTheme: title, subtitle, caption, axis titles, axis text, background colors, and legend position.\n\n\n\n\n\nShow Code\nggsave(\n  \"plot.png\", \n  width = 35, \n  height = 45, \n  units = \"cm\",\n  dpi = 85)\n\n\nSave plot as plot.png!\n\n\n\nrfordatascience/tidytuesday Repo\nrbolt13/tidytuesday Repo\n\n\n\n\n\nQuick Notes\n\nI created a custom dataset for this year’s team results in a Google Sheet, which allowed me to include each team’s region and seed. This addition helps organize the data more effectively and gives readers additional context regarding each team’s perceived strength.\nFor incorporating blue and red text into the subtitle, I utilized the ggtext package. To apply this within the ggplot2 environment, I used the element_markdown() function in place of the element_text() function within the ggplot2::theme() settings.\nI employed scales = \"free_y\" in the facet_wrap() function to prevent y-axis labels from overlapping. This ensures that each facet has its own set of y-axis elements, maintaining clear separation and readability.\nData for the public picks in the championship round was not available, which could have been a compelling addition to this analysis."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20211129-Proof-Idempotent/index.html",
    "href": "blog/Technical-Blog/posts/20211129-Proof-Idempotent/index.html",
    "title": "Prove H and I-H are Idempotent",
    "section": "",
    "text": "Define the hat matrix \\(H=X(X^TX)^{-1}X^T\\). Prove H and I-H are Idempotent\n\n\nProof\nFor H to be Idempotent then \\(HH=H\\)\n\\[\\begin{equation}\\label{HH=H}\n\\begin{split}\nHH & =[X(X^TX)^{-1}X^T][X(X^TX)^{-1}X^T]\\\\\n& = X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T\\quad\\quad(X^TX)^{-1}X^TX=1\\\\\n& = X(X^TX)^{-1}X^T\\\\\n& = H\n\\end{split}\n\\end{equation}\\]\nTherefore by the series of equalities H is idempotent.\nFor I-H to be idempotent then \\((I-H)(I-H)=I-H\\)\n\\[\\begin{equation}\\label{I-H}\n\\begin{split}\n(I-H)(I-H) & =II-HI-IH+HH\\quad\\quad II=I, HI=IH=H, HH=H\\\\\n& = I-H-H+H\\\\\n& = I-H\n\\end{split}\n\\end{equation}\\]\nTherefor by the series of equalities I-H is idempotent.\nQED."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html",
    "href": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html",
    "title": "Midterm Prep. Modern College Geometry",
    "section": "",
    "text": "Notes consists of logic, Euclid’s 5th, Congruence and Length Theorem, Congruence and Angle Theorem, angles and parallel lines, and triangle congruence theorems."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html#example",
    "href": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html#example",
    "title": "Midterm Prep. Modern College Geometry",
    "section": "Example",
    "text": "Example\nConditional Statement: If it is cloudy then it is raining.\nNegation: It could be cloudy and not raining.\nInverse: If it is not cloudy then it is not raining.\nContrapositive: If it is not raining then it is not cloudy.\nConverse. If it is rainy, then it is cloudy."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html#example-1",
    "href": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html#example-1",
    "title": "Midterm Prep. Modern College Geometry",
    "section": "Example",
    "text": "Example\nWhen proving supplementary angles add to \\(180^\\circ\\) we were able to use Euclids 5th element to say that the supplementary interior angles added up to \\(\\geq 180^\\circ\\) because the lines are parallel (and don’t intersect)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html#proof-points",
    "href": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html#proof-points",
    "title": "Midterm Prep. Modern College Geometry",
    "section": "Proof Points",
    "text": "Proof Points\n\n“Because \\(\\triangle ABC\\cong \\triangle DEF\\) then there is an isometry f that superimposes angle ABC on angle DEF. Isometries preserve angle measure, so angles ABC and DEF must have had the same measure.”\n“Suppose angles ABC and DEF have the same measure.” Then explain isometry needed to move angle ABC to angle DEF. “Translation, rotation, and reflection are all isometries, so we’ve shown angles ABC and DEF are congruent.”"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230530-TidyTuesday-CentenariansData/index.html",
    "href": "blog/Technical-Blog/posts/20230530-TidyTuesday-CentenariansData/index.html",
    "title": "Week 22 Tidy Tuesday: Centenarians Data",
    "section": "",
    "text": "For week 22 of #TidyTuesday we dive into the realm of centenarians’ data, uncovering a surprising trend: a remarkably high number of centenarian deaths are recorded in the month of January.\n\nVerified Oldest People\nThe data this week comes from the Wikipedia List of the verified oldest people via frankiethull on GitHub. Thank you for the submission, Frank!\n\nThese are lists of the 100 known verified oldest people sorted in descending order by age in years and days. The oldest person ever whose age has been independently verified is Jeanne Calment (1875–1997) of France, who lived to the age of 122 years and 164 days. The oldest verified man ever is Jiroemon Kimura (1897–2013) of Japan, who lived to the age of 116 years and 54 days. The oldest known living person is Maria Branyas of Spain, aged 116 years, 85 days. The oldest known living man is Juan Vicente Pérez of Venezuela, aged 114 years, 1 day. The 100 oldest women have, on average, lived several years longer than the 100 oldest men.\n\n\n\n\nCode\nThis week I wanted to do more to spice up a bar plot. So I gave the visual a dark black background, and highlighted the main findings in red. This project retains its core components of cleaning, visualizing, and styling, while the index.R file handles data loading, file execution, and the generation of a data visualization.\n\nCleaningVisualizeStyleindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\n# magrittr: %&gt;% pipe function. \n# dplyr: data cleaning functions. \nbase::library(dplyr)\nbase::library(magrittr)\n\n#### Cleaning Function ####\nclean &lt;- function(df){\n  clean_df &lt;- df %&gt;%\n    dplyr::mutate(death_month = base::format(death_date, \"%m\")) %&gt;%\n    dplyr::group_by(death_month) %&gt;%\n    dplyr::summarise(count = dplyr::n()) %&gt;%\n    # highlight jan. \n    mutate(to_highlight = ifelse(death_month == \"01\", \"yes\", \"no\"))\n  clean_df$to_highlight[13] &lt;- \"no\"\n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(ggplot2)\n\n#### Visual Function ####\nvis &lt;- function(clean_df){\n  vis &lt;- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(x = death_month, y = count, fill = to_highlight)) +\n    geom_bar(stat = \"identity\")\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Install Packages\ninstall.packages(\"ggchicklet\", repos = \"https://cinc.rud.is\")\n\n#### Load Packages ####\nbase::library(ggplot2)\nbase::library(ggchicklet)\n\n#### Load Fonts ####\nsysfonts::font_add_google(\"EB Garamond\", \"font\")\nshowtext::showtext_auto()\n\n#### Load Colors ####\ncol1 &lt;- \"white\"\ncol2 &lt;- \"#0d0d0d\"\ncol3 &lt;- \"#b22b2e\"\ncol4 &lt;- \"#6d6a6e\"\n\n#### Style Function ####\nsty &lt;- function(vis){\n  sty &lt;- vis +\n    # labs\n    ggplot2::labs(\n      title = \"High Number of Centenarian Deaths in January\",\n      subtitle = \"Of the 200 oldest verified men and women,\\n the most common month for a centarian to die in is January.\",\n      caption = \"\\nRandi Bolt ~ #TidyTuesday - Verified Oldest People ~ May 2023\",\n      x = \"Death Month\",\n      y = \"Count\")  +\n    # ylim\n    ylim(0, 36) +\n    # add numbers above boxes\n    ggplot2::geom_text(\n      ggplot2::aes(\n        label = count,\n        vjust = -.5),\n      color = col1,\n      size = 20\n    ) + \n    # scales \n    ggplot2::scale_x_discrete(\n      labels = c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\", \"Alive\")\n    ) +\n    scale_fill_manual(values  = c(\"yes\" = col3, \"no\"= col4), guide = \"none\") + \n    # theme\n    ggplot2::theme(\n      plot.title = element_text(\n        size = 100,\n        family = \"font\",\n        face = \"bold\",\n        hjust = .5,\n        vjust = -5,\n        color = col1),\n      plot.subtitle = element_text(\n        size = 55,\n        family = \"font\",\n        hjust = .5,\n        vjust = -20,\n        color = col1),\n      plot.caption = element_text(\n        size = 30,\n        family = \"font\",\n        hjust = .5,\n        color = col1),\n      axis.title = element_text(\n        size = 50, \n        family = \"font\",\n        face = \"bold\",\n        color = col1),\n      axis.text = element_text(\n        size = 45, \n        family = \"font\",\n        color = col1),\n      axis.text.x = element_text(angle = 25), \n      plot.background = element_rect(fill = col2),\n      panel.background = element_rect(fill = col2),\n      panel.grid.major.x = element_blank(),\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank()) +               \n    # ggplot2 barplot with round corners\n    geom_chicklet(radius = grid::unit(3, \"mm\")) \n  return(sty)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(tidyverse)\n\n#### Load Data ####\n# centenarians: 100 verified oldest men and women.\ncentenarians &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-05-30/centenarians.csv')\n\n#### Clean Data ####\nclean_data &lt;- clean(centenarians)\n\n#### Create Data Visual ####\nvis_data &lt;- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis &lt;- sty(vis_data)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\")\n\n\n\n\nWeek 22 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nTo highlight the month of January in the cleaning file I used the dplyr::mutate() function to create a “to_highlight” column for any death month equal to January. Then in the visualize package I assigned the fill aesthetic to the “to_highlight” column.\nTo curve the edges of the boxes (so they look more like tombstones) in the styling file I used ggchicklet::geom_chicklet() to assign a curved radius of 3 milimeters.\nTo angle the labels on the x-axis I used ggplot2::theme(axis.text.x = element_text(angle = 25)).\nSpacing is a little funky, but I think it might work for this visual.\nFrequentist data visuals seem to be the low hanging fruit for these Tid Tuesday submissions, and my goal is to push onto more predictive and model based visual analysis by the end of the summer."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230130-Original-Flashcards/index.html",
    "href": "blog/Technical-Blog/posts/20230130-Original-Flashcards/index.html",
    "title": "Flashcards",
    "section": "",
    "text": "This post uses interactive flashcards to cover terminology in the fields of data science, machine learning, mathematics, probability, and statistics.\n\n\nIntroduction\nI’ve recently been applying to and partaking in interviews for remote data science positions all over the country. The last couple of first round technical interviews I went on I was asked a range of questions related to data science, statistics, machine learning, probability, linear algebra, and mathematics. To test myself on answering these types of questions more confidently I created these definitions “Flashcards”, which are actually tabset panels, a component layout in quarto.\nEach definition can be viewed by clicking on the “Definition” tab for each word. All words are sorted alphabetically, definitions are generally casual, words will be continually added, and regularly updating.\n\n\nA\n\nA/B TestingDefinition\n\n\n\n\n\nTo compare two versions of something, usually a control (A) and a test variable (B).\n\n\n\n\n\nB\n\nBayes TheoremDefinition\n\n\n\n\n\nA method for calculating conditional probability, or the likelihood of one event occurring based on prior knowledge of conditions that might be related to the event.\n\\[P(A|B)=\\frac{P(A|B)P(A)}{P(B)}=\\frac{\\text{(likelihood)}\\times\\text{(prior)}}{\\text{(evidence)}}\\]\n\n\n\n\nBiasDefinition\n\n\n\n\n\nWhen a model or statistic doesn’t provide a true representation of the population.\n\\[bias=\\mathbb{E}[f'(x)]-f(x)\\]\nBias of the estimated function tells us the capacity of the underlying model to predict the values.\nHigh bias = overly-simplified model, under-fitting, high error on both testing and training data.\n\n\n\n\nBinomial PropertyDefinition\n\n\n\n\n\nThe probability of exactly x successes on n repeated trials in an experiment which has two possible outcomes. \\[P_x={n\\choose x}p^xq^{n-x}\\]\n\n\n\n\n\nC\n\nCategorical DataDefinition\n\n\n\n\n\nData that can be divided into groups or categories such as sex, race, and age.\n\n\n\n\nConditional ProbabilityDefinition\n\n\n\n\n\nThe probability of an event (A) given that another event (B) has already occurred.\n\\[P(A|B)=P(A\\cap B)P(B)\\]\n\n\n\n\nConfusion (Error) MatrixDefinition\n\n\n\n\n\nA technique for summarizing performance measurement for machine learning classification algorithms that makes it easy to see whether the system is confusing classes.\n\n\n\n\n\nD\n\nData ClassificationDefinition\n\n\n\n\n\nOrganizing data by relevant categories according to predefined criteria so that it may be used and protected more efficiently.\n\n\n\n\nData LeakageDefinition\n\n\n\n\n\nWhen information outside the training data is used to create the model.\n\n\n\n\nData ScienceDefinition\n\n\n\n\n\nStudying data to find insight using computer science, mathematics, and statistics.\n\n\n\n\nDatabaseDefinition\n\n\n\n\n\nOrganized collection of data.\n\n\n\n\nDecision TreeDefinition\n\n\n\n\n\nA flowchart that starts with one main idea or question and branches out with potential outcomes of each decision using classification and regression techniques.\n\n\n\n\nDerivativeDefinition\n\n\n\n\n\nRate of change.\n\n\n\n\nDescriptive StatisticsDefinition\n\n\n\n\n\nDescribes features and summaries of data such as mean, and variance.\n\n\n\n\nDeterminateDefinition\n\n\n\n\n\nA scalar function made up of the entries of a square matrix. It is used to find the inverse of a matrix, and has a lot of important properties related to systems of linear equations.\n\\[\\begin{bmatrix} a & b \\\\ c & d\\end{bmatrix}=ad-bc\\]\n\n\n\n\nDimensionality ReductionDefinition\n\n\n\n\n\nThe technique of reducing the amount of random variables (or features) while retaining as much information as possible. This is done to reduce complexity, improve performance, and make the data easier to visualize.\n\n\n\n\nDiscrete MathematicsDefinition\n\n\n\n\n\nMathematics that deals with distinct, separate values instead of continuous values.\n\n\n\n\n\nE\n\nEigenvalueDefinition\n\n\n\n\n\nA scalar that is used to transform an eigenvalue, and considered as a factor by which it is stretched. Often denoted by \\(\\lambda\\).\n\n\n\n\nEigenvectorDefinition\n\n\n\n\n\nAre non-zero vectors that do not change direction when any linear transformation is applied.\n\n\n\n\n\nG\n\nGamma FunctionDefinition\n\n\n\n\n\nA generalization of the factorial function, it is commonly used to estimate new data points based on known values.\n\\[\\Gamma(x)=(n-1)!\\]\n\\[\\Gamma(z)=\\int_0^\\infty t^{z-1}e^{-t}dt\\]\n\n\n\n\n\nH\n\nHomogenousDefinition\n\n\n\n\n\nAn equation that contains itself, or one of its derivatives.\n\\[f(zx,zy)=z^n f(x,y)\\]\n\n\n\n\nHypothesis TestingDefinition\n\n\n\n\n\nTesting a hypothesis and comparing it against the null.\n\n\n\n\n\nI\n\nInferential StatisticsDefinition\n\n\n\n\n\nUsed to make predictions about population or data.\n\n\n\n\nInterval DataDefinition\n\n\n\n\n\nData that is measured along a scale, where each point is placed at equal distance from one another. Examples would be temperature, or SAT scores.\n\n\n\n\n\nK\n\nK-Mean ClusteringDefinition\n\n\n\n\n\nAn unsupervised learning algorithm which groups an unlabeled dataset into clusters with similar properties such as mean. An example might be to group similar customers and then to target them using different types of marketing.\n\n\n\n\n\nL\n\nLaw of Large NumbersDefinition\n\n\n\n\n\nAs the sample size increases the mean gets closer to the average of the population.\n\n\n\n\nLinear RegressionDefinition\n\n\n\n\n\nUses a liner approach to modeling the relationship between regressor (predictor) variables \\(x\\) and a response variable \\(y\\).\n\\[y=\\beta_0+\\beta_1x+\\epsilon\\]\n\\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_nx_n+\\epsilon\\]\n\n\n\n\n\nM\n\nMachine LearningDefinition\n\n\n\n\n\nA method that uses algorithms to build models to make predictions or decisions.\n\n\n\n\nMatrixDefinition\n\n\n\n\n\nA rectangular array of numbers arranged in rows and columns which represent a mathematical expression.\n\n\n\n\n\nN\n\nNeural NetworkDefinition\n\n\n\n\n\nA type of artificial intelligence that uses connected nodes which loosely model the neurons in the brain. Each node, also known as a neuron, is connected by what is called an edge. Both neurons and edges have a weight that adjusts as the model learns, and can increase or decrease the strength of the signal which travels from the first layer (input) to the last layer (output).\n\n\n\nA neural network showing nodes connected by edges, an input layer, hidden layers, and an output layer.\n\n\n\n\n\n\nNominal DataDefinition\n\n\n\n\n\nIs categorical data that groups variables into labeled categories that do not overlap, and cannot be ranked. Nominal data needs to be grouped to be analyzed. Examples would be sex or race.\n\n\n\n\n\nO\n\nOrdinal DataDefinition\n\n\n\n\n\nIs categorical data that has an order or ranking system such as education level, economic status, or satisfaction rating.\n\n\n\n\nOverfittingDefinition\n\n\n\n\n\nWhen machine learning models fit exactly to the training model, and therefore may fail to predict future observations.\n\n\n\n\n\n\nR\n\nRandom Forest ModelDefinition\n\n\n\n\n\nA classification algorithm that consists of many decision trees, and can correct decision trees’ habit of overfitting to their training set.\n\n\n\n\nRatio DataDefinition\n\n\n\n\n\nIs quantitative data that has a true zero such as speed, age, and weight.\n\n\n\n\nRegularizationDefinition\n\n\n\n\n\nA technique to reduce the errors of overfitting by adding extra information.\n\n\n\n\n\nS\n\nSnowflake and Start SchemeDefinition\n\n\n\n\n\nBoth are logical arrangements of a multidimensional database, where the fact table is in the middle of the structure, and it is surrounded by dimension tables. A snowflake scheme has normalized dimension tables meaning there are sub-dimensional tables, whereas a star schema is denormalized and easier to query since there are fewer joins between tables.\n\n\nStar Schema\n\n\nSnowflake Schema\n\n\n\n\n\n\n\nStationary ProcessDefinition\n\n\n\n\n\nA type of stochastic (random) process whose joint probability distribution does not change over time. An example would be white noise.\n\n\n\n\nStatisticsDefinition\n\n\n\n\n\nApplied math used to study data to form a judgment in a case of real world applications.\n\n\n\n\nSystems of Linear EquationsDefinition\n\n\n\n\n\nTwo or more linear equations working together.\n\n\n\n\n\nT\n\nTraining and Test DataDefinition\n\n\n\n\n\nTraining data is a subset of the original data which is used to train machine learning models.\nTest data is another subset of the original data which is independent of the training data, and used to test the accuracy of the model.\n\n\n\n\nTransformationDefinition\n\n\n\n\n\nA linear mapping between two vector spaces that preserves the operations of vector addition and scalar multiplication.\n\n\n\n\nType 1 and Type 2 ErrorDefinition\n\n\n\n\n\nType 1 error is a false positive (rejects the null which is actually true), and a type 2 error is a false negative (fails to reject the null which is actually false).\n\n\n\nLeft image reads “Type 1 error (false positive)”, and shows a doctor telling a man he is pregnant. Right image reads “Type 2 error (false negative)”, and shows a doctor telling a pregnant patient “You’re not pregnant”.\n\n\n\n\n\n\n\nU\n\nUneven or Unbalanced DataDefinition\n\n\n\n\n\nWhen the target variable has more observations in a specific class than the others. It would not be a good idea to use accuracy as a performance measure for highly imbalanced data."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240202-Ready4R-skimr/index.html",
    "href": "blog/Technical-Blog/posts/20240202-Ready4R-skimr/index.html",
    "title": "Ready4R: skimr Package",
    "section": "",
    "text": "The inaugural installment of Ready4R focuses on the skimr package, maintained by rOpenSci.\n\n\n\nSummary table of numeric values of cereals.\n\n\n\nIntroduction\nReady4R is a mailing list offering a free online course initiated by local Oregonian Ted Laderas to impart foundational knowledge in rstats and the tidyverse. Subscribers receive a weekly email delving into various methods for data exploration and analysis. On a weekly basis, I will look into these examples, providing additional insights based on my own experiences.\n\n\nSkim You Data\nThe inaugural installment focuses on the skimr package, maintained by rOpenSci.Ted emphasizes its usefulness with new datasets to grasp the broader picture. In the following code snippet, I install the skimr package and load all necessary data manipulation packages for this tutorial.\n\n\nShow code\n# Install Package\n# install.packages(\"skimr\")\n# Load Packages\nlibrary(skimr)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(janitor)\nlibrary(ggplot2)\n\n\nNext, we proceed to obtain the dataset, which is the Kaggle 80 Cerals. I am using a slightly modified version compared to Ted’s, necessitating adjustments to the code below.\n\n\nShow code\n# Load Data\ncereals &lt;- readr::read_csv(\"../../../../data/cereal.csv\") |&gt;\n  # clean names by converting to lowercase, replace spaces \n  # with underscore and removes special characters\n  janitor::clean_names() |&gt;\n  # make shelf an ordered factor\n  dplyr::mutate(shelf = factor(shelf, ordered = TRUE)) |&gt;\n  # convert mfr and type columns to categorical data\n  dplyr::mutate(across(c(\"mfr\", \"type\"), as.factor))\n\n\n\nOverall Summary\nBefore getting into the skimr package, let’s start with a traditional summary of the dataset:\n\n\nShow code\nsummary(cereals)\n\n\n     name           mfr    type      calories        protein     \n Length:77          A: 1   C:74   Min.   : 50.0   Min.   :1.000  \n Class :character   G:22   H: 3   1st Qu.:100.0   1st Qu.:2.000  \n Mode  :character   K:23          Median :110.0   Median :3.000  \n                    N: 6          Mean   :106.9   Mean   :2.545  \n                    P: 9          3rd Qu.:110.0   3rd Qu.:3.000  \n                    Q: 8          Max.   :160.0   Max.   :6.000  \n                    R: 8                                         \n      fat            sodium          fiber            carbo     \n Min.   :0.000   Min.   :  0.0   Min.   : 0.000   Min.   :-1.0  \n 1st Qu.:0.000   1st Qu.:130.0   1st Qu.: 1.000   1st Qu.:12.0  \n Median :1.000   Median :180.0   Median : 2.000   Median :14.0  \n Mean   :1.013   Mean   :159.7   Mean   : 2.152   Mean   :14.6  \n 3rd Qu.:2.000   3rd Qu.:210.0   3rd Qu.: 3.000   3rd Qu.:17.0  \n Max.   :5.000   Max.   :320.0   Max.   :14.000   Max.   :23.0  \n                                                                \n     sugars           potass          vitamins      shelf      weight    \n Min.   :-1.000   Min.   : -1.00   Min.   :  0.00   1:20   Min.   :0.50  \n 1st Qu.: 3.000   1st Qu.: 40.00   1st Qu.: 25.00   2:21   1st Qu.:1.00  \n Median : 7.000   Median : 90.00   Median : 25.00   3:36   Median :1.00  \n Mean   : 6.922   Mean   : 96.08   Mean   : 28.25          Mean   :1.03  \n 3rd Qu.:11.000   3rd Qu.:120.00   3rd Qu.: 25.00          3rd Qu.:1.00  \n Max.   :15.000   Max.   :330.00   Max.   :100.00          Max.   :1.50  \n                                                                         \n      cups           rating     \n Min.   :0.250   Min.   :18.04  \n 1st Qu.:0.670   1st Qu.:33.17  \n Median :0.750   Median :40.40  \n Mean   :0.821   Mean   :42.67  \n 3rd Qu.:1.000   3rd Qu.:50.83  \n Max.   :1.500   Max.   :93.70  \n                                \n\n\nThe above summary provides a wealth of information. For columns designated as factors (mfr, type, and shelf), we observe counts for each category. Other column types display quantiles.\nNow, let’s utilize skimr::skim to generate a condensed summary of the dataset:\n\n\nShow code\nskim_output &lt;- skimr::skim(cereals)\nsummary(skim_output)\n\n\n\nData summary\n\n\nName\ncereals\n\n\nNumber of rows\n77\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n3\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nThis condensed summary, as Ted aptly notes, offers a more succinct overview. Discrepancies in variable counts by type often signal the need for variable transformation. Personally, I find this approach invaluable, especially when dealing with datasets containing numerous columns. Quickly verifying high-level assumptions can significantly streamline the analysis process. Let’s explore different types of summaries.\n\n\nCharacter Summary\nBelow, we validate our assumptions:\n\nThere is only one character column, “name”.\nThere are 77 unique rows, aligning with our assumption that each row represents a different cereal.\n\n\n\nShow code\nskimr::yank(skim_output, \"character\")\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1\n3\n38\n0\n77\n0\n\n\n\n\n\n\n\nFactor Summary\nBelow, we confirm our assumptions:\n\nThere are three factor columns: mfr, type, and shelf.\nNo missing values are present.\nShelf is the only ordered factor.\nThe dataset comprises seven manufacturers, two types of cereal (cold and hot), and three shelf heights (1: floor, 2: middle, 3: top).\n\n\n\nShow code\nskimr::yank(skim_output, \"factor\")\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nmfr\n0\n1\nFALSE\n7\nK: 23, G: 22, P: 9, Q: 8\n\n\ntype\n0\n1\nFALSE\n2\nC: 74, H: 3\n\n\nshelf\n0\n1\nTRUE\n3\n3: 36, 2: 21, 1: 20\n\n\n\n\n\n\n\nNumeric Summary\nLastly, we examine the numeric summary, which provides information similar to the traditional summary. However, it also includes histograms, offering additional insights.\n\n\nShow code\nskimr::yank(skim_output, \"numeric\")\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncalories\n0\n1\n106.88\n19.48\n50.00\n100.00\n110.00\n110.00\n160.0\n▁▂▇▂▁\n\n\nprotein\n0\n1\n2.55\n1.09\n1.00\n2.00\n3.00\n3.00\n6.0\n▇▆▂▁▁\n\n\nfat\n0\n1\n1.01\n1.01\n0.00\n0.00\n1.00\n2.00\n5.0\n▇▂▁▁▁\n\n\nsodium\n0\n1\n159.68\n83.83\n0.00\n130.00\n180.00\n210.00\n320.0\n▃▂▇▇▂\n\n\nfiber\n0\n1\n2.15\n2.38\n0.00\n1.00\n2.00\n3.00\n14.0\n▇▃▁▁▁\n\n\ncarbo\n0\n1\n14.60\n4.28\n-1.00\n12.00\n14.00\n17.00\n23.0\n▁▁▆▇▃\n\n\nsugars\n0\n1\n6.92\n4.44\n-1.00\n3.00\n7.00\n11.00\n15.0\n▅▇▇▆▇\n\n\npotass\n0\n1\n96.08\n71.29\n-1.00\n40.00\n90.00\n120.00\n330.0\n▇▇▂▁▁\n\n\nvitamins\n0\n1\n28.25\n22.34\n0.00\n25.00\n25.00\n25.00\n100.0\n▁▇▁▁▁\n\n\nweight\n0\n1\n1.03\n0.15\n0.50\n1.00\n1.00\n1.00\n1.5\n▁▁▇▁▁\n\n\ncups\n0\n1\n0.82\n0.23\n0.25\n0.67\n0.75\n1.00\n1.5\n▂▇▇▁▁\n\n\nrating\n0\n1\n42.67\n14.05\n18.04\n33.17\n40.40\n50.83\n93.7\n▅▇▅▁▁\n\n\n\n\n\n\n\nOverall\nExploring this has been enlightening, and I anticipate revisiting it. It’s surprising how frequently I find myself explaining to non-data professionals that the mean (or average) isn’t always the most reliable indicator of sample behavior. Data can be heavily skewed, making visualizations essential for accurate interpretation. While tools like ggplot offer sophisticated visualization options, the initial data review provided by skimr is invaluable."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220228-Workshop-RStudioBuildingaBlogwithR/index.html",
    "href": "blog/Technical-Blog/posts/20220228-Workshop-RStudioBuildingaBlogwithR/index.html",
    "title": "RStudio: Building a Blog with R",
    "section": "",
    "text": "These are my notes from Posit’s (formerly R Studio) “Building a Blog with R” tutorial, which was presented by Isabella Velasquez on January 25th, 2022. Materials for this presentation are available on Github. Check out Posit meet-up for more from Posit.\n\n\n1. About Isabella Velesquez\nEmail: isabella.velasquez@rstudio.com\n\nWorks at Posit.\nSeattle Lady Co-Organizer.\nFirst R-Ladies talk in 2018.\n\n\n\n2. Agenda\n\nWhy create a blog?\nDeciding on a topic.\nTools for building a blog.\n\n\n\n3. Why create a blog?\n\nWhen you’re given the same advice 3 times, write a blog post.\nShare what you’ve learned.\nWrite your opinions.\nShare updates, and news.\nExternal blogs (for business)\n\nPosit has 4 different external blogs:\n\nThe Posit Blog\nPosit AI Blog\nTidyverse\nR Views\n\n\nInternal blogs (for business)\n\nShare information more easily and effectively.\nImprove collaboration.\nServing as a bulletin board for projects.\n\n\n\n\n4. Types of Posts\n\nStandard lists\nHow To’s / tutorials\nNew posts\nProblem - and - solution\nFAQ\nCheat sheets\nChecklists\nInfo graphics\nPresentations\nDebates\nInspiration\nInterviews\n\n\n\n5. Seperating Posts\n\nTutorials (learning oriented)\nHow To’s (task oriented)\nExplanation (understanding oriented)\nReference (information oriented)\n\n\n\n6. Building a Blog with R\n\nKnowledge of R and R Markdown.\nVersion Control (Github)\nNetlify\n\n\n\n7. Overall Thoughts\nWhile this was called “Building a blog”, there wasn’t a lot of blog building. It was very business and product information heavy.\n\n\n8. Recommended Blog (from chat)\nMachine Learning Mastery"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230425-TidyTuesday-LondonMarathon/index.html",
    "href": "blog/Technical-Blog/posts/20230425-TidyTuesday-LondonMarathon/index.html",
    "title": "Week 17 Tidy Tuesday: London Marathon",
    "section": "",
    "text": "In this week’s #TidyTuesday submission I delve into London Marathon data, unveiling the number of winners by nationality throughout the marathon’s history.\n\nLondon Marathon\nThe data this week comes from Nicola Rennie’s LondonMarathon R package. This is an R package containing two data sets scraped from Wikipedia (1 November 2022) on London Marathon winners, and some general data. How the dataset was created, and some analysis, is described in Nicola’s post “Scraping London Marathon data with {rvest}”. Thank you for putting this dataset together @nrennie!\n\n\n\nCode\nThis week I wanted to add flags for each nationality, so two more functions are added to this weeks workflow:\n\nflags(): which reads in each flags .pgn, and saves them as a list.\nadd_flag(): reads in the list of flags, and overlays the images on the graph.\n\nAdditionally are the three key functions: cleaning, visualizing, and styling, as well as the index.R file which integrates data loading, function execution, and the generation of of a data visualization.\n\nCleaningVisualizingStylingFlagsAdd Flagsindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\nlibrary(dplyr)\nlibrary(magrittr)\n\n#### Cleaning Function ####\nclean &lt;- function(data){\n  clean_data &lt;- data %&gt;%\n    dplyr::group_by(Nationality) %&gt;%\n    dplyr::summarise(\n      nat_winners = dplyr::n()) %&gt;%\n    dplyr::arrange(\n      dplyr::desc(nat_winners))\n  return(clean_data)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nlibrary(ggplot2)\n\n#### Load Colors ####\ngeom_bar_color &lt;- \"#DD733A\"\n\n#### Visual Function ####\nvis &lt;- function(df){\n  vis &lt;- ggplot2::ggplot(\n    df,\n    ggplot2::aes(\n      x = nat_winners,\n      y = stats::reorder(\n        Nationality, \n        nat_winners))) +\n    ggplot2::geom_bar(\n      stat = \"identity\",\n      fill = geom_bar_color)\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nlibrary(ggplot2)\n\n#### Load Colors ####\nbackground_color &lt;- \"#596D78\"\ntitle_color &lt;- \"#ABB3B8\"\ngraph_color &lt;- \"#ABB3B8\"\nbonus_1 &lt;- \"#4F555B\"\nbonus_2 &lt;- \"#B4AE59\"\n\n#### Style Function ####\nstyle &lt;- function(vis){\n  sty &lt;- vis +\n    # labels\n    ggplot2::labs(\n      title = \"Nationality of London Marathon Winners\",\n      subtitle = \"\",\n      caption = \"Graph by Randi Bolt \\n Data from #TidyTuesday\",\n      x = \"Number of Winners\",\n      y = \"\") +\n    # values next to bars\n    ggplot2::geom_text(\n      ggplot2::aes(label = nat_winners,\n                   hjust = 1.2)) +\n    ggplot2::theme(\n      # title\n      plot.title = element_text(\n        size = 18,\n        face = \"bold\",\n        color = title_color,\n        hjust = .5),\n      # caption \n      plot.caption = element_text(\n        color = title_color),\n      # graph background\n      panel.background = element_rect(\n        fill = graph_color),\n      # x and y axis labels\n      axis.title = element_text(\n        size = 14, \n        color = title_color),\n      # axis ticks text\n      axis.text = element_text(\n        siz = 10,\n        color = title_color),\n      # background\n      plot.backgroun = ggplot2::element_rect(\n        fill = background_color\n      )\n    )\n  return(sty)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(png)\n\n#### Load Flags ####\nuk &lt;-  readPNG(\"flags/uk.png\")   \nkenya &lt;-  readPNG(\"flags/kenya.png\") \nusa &lt;-  readPNG(\"flags/usa.png\")\nswi &lt;-  readPNG(\"flags/switzerland.png\")\nethi &lt;-  readPNG(\"flags/ethiopia.png\")\nnor &lt;-  readPNG(\"flags/norway.png\")\nire &lt;-  readPNG(\"flags/ireland.png\")\nmex &lt;-  readPNG(\"flags/mexico.png\")\ngermany &lt;-  readPNG(\"flags/germany.png\") \nport &lt;-  readPNG(\"flags/portugal.png\")\nitaly &lt;-  readPNG(\"flags/italy.png\")\ncan &lt;-  readPNG(\"flags/canada.png\")\nsweden &lt;-  readPNG(\"flags/sweden.png\")\njapan &lt;-  readPNG(\"flags/japan.png\")\nfran &lt;-  readPNG(\"flags/france.png\")\ndenmark &lt;-  readPNG(\"flags/denmark.png\")\naus &lt;-  readPNG(\"flags/australia.png\")\npol &lt;- readPNG(\"flags/poland.png\")\nneth &lt;- readPNG(\"flags/netherlands.png\")\nmor &lt;- readPNG(\"flags/morocco.png\")\nspain &lt;- readPNG(\"flags/spain.png\")\nsu &lt;- readPNG(\"flags/soviet_union.png\")\nchina &lt;- readPNG(\"flags/china.png\")\nbel &lt;- readPNG(\"flags/belgium.png\")\n\n#### Flag List ####\nflag_list &lt;- list(\n  uk, kenya, usa,\n  swi, ethi, nor,\n  ire, mex, germany,\n  port, italy, can,\n  sweden, japan, fran,\n  denmark, aus, pol,\n  neth, mor, spain,\n  su, china, bel)\n\n#### Save Flags ####\nsaveRDS(flag_list, file=\"data/flag_list.RData\")\n\n\n\n\n\n\nShow Code\n#### Load Packages #### \nlibrary(ggplot2)\nlibrary(grid)\n\n#### Load Data ####\nflags_list &lt;- readRDS(\"data/flag_list.RData\")\n\n#### Add Flags ####\nadd_flags &lt;- function(styled_vis){\n  for (i in 1:length(flags_list)){\n    flag_new &lt;- styled_vis + \n      # add static annotation\n      ggplot2::annotation_custom(\n        # cycles through list and stacking flags on graph\n        grid::rasterGrob(\n          flags_list[[i]], \n          width=1, height=1),\n        xmin = -2, xmax = -.1,\n        ymin = 24.55-i, ymax = 25.45-i)\n    styled_vis &lt;- flag_new\n  }\n  return(flag_new)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(tidyverse) \nbase::library(png)\n#### Load Data ####\nremotes::install_github(\"nrennie/LondonMarathon\")\ndata(winners, package = \"LondonMarathon\")\n\n#### Clean Data ####\ndata &lt;- clean(winners)\n\n#### Create Data Visual ####\ndata_vis &lt;- vis(data)\n\n#### Style Data Visual ####\nsty_vis &lt;- style(data_vis)\n\n#### Add Flags ####\nsty_vis_with_flags &lt;- add_flags(sty_vis)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\")\n\n\n\n\nWeek 17 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nDownload images from wikimedia with utils::download.file(url = \"\", destfile = \"\", mode = \"wb).\nAdd images to data visual with ggplot2::annotation_custion() and grid::rasterGrob.\nggplot2::labs() reads markdown such as newline, \\n."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220124-Original-LatexHacks/index.html",
    "href": "blog/Technical-Blog/posts/20220124-Original-LatexHacks/index.html",
    "title": "Latex",
    "section": "",
    "text": "Here is some short and simple latex.\n\n\nBasic Symbols :\n\n\\(\\sim\\) : \\sim\n\\(\\circ\\) : \\circ\n\\(\\square\\) : \\square\n\\(\\equiv\\) : \\equiv\n\\(\\cong\\) : \\cong\n\\(\\unlhd\\) : \\unlhd\n\\(\\div\\) : \\div\n\\(\\nless\\) : \\nless\n\\(\\ngtr\\) : ngtr\n\\(\\emptyset\\) : \\emptyset\n\\(\\subseteq\\) : \\subseteq\n\\(a\\choose b\\) : a\\choose b\n\\(\\underset{i\\in I}U\\) : \\underset{i\\in I}U\n\\(\\Leftrightarrow\\) : \\Leftrightarrow\n\\(\\langle\\rangle\\) : \\langle\\rangle\n\\(\\overrightarrow{\\rm AB}\\) : \\overrightarrow{\\rm AB}\n\\(\\underline{\\text{Underline Text}}\\) : \\underline{\\text{Underline Text}}\n\\(\\mathbb{R}\\) : \\mathbb{R}\n\nbb : blackboard bold\n\n\\(\\mathcal{F}\\) : \\mathcal{F}\n\\(\\mathscr{F}\\) : \\mathscr{F}\n\n\n\nGreek :\n\n\\(\\tau\\) : \\tau\n\\(\\rho\\) : \\rho\n\\(\\alpha\\) : \\alpha\n\\(\\beta\\) : \\beta\n\\(\\Gamma\\) : \\Gamma\n\\(\\epsilon\\) : \\epsilon\n\\(\\mathcal{E}\\) : \\mathcal{E}\n\\(\\varepsilon\\) : \\varepsilon\n\\(\\varphi\\) : \\varphi\n\n\n\nInline :\nLimits above and below sums and integrals\n\n\\(\\sum\\limits_{n}^{i}\\int_0^1\\) : \\limits\n\nMatrices and Matrix Equations\n\n\\(\\begin{smallmatrix} 1 & 0 \\\\ 0 & 1\\end{smallmatrix}\\) : \\begin{smallmatrix} 1 & 0 \\\\ 0 & 1\\end{smallmatrix}\n\n\\(I=[\\begin{smallmatrix} 1 & 0 \\\\ 0 & 1\\end{smallmatrix}]\\)\n\\((\\begin{smallmatrix} 1 & 1 \\\\ 1 & 0\\end{smallmatrix})(\\begin{smallmatrix} 1 & 1 \\\\ 0 & 1\\end{smallmatrix})\\ne(\\begin{smallmatrix} 1 & 1 \\\\ 0 & 1\\end{smallmatrix})(\\begin{smallmatrix} 1 & 1 \\\\ 1 & 0\\end{smallmatrix})\\)\n\n\n\n\nMultiple Lines :\nFunction\n\n\\(F(x)=\\begin{cases}1 & x\\geq 0\\\\0 & \\text{otherwise}\\end{cases}\\) : F(x)=\\begin{cases} . . . \\end{cases}\n\n1 & x \\geq 0 \\\\\n0 & \\text{otherwise}\n\n\nMatrix\n\n\\(F(x)=\\begin{bmatrix}1\\\\2\\\\3\\\\4\\\\5\\\\6\\\\\\end{bmatrix}\\) : F(x)=\\begin{bmatrix}1\\\\2\\\\3\\\\4\\\\5\\\\6\\\\\\end{bmatrix}\n\nSeries of Equalities\n\n\\(\\begin{equation}\\label{a}\\begin{split}x &= a+b+c\\\\&=1+2+3\\\\&=6\\end{split}\\end{equation}\\)\n\n: \\begin{equation}\\{label}\\begin{split}... \\end{split}\\end{equation}\n\nx &= a+b+c \\\\\n&= 1+2+3\n&= 6\n\n\n\nPotential Errors :\nSpelling\n\nEx: \\overlien{AB} should be \\overline{AB}\n\nlabel , table\n\n\nMore than two backslashes\n\nEx: Equation will work but \\end{equation} will show at the end. One of the lines has more than two backslashes at the end of at least one of the lines.\n\nSpace before final $\n\nEx: $\\angle ABC $ should be $\\angle ABC$\n\nMore $ on one side of equation than the other\n\nEx: $A^2+B^2=C^2$$ should be $A^2+B^2=C^2$\n\nClosing {}\n\nEx: $\\int\\limits_{1}^{2$ should be $\\int\\limits_{1}^{2}$\n\nUnderset on the wrong side\n\nEx: U\\underset{i\\in I} should be \\underset{i\\in I}U\n\n\n\nAdvice :\nDetextify\n\nIf you don’t know what a symbol is, draw it in Detextify here.\n\nGoogle Docs Equations Boxes\n\nMost latex backslashes work in google doc’s equation boxes. If I have to do a “quick” homework, and dont want to spend a lot of time formatting a pdf in R, I will use google docs and latex in the equations boxes.\n\nNote: This is how I began learning latex.\nWhy learn Latex?\n\nGenerally speaking it looks nicer, especially on reports, projects, and presentations.\nA lot of my peers and professors who wrote math by hand would have problems in their dominate writing hand. Typing math with latex helps spread that tension out to two hands.\nIt saves time in the long run, since updating a line in a typed document is a lot easier than re-writing an entire problem by hand. Not to mention the ability to use copy paste.\n\n\n\nBonus:\nColors\n\n\\(\\color{red}\\text{colored text}\\) : \\color{red}\\text{colored text}"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230609-Workshop-PythonCommands/index.html",
    "href": "blog/Technical-Blog/posts/20230609-Workshop-PythonCommands/index.html",
    "title": "Basic Python Commands for Virtual Environments and Package Management",
    "section": "",
    "text": "In this blog post, we will explore basic Python commands for managing virtual environments and installing packages. These commands will help you set up isolated environments for your Python projects, ensuring clean and organized development.\n\n\nIntroduction\nThe information provided here is based on the YouTube video titled “Python Tutorial: VENV (Mac & Linux) - How to Use Virtual Environments with the Built-In venv Module” by Corey Schafer.\nNote: Remember not to commit the virtual environment (venv) and include it in the .gitignore file. However, do commit the requirements.txt file for package version control.\nLet’s dive into the essential Python commands:\n\n\n1. Checking Installed Packages in the Global Environment\nTo view the list of packages installed on your machine in the global environment, execute the following command in your terminal:\n\npip3 list\n\n\n\n2. Creating a Virtual Environment\nTo create a virtual environment, use the following command:\n\npython3 -m venv project_name\n\n\n\n3. Creating a Virtual Environment in a Project Folder\nIt’s important to avoid placing your project files within the virtual environment. Instead, create the virtual environment in a project folder by executing the following command:\n\npython3 -m venv project_folder/project_name\n\n\n\n4. Activating the Virtual Environment\nTo activate the virtual environment, run the appropriate command based on your operating system:\n\nactivate virtual enviroment\n\n\n\n5. Checking the Python Version in the Virtual Environment\nTo determine the Python version being used within the virtual environment, use the following command:\n\nwhich python\n\n\n\n6. Checking Installed Packages in the Virtual Environment\nTo view the packages installed within the virtual environment, execute the following command:\n\npip list\n\n\n\n7. Installing Packages in the Virtual Environment\nTo install packages within the virtual environment, use the following command:\n\npip install package_name\n\n\n\n8. Creating a File of Packages and Versions\nTo create a file containing a list of installed packages and their versions, use the following command:\n\npip freeze &gt; requirements.txt\n\n\n\n9. Installing Packages from a Requirements File\nTo install packages from a requirements file, execute the following command:\n\npip install -r requirements.txt\n\n\n\n10. Deactivating the Virtual Environment\nTo deactivate the virtual environment and return to the global environment, use the following command:\n\ndeactivate \n\n\n\nConclusion\nThese basic Python commands provide a solid foundation for managing virtual environments and package installations. By utilizing virtual environments, you can isolate project dependencies and ensure consistent and reproducible development environments. Remember to create and commit a requirements.txt file for version control of your project’s dependencies, while excluding the virtual environment itself from version control."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240212-Ready4R-crosstables/index.html",
    "href": "blog/Technical-Blog/posts/20240212-Ready4R-crosstables/index.html",
    "title": "Ready4R: Crosstables",
    "section": "",
    "text": "This week the Ready4R mailing list focus on crosstables, also known as two-way tables.\n\n\n\nA crosstable of manufacturers cereals with mascots.\n\n\n\nIntroduction\nReady4R is a mailing list offering a free online course initiated by local Oregonian Ted Laderas to impart foundational knowledge in rstats and the tidyverse. Subscribers receive a weekly email delving into various methods for data exploration and analysis. On a weekly basis, I will look into these examples, providing additional insights based on my own experiences.\n\n\nCrosstables\nCrosstables are a helpful way to compare the results of two or more variables in a table, so that we can start asking questions about how they relate. We start this week using the same cereals dataset, but slightly modifying it.\n\n\nShow code\n# Install Package\n# Load Packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(janitor)\n# Load Data\ncereals &lt;- readr::read_csv(\"../../../../data/cereal.csv\") |&gt;\n  # clean names by converting to lowercase, replace spaces \n  # with underscore and removes special characters\n  janitor::clean_names() |&gt;\n  # make shelf an ordered factor\n  dplyr::mutate(shelf = factor(shelf, ordered = TRUE)) |&gt;\n  # convert mfr and type columns to categorical data\n  dplyr::mutate(across(c(\"mfr\", \"type\"), as.factor))\n# Rename Labels\nmanu_labels &lt;- c(\"American Home\"=\"A\",\n                   \"General Mills\"=\"G\",\n                   \"Kelloggs\"=\"K\",\n                   \"Nabisco\" = \"N\",\n                   \"Post\" = \"P\",\n                   \"Quaker Oats\" = \"Q\", \n                   \"Ralston Purina\" = \"R\")\ncereals &lt;- cereals |&gt;\n  dplyr::mutate(mfr = forcats::fct_recode(mfr, !!!manu_labels))\n\n\n\n\nCrosstabs with janitor::tabyl()\nFor a single variable we can use janitor::tabyl() to view counts and percentages, as shown below:\n\n\nShow code\ncereals |&gt;\n  janitor::tabyl(shelf) \n\n\n shelf  n   percent\n     1 20 0.2597403\n     2 21 0.2727273\n     3 36 0.4675325\n\n\nA slick way to get counts and percentage per shelf (AKA our ordered factor variable). Next we may want to know whether the manufacturers (factor variable) are evenly distributed in terms of cereal type (factor variable). Note: Use adorn_percentages() and adorn_n() interchangeably to get percentages or counts.\n\n\nShow code\ncereals |&gt;\n  # Create frequency table of manufacturers by type\n  janitor::tabyl(mfr, type) |&gt;\n  # Calculate percentages based on hot and cold cereal totals\n  janitor::adorn_percentages(denominator = \"row\") |&gt;\n  # Format percentages\n  janitor::adorn_pct_formatting() |&gt;\n  # Add totals\n  janitor::adorn_ns() |&gt;\n  # Nicely format table\n  knitr::kable()\n\n\n\n\n\nmfr\nC\nH\n\n\n\n\nAmerican Home\n0.0% (0)\n100.0% (1)\n\n\nGeneral Mills\n100.0% (22)\n0.0% (0)\n\n\nKelloggs\n100.0% (23)\n0.0% (0)\n\n\nNabisco\n83.3% (5)\n16.7% (1)\n\n\nPost\n100.0% (9)\n0.0% (0)\n\n\nQuaker Oats\n87.5% (7)\n12.5% (1)\n\n\nRalston Purina\n100.0% (8)\n0.0% (0)\n\n\n\n\n\n\\(\\underline{\\textbf{Insights}}\\)\n\nOnly 1 manufacturer makes hots cereals: American Home\nOnly 2 manufacturers make both hot and cold cereal: Nabisco and Quaker Oats\nThe remaining 4 manufacturers only make cold cereals: General Mills, Kelloggs, Post, and Raiston Purina\n\n\n\nShelf Height and Marketing\nNow let’s look at the distribution of cereals across different shelf heights (1 = bottom, 2 = middle, and 3 = top). According to an article cited by Ted titled Cereal aisle psychology: All eyes on the consumer, researchers conducted a two-part study confirming that cereals targeting children are typically placed about 23 inches off the ground. They observed that cereals on the top shelves often feature characters staring straight ahead or slightly upward to make eye contact with adults, while those on lower shelves, adorned with cartoon characters with large inviting eyes, create eye contact with children.\nLets use crosstables to visualize how manufacturers distribute their cereals across shelves.\n\n\nShow code\ncereals |&gt;\n  # Create frequency table of manufacturers by shelf\n  janitor::tabyl(mfr, shelf) |&gt;\n  # Calculate percentages based on shelf total\n  janitor::adorn_percentages(denominator=\"row\") |&gt;\n  # Format percentages\n  janitor::adorn_pct_formatting() |&gt;\n  # Add totals\n  janitor::adorn_ns() |&gt;\n  # Nicely format table\n  knitr::kable()\n\n\n\n\n\nmfr\n1\n2\n3\n\n\n\n\nAmerican Home\n0.0% (0)\n100.0% (1)\n0.0% (0)\n\n\nGeneral Mills\n27.3% (6)\n31.8% (7)\n40.9% (9)\n\n\nKelloggs\n17.4% (4)\n30.4% (7)\n52.2% (12)\n\n\nNabisco\n50.0% (3)\n33.3% (2)\n16.7% (1)\n\n\nPost\n22.2% (2)\n11.1% (1)\n66.7% (6)\n\n\nQuaker Oats\n12.5% (1)\n37.5% (3)\n50.0% (4)\n\n\nRalston Purina\n50.0% (4)\n0.0% (0)\n50.0% (4)\n\n\n\n\n\n\\(\\underline{\\textbf{Insights}}\\)\n\nAmerican Home exclusively markets its cereals on the second shelf.\nNabisco primarily markets its cereals on the bottom shelf, presumably targeting children.\nGeneral Mills, Kelloggs, Post, and Quaker Oats mainly target adults, as they primarily market their cereals on the top shelf..\nRalston Purina evenly splits its marketing efforts between the top and bottom shelves.\n\n\n\nWhich shelves have cereal mascots?\nNext, let’s investigate which shelves feature cereal mascots, assuming that mascots often indicate cereals marketed towards children. Using data scraped by Ted from the article “268 Cereal Mascots,” we can identify cereals with mascots and their distribution across shelves.\nBelow we clean and join our data sets, and then check the dimmensions are correct.\n\n\nShow code\n# Load Data\nm_c &lt;- utils::read.csv(\"../../../../data/mascots.csv\",\n                       row.names = NULL)\n# Clean cereal names\nm_c &lt;- m_c |&gt;\nmutate(name=stringr::str_replace(name,\"Cap’n Crunch cereals\", \"Cap'n'Crunch\")) |&gt;\n  mutate(name=stringr::str_replace(name, \"Count Chocula cereal\", \"Count Chocula\")) |&gt;\n  mutate(name=stringr::str_replace(name, \"Honey Smacks\", \"Smacks\")) |&gt;\n  mutate(name=stringr::str_replace(name, \"Mini-Wheats\", \"Frosted Mini-Wheats\")) \n# join with cereals data\nmascot_count &lt;- cereals |&gt;\n  left_join(y=m_c, by=\"name\") |&gt;\n  mutate(has_mascot = ifelse(is.na(mascot), \"No\", \"Yes\")) \n# check dimension\ndim(mascot_count)\n\n\n[1] 80 19\n\n\nLet’s do a Quick Check!, and look at the second shelf to see any obvious errors in the data.\n\n\nShow code\nmascot_count |&gt;\n  # Filter to second shelf\n  dplyr::filter(shelf==2) |&gt;\n  # Select only name, manufacturer, mascot, and has_mascot\n  select(name, mfr, mascot, has_mascot) |&gt;\n  # Arrange by mascot \n  arrange(mascot) |&gt;\n  # Nicely format table\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nname\nmfr\nmascot\nhas_mascot\n\n\n\n\nCocoa Puffs\nGeneral Mills\nBuzz the Bee\nYes\n\n\nCap’n’Crunch\nQuaker Oats\nCap’n Crunch (Horatio Magellan Crunch)\nYes\n\n\nCocoa Puffs\nGeneral Mills\nCocoa Puffs’ Sheik of Shake\nYes\n\n\nCorn Pops\nKelloggs\nCornelius (Corny) the Corn\nYes\n\n\nCount Chocula\nGeneral Mills\nCount Chocula\nYes\n\n\nSmacks\nKelloggs\nDig’em Frog\nYes\n\n\nFruity Pebbles\nPost\nFred Flintstone\nYes\n\n\nLucky Charms\nGeneral Mills\nLucky the Leprechaun\nYes\n\n\nFrosted Mini-Wheats\nKelloggs\nMr. Mini-Wheats\nYes\n\n\nCocoa Puffs\nGeneral Mills\nSonny the Cuckoo Bird\nYes\n\n\nRaisin Bran\nKelloggs\nThe Raisin Bran Sun\nYes\n\n\nFroot Loops\nKelloggs\nToucan Sam\nYes\n\n\nTrix\nGeneral Mills\nTrix Rabbit\nYes\n\n\nCinnamon Toast Crunch\nGeneral Mills\nWendell\nYes\n\n\nApple Jacks\nKelloggs\nNA\nNo\n\n\nCream of Wheat (Quick)\nNabisco\nNA\nNo\n\n\nGolden Grahams\nGeneral Mills\nNA\nNo\n\n\nHoney Graham Ohs\nQuaker Oats\nNA\nNo\n\n\nKix\nGeneral Mills\nNA\nNo\n\n\nLife\nQuaker Oats\nNA\nNo\n\n\nMaypo\nAmerican Home\nNA\nNo\n\n\nNut&Honey Crunch\nKelloggs\nNA\nNo\n\n\nStrawberry Fruit Wheats\nNabisco\nNA\nNo\n\n\n\n\n\nTed notes that there are duplicate rows for cereals, because mascots can change over time, or have multiple mascots. This will be problematic for our crosstable which works on counts, so we do a little extra cleaning in the code below.\n\n\nShow code\nmascot_count |&gt;\n  # Select name, shelf, and has_mascot columns\n  select(name, shelf, has_mascot) |&gt;\n  # Remove the duplicate rows\n  distinct() |&gt;\n  # Create crosstable of shelfs by has_mascot\n  janitor::tabyl(shelf, has_mascot) |&gt;\n  # Calculate percentage based on Yes or No to having a mascot\n  janitor::adorn_percentages() |&gt;\n  # Format percentages\n  janitor::adorn_pct_formatting() |&gt;\n  # Add totals\n  janitor::adorn_ns() |&gt;\n  # Nicely format table\n  knitr::kable()\n\n\n\n\n\nshelf\nNo\nYes\n\n\n\n\n1\n60.0% (12)\n40.0% (8)\n\n\n2\n42.9% (9)\n57.1% (12)\n\n\n3\n100.0% (36)\n0.0% (0)\n\n\n\n\n\n\\(\\underline{\\textbf{Insights}}\\)\n\nCereals with mascots are absent from the top shelf, suggesting that cereal manufacturers predominately target children on the first and second shelves.\n\nFinally, let’s identify which manufacturer boasts the highest number of cereals with mascots:\n\n\nShow code\nmascot_count |&gt;\n  # Select name, manufacturer, and has_mascot\n  select(name, mfr, has_mascot) |&gt;\n  # Remove the duplicate rows\n  distinct() |&gt;\n  # Create crosstable of shelfs by has_mascot\n  janitor::tabyl(mfr, has_mascot) |&gt;\n  # Calculate percentages based on column totals\n  janitor::adorn_percentages(denominator = \"col\") |&gt;\n  # Format percentages\n  janitor::adorn_pct_formatting() |&gt;\n  # Add totals\n  janitor::adorn_ns() |&gt;\n  # Nicely format table\n  knitr::kable()\n\n\n\n\n\nmfr\nNo\nYes\n\n\n\n\nAmerican Home\n1.8% (1)\n0.0% (0)\n\n\nGeneral Mills\n24.6% (14)\n40.0% (8)\n\n\nKelloggs\n26.3% (15)\n40.0% (8)\n\n\nNabisco\n10.5% (6)\n0.0% (0)\n\n\nPost\n12.3% (7)\n10.0% (2)\n\n\nQuaker Oats\n12.3% (7)\n5.0% (1)\n\n\nRalston Purina\n12.3% (7)\n5.0% (1)\n\n\n\n\n\n\\(\\underline{\\textbf{Insights}}\\)\n\nGeneral Mills and Kelloggs lead with the most cereals featuring mascots.\nAmerican Home and Nabisco have no cereals with mascots.\n\n\n\nOverall\nCrosstables serves as a powerful tool for exploring relationships within datasets. Through these analyses, we gain valuable insights into how cereal manufacturers strategically position their products, offering a glimpse into the fascinating world of consumer psychology and marketing dynamics.\n\n\nW.E.B. Dubois’ Visualizations (Black History Month)\nBorn in 1868, Du Bois was not only a scholar and activist but also an innovative thinker in the realm of data representation. As the first Black American to earn a doctorate from Harvard, he embarked on a journey through Europe before returning to the United States to focus on social sciences.\nDriven by a fervent desire to elevate the rights and livelihoods of Black people, Du Bois recognized the power of compelling evidence in effecting societal change. However, he understood that mere statistical data alone could not dismantle generations of racial oppression.\nOne of his seminal works emerged during the 1900 Paris Exposition, where Du Bois seized the opportunity to narrate the story of Black Americans through a novel medium: charts. His collection of 63 data visualizations titled “The Exhibit of American Negroes” was divided into two sections. The first, “A Series of Statistical Charts Illustrating the Condition of the Descendants of Former African Slaves Now in Residence in the United States of America,” offered a broad view of the data at national and international levels. The companion piece, “The Georgia Negro,” focused on a singular state, providing a localized perspective.\nDu Bois meticulously organized his charts into three distinct viewpoints—international, national, and local—thereby weaving a comprehensive narrative encompassing African American history, education, business development, and property ownership. Through interconnected data points, he crafted an elaborate story that evolved seamlessly from the local to the international arena, reinforcing the narrative of Black empowerment and resilience.\n\\(\\underline{\\textbf{Articles}}\\)\n\nHow W.E.B. Du Bois used data visualization to confront prejudice in the early 20th century by Jason Forrest\nW.E.B. Du Bois Portrait Gallary by Chimdi Niwosu"
  },
  {
    "objectID": "blog/Personal-Blog/posts/2024_01-MLK_Storm/index.html",
    "href": "blog/Personal-Blog/posts/2024_01-MLK_Storm/index.html",
    "title": "2024 January Winter Storm",
    "section": "",
    "text": "In January 2024, Oregon was battered by a formidable storm, leaving a lasting imprint on our community. Months later, the scars of destruction still linger in our neighborhood, evident in the tarps draped over homes awaiting repair from the trees that crashed through them."
  },
  {
    "objectID": "blog/Personal-Blog/posts/2024_01-MLK_Storm/index.html#timeline-of-events",
    "href": "blog/Personal-Blog/posts/2024_01-MLK_Storm/index.html#timeline-of-events",
    "title": "2024 January Winter Storm",
    "section": "Timeline of Events",
    "text": "Timeline of Events\nFriday, Jan. 12th\nA winter storm watch for the weekend warned of cold weather and snow. Despite no snowfall, we erred on the side of caution, rescheduling plans for my partner Tanner’s family birthday dinner.\nSaturday Jan. 13th\nThe day started calmly but turned tumultuous as winds picked up in the afternoon, leading to a power outage by 3 PM. Seeking refuge at our local watering hole, we waited for an update from PGE that wouldn’t come for a few days. Enduring the biting cold again, we walked home, and huddled under blankets with our beloved cat, Lulu (as seen in the picture above).\nSunday Jan. 14th\nWith no updates from the power company, our day was spent navigating the cold reality of a powerless home. Concern for Lulu’s well-being prompted makeshift measures to retain heat, including burning old furniture and fortifying our living room against the chill. The fire went out after midnight, but the room still retained heat more than the other rooms in the house.\nMonday Jan. 15th\nAs another storm loomed on the horizon, we made the decision to seek shelter at Tanner’s mom’s house, braving treacherous roads with our faithful companion, Lulu. The drive marked the first communication from the power company for power to return 10PM Tuesday.\nTuesday Jan. 16th\nDespite the icy roads and uncertainty, we received a belated message of restored power at midnight. A modest birthday gift for Tanner amidst the challenges.\nWednesday Jan. 17th\nEager to return home, we cautiously navigated icy roads, only to be greeted by the aftermath of a flooded bathroom upon arrival. Despite intermittent power, we resolved to stay put, improvising with limited resources. PGE says power should be back on tomorrow night at 10PM.\nThursday Jan. 18th\nTanner ventured out in search of firewood checking multiple locations, only to find some at Lowes. Power was out all day, and did not return until 8pm.\nFriday Jan. 19th\nPower was on for most of the day, but went out twice in the later afternoon. Roads are icy, and un-drivable.\nSaturday - Sunday\nPower stayed on, but roads and sidewalks were still icy."
  },
  {
    "objectID": "blog/Personal-Blog/posts/2024_01-MLK_Storm/index.html#prepared-checklist",
    "href": "blog/Personal-Blog/posts/2024_01-MLK_Storm/index.html#prepared-checklist",
    "title": "2024 January Winter Storm",
    "section": "Prepared Checklist",
    "text": "Prepared Checklist\n\nTurn off water lines.\nCheck travel chargers are fully charged.\nEssential electronics are fully charged.\nSpare batteries.\nSpare candles.\nRemove water from glass.\nPrepare overnight bag for quick exits."
  },
  {
    "objectID": "blog/Personal-Blog/posts/2024_01-MLK_Storm/index.html#bonus-items",
    "href": "blog/Personal-Blog/posts/2024_01-MLK_Storm/index.html#bonus-items",
    "title": "2024 January Winter Storm",
    "section": "Bonus Items",
    "text": "Bonus Items\n\nCostco generator.\nBatteries for headlamps.\nSpare firewood."
  },
  {
    "objectID": "blog/Personal-Blog/posts/2024_01-MLK_Storm/index.html#additional-articles",
    "href": "blog/Personal-Blog/posts/2024_01-MLK_Storm/index.html#additional-articles",
    "title": "2024 January Winter Storm",
    "section": "Additional Articles",
    "text": "Additional Articles\n\nOEM - 2024 January winter Storm Spotlight\nTanner Heffner - lessons from the storm\nPortland.gov - Dedicated team, community support help Portland weather a winter storm to start 2024"
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html",
    "title": "Book Review: The Only Good Indians",
    "section": "",
    "text": "Rating: 10/10  Overview: This book is a gripping horror novel that intertwines the lives of four Native American men with a chilling legacy from their past. A decade after a haunting event, the repercussions begin to terrorize them in ways they never imagined. This tale uniquely blends psychological horror with poignant reflections on cultural identity and the inescapable shadows of history"
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#introduction-to-horror",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#introduction-to-horror",
    "title": "Book Review: The Only Good Indians",
    "section": "1.1. Introduction to Horror",
    "text": "1.1. Introduction to Horror\nThis was my first foray into horror novels. I chose “The Only Good Indians” specifically because I was unfamiliar with it, hoping to be genuinely surprised by the story’s twists. And indeed, I was not disappointed.\nFrom the first chapter, the novel sets a brisk pace and a tone that blends the ordinary with a sense of impending dread. The graphic content, while occasionally shocking, serves to intensify the atmosphere without overshadowing the plot. There’s a rich layer of mystery throughout the book, which I found thoroughly engaging and well worth the venture into this new genre."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#cultural-identity-and-heritage",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#cultural-identity-and-heritage",
    "title": "Book Review: The Only Good Indians",
    "section": "1.2. Cultural Identity and Heritage",
    "text": "1.2. Cultural Identity and Heritage\nThe novel is not only a narrative about personal survival but also a profound exploration of the complexities of cultural identity. Jones showcases how traditions and ancient beliefs can persist in contemporary life, influencing the characters’ actions, their worldviews, and their interactions with both the spiritual and the physical worlds.\nThe heritage of the characters is not merely a backdrop but acts as a pivotal force in driving the plot and deepening the horror elements of the story. Through vivid descriptions and culturally specific details, Jones brings to life the distinct experiences of his characters, providing readers with a deeper understanding of their lives and the pressures they face from both within and outside their community.\nFurthermore, the book engages with the idea of legacy—what one generation passes down to the next and how these inheritances can shape lives in ways that are both visible and hidden. The reverence for traditional practices and the struggle with the ghosts of history are woven into the narrative, creating a rich tapestry that highlights the resilience and complexity of Native American life.\nBy embedding these themes in a horror context, Jones not only tells a thrilling story but also invites readers to reflect on the cultural struggles and triumphs of his characters, offering a poignant commentary on identity, community, and the indelible marks of heritage. This approach enriches the reader’s experience, providing not just suspense but a meaningful exploration of what cultural identity means in a modern world fraught with both past traumas and present challenges."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#suvivors-guilt-and-consequences-of-the-past",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#suvivors-guilt-and-consequences-of-the-past",
    "title": "Book Review: The Only Good Indians",
    "section": "1.3. Suvivor’s Guilt and Consequences of the Past",
    "text": "1.3. Suvivor’s Guilt and Consequences of the Past\nThe narrative is heavily centered on the concept of survivor’s guilt, where the characters are not only dealing with the immediate threats that emerge around them but also grappling with the emotional and psychological consequences of events that occurred many years prior. This guilt shapes their decisions, relationships, and perceptions of self, creating a tension that is both internal and external.\nThe consequences of past actions are portrayed as inescapable and lingering, suggesting that the past is never truly past but continues to influence the present in profound and often horrifying ways. The characters find themselves haunted not just by a supernatural presence but by the weight of their own memories and the choices they made. This intersection of personal history and supernatural consequence creates a compelling narrative that questions the possibility of ever truly escaping one’s actions.\nJones uses these themes to explore deeper societal issues as well, reflecting on how historical injustices and personal traumas are interlinked and how they continue to affect individuals and communities. The novel suggests that the horrors of the past are not easily forgotten and can resurface, sometimes monstrously, shaping the lives of those involved for years to come."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#nature-and-supernatural",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#nature-and-supernatural",
    "title": "Book Review: The Only Good Indians",
    "section": "1.4. Nature and Supernatural",
    "text": "1.4. Nature and Supernatural\nJones uses a blend of nature and the supernatural to explore themes of respect, desecration, and reconciliation. The natural world in “The Only Good Indians” is not just a passive setting but a dynamic force that interacts with the characters, influencing their lives and driving the narrative forward. This approach heightens the horror elements of the story, as the environment and its spirits become sources of tension and terror that the characters must navigate.\nThe supernatural occurrences in the novel are deeply tied to these natural settings, suggesting that the land itself holds memories and has the power to manifest them in ways that are both protective and punitive. This connection emphasizes the characters’ respect for and symbiosis with their environment, as well as the consequences when this balance is disrupted."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#socail-commentary",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#socail-commentary",
    "title": "Book Review: The Only Good Indians",
    "section": "1.5. Socail Commentary",
    "text": "1.5. Socail Commentary\nJones’s narrative is not just a horror story but also a poignant examination of the social issues that affect Native American people. Through the lives of his characters, the novel addresses themes such as systemic oppression, cultural erasure, and the struggles with identity that many indigenous people face. These elements are woven into the fabric of the story, reflecting real-world concerns in a manner that is both engaging and enlightening.\nThe horror elements of the book often serve as metaphors for the larger, more insidious fears that these communities contend with—such as the loss of cultural identity and the ongoing effects of historical traumas. The supernatural occurrences in the novel can be seen as manifestations of these larger societal issues, making the horror feel both immediate and deeply symbolic.\nJones also touches on resilience and the importance of community solidarity in facing these challenges. The characters’ responses to the supernatural threats are informed by their cultural heritage and collective experiences, showcasing the strength found in shared histories and traditions."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#the-horror-of-everyday-life",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#the-horror-of-everyday-life",
    "title": "Book Review: The Only Good Indians",
    "section": "1.6. The Horror of Everyday Life",
    "text": "1.6. The Horror of Everyday Life\nJones cleverly elevates everyday scenarios and settings to create vessels of horror. This technique not only blurs the lines between the ordinary and the supernatural but also heightens the tension and suspense throughout the novel. The familiar becomes sinister, turning the characters’ routine lives into a landscape where horror can emerge at any moment.\nThis theme serves as a powerful metaphor for the internal and external battles the characters face. The everyday struggles of the characters, who deal with issues like identity, cultural heritage, and personal demons, are amplified by the horror elements, suggesting that sometimes the real terror lies in the challenges we encounter daily.\nJones uses this setting to reflect on how the past continually shadows the present, making even simple decisions or encounters fraught with deeper meanings and potential dangers. The ordinary moments are depicted as being just as capable of producing fear and anxiety as the extraordinary ones, which makes the horror in the novel more relatable and unnerving."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#narrative-techniques",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#narrative-techniques",
    "title": "Book Review: The Only Good Indians",
    "section": "1.7. Narrative Techniques",
    "text": "1.7. Narrative Techniques\nListening to “The Only Good Indians” on audiobook provided a uniquely immersive experience, thanks largely to the narrator’s masterful delivery. The narrator’s voice modulation was exceptional, adeptly building suspense and accentuating the story’s eerie atmosphere. Each character was brought to life with distinct vocal nuances, making the narrative easy to follow and deeply engaging. The pacing was particularly effective—swift during climactic scenes to heighten tension, and slower during more reflective moments, allowing the story’s deeper themes to resonate. Furthermore, the narrator’s sensitivity to the cultural elements of the book added a layer of authenticity that enriched my understanding and appreciation of the narrative. Overall, the audiobook version added a compelling auditory dimension to the novel, making my experience both memorable and haunting."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#title-significance",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#title-significance",
    "title": "Book Review: The Only Good Indians",
    "section": "2.1. Title Significance",
    "text": "2.1. Title Significance\nThe title of Stephen Graham Jones’ novel, “The Only Good Indians,” carries profound thematic and symbolic weight, particularly as it echoes the historically violent phrase, “The only good Indian is a dead Indian.” This saying, which became popular in the late 19th century, reflects the brutal attitudes and genocidal sentiments directed toward Native American populations during the westward expansion in the United States. By choosing this title, Jones engages in a subversive act of linguistic reclamation, turning a phrase that epitomizes racial hatred into a critique of those very notions.\nThe novel itself mirrors the violence of this old saying, dealing with both the literal and metaphorical hauntings of the characters by their past actions and the broader historical injustices inflicted upon their ancestors. This reflection is not only about survival but also about the struggles of cultural identity in contemporary society, as the characters grapple with the implications of being considered a “good” Indian.\nThe irony of using such a historically derogatory statement as the title adds a layer of critique; through the stories of the characters, Jones highlights the resilience and humanity of Native American individuals, countering the dehumanizing undertones of the original phrase. The title also prompts readers to consider deeper moral and ethical questions about identity, redemption, and the nature of good and evil, challenging entrenched stereotypes and inviting a reexamination of the complexities that these judgments entail. Through this charged title, Jones ensures that the themes of historical context and contemporary reality are palpable, enriching the narrative to transcend horror and become a poignant commentary on society."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#character-dynamics-and-background",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#character-dynamics-and-background",
    "title": "Book Review: The Only Good Indians",
    "section": "2.2 Character Dynamics and Background",
    "text": "2.2 Character Dynamics and Background\nThis book offers a rich tapestry of character complexity and moral ambiguity, particularly through characters like Gabe and Victor Yellow Tail. These characters are depicted with depth and nuance, navigating a world where choices are often constrained by external forces and internal conflicts. This portrayal effectively showcases their roles as anti-heroes, individuals who grapple with their flaws and the moral complexities of their decisions.\nGabe, one of the four friends central to the story, exemplifies the anti-hero archetype. His life is marked by a blend of good intentions and flawed actions, creating a character that is both relatable and tragic. Gabe’s decisions are often influenced by the immediate needs and pressures of his environment, reflecting the limited choices available to him. His struggles with alcohol, his role as a father, and his attempts to maintain a semblance of normalcy in the face of haunting past actions all contribute to a portrait of a man caught between the desire for redemption and the pull of his circumstances. Gabe’s moral ambiguity is highlighted by his participation in the elk hunt that sets the tragic events of the novel in motion, an act driven by youth and rebellion that later returns to haunt him.\nVictor Yellow Tail, on the other hand, serves as a foil to the main group of friends. As a tribal police officer, Victor represents an uncomfortable intersection of community and authority who is seen as someone who has ostensibly made ‘better’ choices than his peers. His role as a law enforcer is doubly contentious as he is seen not just as upholding the law, but specifically as enforcing “the white man’s law” on reservation land. This position places him in a delicate balance between his duties and his cultural identity, making him a figure of both respect and resentment within the community. Victor’s interactions with the community, and his ultimate fate in the story, serve to underline the themes of justice and retribution that are central to the novel. His character raises questions about the nature of authority and righteousness within a community still grappling with its history and the ongoing challenges of its present.\nBoth Gabe and Victor, in their respective roles, navigate the world with a complex mixture of heroism and villainy. Their stories highlight the inherent challenges in defining morality within a community where historical injustices and personal failures intertwine. Through these characters, Jones explores the idea that morality is not a fixed state but a fluid dynamic, influenced by past actions, personal motivations, and the broader social and cultural forces at play."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#cultural-and-tribal-identities",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#cultural-and-tribal-identities",
    "title": "Book Review: The Only Good Indians",
    "section": "2.3. Cultural and Tribal Identities",
    "text": "2.3. Cultural and Tribal Identities\nAt the heart of the novel, the tensions between Crow and Blackfeet cultures are embodied in the characters and their backgrounds, subtly informing their interactions and conflicts. Jones does not explicitly detail the historical conflicts between the Crow and Blackfeet in the novel, but the weight of history is palpable in the characters’ lives. Historically, the Crow and Blackfeet were often in conflict over territory and resources, a backdrop that enriches the narrative’s tension. This history adds layers of meaning to the characters’ struggles with belonging and loyalty, highlighting how historical inter-tribal dynamics continue to affect contemporary relationships and self-perception.\nMoreover, the novel touches on the internalized prejudices and stereotypes that can prevail within and between Native American communities. Through the characters’ interactions and internal monologues, Jones subtly addresses how these historical animosities can lead to internalized racism and self-hatred, complicating the characters’ abilities to form a cohesive identity. The struggle for identity and acceptance is portrayed not just as an internal battle but as a challenge shaped by the long shadows of inter-tribal histories.\nIn tying these personal and inter-tribal tensions to the broader themes of the novel, Jones skillfully uses the horror genre as a metaphorical landscape to discuss cultural disintegration and identity conflicts."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#symbolism-and-mirroring",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#symbolism-and-mirroring",
    "title": "Book Review: The Only Good Indians",
    "section": "2.4. Symbolism and Mirroring",
    "text": "2.4. Symbolism and Mirroring\nThe illegal elk hunt that the characters partake in their youth serves as a critical symbol in the novel. This act of killing a pregnant elk in a restricted area is not only a breach of legal and cultural norms but also a violation of sacred life, which mirrors the historical violence inflicted upon Native American communities. This act of violence against nature symbolically represents the broader violence against Native Americans—both are acts of invasion and disrespect against beings seen as lesser by an oppressive force. The elk, particularly because it was pregnant, symbolizes not just life but potential life, reflecting the deep cuts made into the fabric of Native communities, where not only lives were taken but potential futures were destroyed.\nThe mirroring effect is a powerful tool used by Jones to draw parallels between individual actions and historical events. By aligning the slaughter of the elk with the genocidal history faced by Native Americans, the novel invites readers to reflect on the cyclical nature of violence and the ways in which historical injustices continue to resonate in contemporary settings. The characters’ personal guilt and the supernatural retribution they face are not just about their actions but also about their roles within these larger historical cycles. They become stand-ins for broader cultural narratives, living out themes of retribution and guilt that have been part of their heritage."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#the-elk-headed-woman",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#the-elk-headed-woman",
    "title": "Book Review: The Only Good Indians",
    "section": "2.5. The Elk-Headed Woman",
    "text": "2.5. The Elk-Headed Woman\nAs a figure of horror, the Elk-Headed Woman is terrifying and relentless. Her appearances are marked by a chilling blend of the natural and the supernatural, as she combines elements of a vengeful spirit and a wronged animal. Her pursuit of the main characters, who were responsible for the death of her elk form and her unborn calf during an illegal hunt, is both brutal and ghostly, imbuing the story with a palpable sense of dread and inevitability. Her actions are gruesome and her presence suffuses the novel with a tension that is typical of horror but with an added layer of psychological depth. This depth comes from her motivations, which unfold as the story progresses, reshaping the reader’s understanding of her role in the narrative.\nThe revelation of the Elk-Headed Woman’s desire for her lost baby introduces profound themes of motherhood and loss, adding layers to her character that extend beyond mere vengeance. This aspect of her motivation reveals her actions not just as blind revenge but as a mother’s grief-stricken response to the violent loss of her child. In this light, her relentless pursuit of the men reflects the depth of her loss and the lengths to which she will go for what she sees as reconciliation or retribution. This revelation complicates the reader’s perception of her as merely a villain, inviting a more empathetic understanding of her fury and actions.\nMoreover, the Elk-Headed Woman’s presence and motivations are deeply intertwined with the characters’ feelings of guilt and the symbolic weight of their past actions. She embodies the consequences of their youthful indiscretions, transforming their guilt into a literal haunting that stalks them throughout their lives. This connection emphasizes the theme that the past is never truly past when its wounds remain unaddressed. The Elk-Headed Woman, therefore, stands as a constant reminder of the characters’ responsibility for their actions and the pain they have caused."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#the-final-girl",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#the-final-girl",
    "title": "Book Review: The Only Good Indians",
    "section": "2.6. The Final Girl",
    "text": "2.6. The Final Girl\nIn the novel, Denora is positioned as the “final girl.” Unlike the typical final girl trope where the character often begins as somewhat innocent or unassuming, Denora’s role is steeped in cultural identity and personal resilience from the outset. She is not only a survivor but also a representation of a new generation grappling with both the legacy of the past and the pressures of the present.\nDenora’s confrontation with the Elk Head Woman during the climactic scenes of the book redefines the trope. Here, the antagonist is not merely a slasher or a faceless monster but a manifestation of cultural trauma and historical violence. The Elk Head Woman is a symbol of the consequences that come from the characters’ disconnection from their traditions and their community.\nDenora’s showdown with this spirit is significant because it is not just about physical survival but about cultural survival and identity. Her role as the final girl underscores a thematic focus on healing and reconciliation. It highlights the possibility of overcoming generational traumas through understanding and facing the horrors of the past directly.\nThis confrontation also shifts the perspective on how strength and vulnerability are portrayed in horror narratives. Denora’s strength comes from her deep connection to her culture and her ability to face the truth of her community’s and her ancestors’ actions. This makes her survival not just a personal victory but a communal hope for renewal and change.\nBy placing Denora at the center of this final confrontation, Jones not only subverts the traditional final girl trope but enriches it, making it a vehicle for deeper commentary on resilience, identity, and redemption within Native American communities. Her survival is symbolic of the broader survival and adaptation of her culture, offering a powerful message about the endurance of heritage and the strength found in facing one’s history."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240401-BookReview-HowToWinFriendsAndInfluencePeople/index.html",
    "href": "blog/Book-Reviews/posts/20240401-BookReview-HowToWinFriendsAndInfluencePeople/index.html",
    "title": "Book Review: How to Win Friends & Influence People",
    "section": "",
    "text": "Rating: 3/10  Overview: Published in 1936, this book carries the unmistakable tone of its era. While it includes some noteworthy quotes and offers insights at a high level, many of the ideas presented are quite basic. Through Dale Carnegie’s extensive use of personal stories and anecdotes, it becomes apparent that these strategies could also be interpreted as manipulative.\n\n\nIntroduction\nIn the landscape of personal development and interpersonal skills, few books have stood the test of time as prominently as Dale Carnegie’s “How to Win Friends and Influence People.” Despite its age, the core principles Carnegie presents remain remarkably relevant, addressing the timeless human desire for connection and understanding. However, it’s not without its controversies; some readers may find its techniques manipulative, and some of Carnegie’s idols (such as Robert E. Lee and John D. Rockefeller) and sexitst pro-capitalist ideals problematic.\nIn the following, I share my notes - a compilation of quotes and insights that I found particularly striking. These reflections are intended to capture the essence of Carnegie’s advice through my lens, underscoring its potential wisdom.\n\n\nPart One: Fundamental Techniques in Handling People\nThis section covers the basic principles of interacting with others, emphasizing the importance of avoiding criticism, condemnation, or complaints.\n\nBy criticizing, we do not make lasting changes and often incur resentment.\nDon’t criticize them, they are doing just as you would under similar circumstances.\nAny fool can criticize, complain, and condemn—and most fools do. But it takes character and self-control to be understanding and forgiving.\nTell me what makes you feel important and I’ll tell you the type of person you are.\n\n\n\nI learned thirty years ago that it is foolish to scold. I have enough trouble overcoming my own limitations without fretting over the fact that God has not seen fit to distribute evenly the gift of intelligence.\n\nJohn Wanamaker\n\n\n\n\n\nJudge not, that ye be not judeged. For with what judgment ye judge, ye shall be judged: and with what measure ye mete, it shall be measured to you again. And why beholdest thou the mote that is in thy brother’s eye, but considerest not the beam that is in thine own eye?\n\nMatthew 7:1-3\n\n\n\n\n\nI will speak ill of no one. I will speak all the good I know of everybody.\n\nBenjamin Franklin\n\n\n\n\n\nTo understand all is to forgive all.\n\nEvelyn Waugh\n\n\n\n\n\nI shall not pass this way again, so when I do it let me be kind.\n\nEtienne de Grellet\n\n\n\n\n\nPart Two: Six Ways to Make People Like You\nHere, Carnegie presents simple strategies to become more likable and build stronger, more positive relationships with others.\n\nBecome genuinely interested in other people.\n\nYou can make more friends in two months by becoming interested in other people that in 2 years by trying to get other people interested in you.\nIt is the individual who is not interested in their fellow man that has the greatest difficulties in life and provides the greatest injury to others. It’s from those individuals that all human failures spring.\nIf the author doesn’t like people, than people won’t like their stories.\nWe all like people who admire us.\n“I love my audience.”\n\n\n\nEvery man I meet is my superior in some way, and in that I learn from him.\n\nRalph Waldo Emerson\n\n\n\nSmile.\n\nEncouragement is more effective teaching device than punishment.\nStop talking about what you want, and see things from other peoples sides.\nAct and speak as if cheerfulness is already there.\nHappiness depends on inside conditions. It is what you think about it.\nTo think rightly is to create.\n\n\n\nThere is nothing good or bad, but thinking makes it so.\n\nShakespeare\n\n\n\n\n\nWe become like that on which our hearts are fixed. Carry your chin in and the crown of your head high. We are gods in the chrysalis.\n\nJeffrey J. Denning\n\n\n\nRemember that people like hearing their name.\nBe a good listener. Encourage others to talk about themselves.\nTalk in terms of the other person’s interests.\nMake the other person feel important and do it sincerely.\n\nOil the cogs with little things such as please, thank you, if you don’t mind, and so on.\nRecognize other people’s importance and recognize it sincerely.\nTalk to people about themselves and they will listen for hours.\n\n\n\nThe deepest urge in human nature is the desire to be important.\n\nJohn Dewey\n\n\n\n\n\nThe deepest principle in human nature is the craving to be appreciated.\n\nWilliam James\n\n\n\n\n\nDo unto others how you would have others do unto you.\n\nMathew 7:12\n\n\n\n\n\nBut man, proud man, Drest in a little brief authority, Most ignorant of what he’s most assur’d; His glassy essence, like an angry ape, Plays such fantastic tricks before high heaven, As make the angels weep.\n\nShakespeare\n\n\n\n\n\n\nPart Three: How to Win People to Your Way of Thinking\nThis part offers tactics for persuading others to your point of view without causing offense or arousing resentment.\n\nThe best way to win an argument is to avoid it.\n\nYou can’t win an argument.\nDistrust your first impression.\nWelcome the disagreement.\nWhen two partners always agree then one of them is not necessary.\nPerhaps there is something you haven’t thought about. Be thankful it’s been brought to your attention.\nWatch out for your first reaction. It may be you at your worst, not your best.\nYou can measure the size of a person by what makes them angry.\nBuild bridges of understanding, not barriers of misunderstanding.\nDwell on areas where you agree with others.\nPromise to think over their ideas and mean it. It’s easier to agree to think about their points.\nThank your opponents sincerely for their interests. Anyone who takes the time to disagree with you is interested in the same things you are.\nThink of your opponents as people who want to help you. Think of your opponents as your friends.\nWhen one person is yelling the other should be listening because when two people yell then no one is listening.\n\nQuestions to Ask Yourself\n\nCould my opponents be right or partially right?\nIs there truth or merit in their position or argument?\nIs my reaction one that will relieve the problem, or just relieve my frustration?\nWill my reaction draw my opponents further away or closer to me?\nWill my reaction demonstrate the estimation of good will people have of me?\nWill I win or lose? What price will I have to pay if I win?\nIf I am quiet about it will the disagreement blow over?\nIs this difficult situation an opportunity for me?\n\n\n\nIf you argue you will never have a person’s good will.\n\nBenjamin Franklin\n\n\n\n\n\nHere lies the body of William Jay, Who died maintaining his right of way— He was right, dead right, as he sped along, But he’s just as dead as if he were wrong.\n\nCarnege\n\n\n\n\n\nHatred is never ended by hatered, but by love.\n\nBuddah\n\n\n\n\n\nYield larger things to which you can show no more than equal right; and yield lesser ones, though clearly your own. Better give your path to a dog than be bitten by him in contesting for the right. Even killing the dog would not cure the bite.\n\nAbraham Lincoln\n\n\n\nShow respect for the other person’s opinions. Never say “You’re wrong”.\n\nIf you are going to prove anything, don’t make anybody know it.\nBe wiser than other people if you can; but do not tell them so.\nWhen you argue you are not persuading.\nRidicule and abuse never make someone agree with you.\nBenjamin Franklin never spoke in certainties.\nReply to your opposition stating that in some cases you could see their perspective being right. You could also see some cases in which that is not the case.\nConversations will go more pleasantly and a modest suggestion will be better received.\nYou will have less mortification when found to be in the wrong, and more people to agree with you when you are right.\nMake it a point not to disagree or tell the other person they are wrong.\nBeing diplomatic will help you gain your point.\n\n\n\nMen must be taught as if you taught them not, and thing proposed as things forgot.\n\nAlexander Pope\n\n\n\n\n\nYou cannot teach a man anything, only help him find it within himself.\n\nGalileo\n\n\n\n\n\nWe sometimes find ourselves changing our minds without any resistance or heavy emotion, but if we are told that we are wrong we resent the imputation and harden our hearts. We are incredibly heedless in the formation of our beliefs, but find ourselves filled with an illicit passion for them when anyone proposes to rob us of their companionship. It is obviously not the ideas themselves that are dear to us, but our self-esteem which is threatened.\n\nJames harvey Robinson, The Mind in the Making\n\n\n\n\n\nBen you are impossible. Your opinions have a slap in them for everyone who differs with you. They have become so offensive that nobody cares for them. Your friends find that they enjoy themselves better when you are not around. You know so much that no one can tell you anything. Indeed no one is going to try for the act and effort would only lead to discomfort and hard work. Therefore you are likely never to know anymore than you do now, which is very little.\n\nQuote from Benjamin Franklins Autobiography\n\n\n\n\n\nI judge people by their principles not by my own.\n\nMLK\n\n\n\n\n\nAgree with thyn adversary quickly.\n\nJesus\n\n\n\nIf you are wrong admit it quickly and emphatically.\n\nWhen you say the other person’s criticism of yourself first, then the criticism is coming from your mouth, and the blow will be much softer.\nAny fool can defend their mistakes, but it takes a large person to discuss failure.\nBy fighting you never get enough, but by yielding you get more than you expected.\n\nBegin in a friendly way.\n\nGentleness and friendliness are stronger than brute force.\n\n\n\nIf you come at me with your fists doubled, I think I can promise you that mine will double as fast as yours; but if you come to me and say, ‘Let us sit down and take counsel together, and, if we differ from each other, understand why it is that we differ, just what the points at issue are,’ we will presently find that we are not so far apart after all, that the points on which we differ are few and the points on which we agree are many, and that if we only have the patience and the candor and the desire to get together, we will get together.\n\nWoodrow Wilson\n\n\n\n\n\nA drop of honey catches more flies than a gallon of gall.\n\nAbraham Lincoln\n\n\n\nGet the other person saying “yes yes” immediately.\n\nBegin and keep on emphasizing the things on which you agree. You are both striving for the same end, but your only difference is of method not of purpose.\nA no response is a most difficult handicap to overcome. When you have said no, all your pride and personality demands that you remain consistent with yourself.\nIt takes more force to go in the opposite direction.\nWhen someone says no they are doing more than that. The body is doing more than that. When a person says yes, no physical or mental withdraw happens.\nA skillful speaker gets a yes response.\nThe more yeses we can receive, the more likely we are to succeed at capturing our proposal.\nThe Socratic method is based on getting a yes response.\nAsk gentle questions to get a yes response.\nHe who treads softly goes far.\n\nLet the other person do a great deal of the talking.\n\nLet the other person talk themselves out.\n\nLet the other person feel the idea is theirs.\nTry honestly to see things from the other person’s point of view.\n\nSuccess when dealing with people depends on a sympathetic grasp of the other person’s viewpoint.\nCooperativeness in conversation is achieved when you show that you consider the other person’s ideas and feelings as important as your own.\nThink always in terms of the other person’s point of view.\n\nBe sympathetic to the other person’s ideas and desires.\n\nI don’t blame you for feeling how you do, and if I were you I would feel as you do.\n\n\nThere but for the grace of god go I.\nProverb\n\nAppeal to the nobler motives.\n\nBasically there is a real reason why someone does something, and then the reason they tell people (the nobler reason). Appeal to these reasons.\nStory about a guy who didn’t want a specific picture of him in the paper, but wrote a letter saying that it was his mother who did not like this picture. Appealing to the nobler motive of respecting a mothers wishes.\n\nDramatize your ideas.\n\nShow your point, don’t just explain it.\n\nThrow down a challenge.\n\nMotivate with friendly competition.\n\n\n\n\nPart Four: Be a Leader: How to Change People Without Giving Offense or Arrousing Resentment\nThe final section focuses on leadership and how to encourage others to change their behavior or beliefs in a positive way.\n\nBeing a leader often involves changing people’s behaviors and attitudes.\n\n\nBegin with praise and honest appreciation.\nCall attention to people’s mistakes indirectly.\nTalk about your own mistakes before criticizing others.\nAsk questions instead of giving direct orders.\nLet the other person save face.\nPraise the slightest improvement, and praise every improvement. Be hearty in your approbation and lavish in your praise.\nGive the other person a fine reputation to live up to.\n\n\nAssume a virtue if you have it not.\n\nShakespeare\n\n\n\nUse encouragement. Make the fault seem easy to correct.\nMake the other person happy about doing the thing that you suggest.\n“Playbook”\n\nBe sincere, and do not promise anything.\nKnow what you want the other person to do.\nBe empathetic, and ask what the other person wants.\nWhat are the benefits the other person will get doing what you want?\nMatch those benefits to the things that they want.\nWhen you make your request, put it in a form that they personally will benefit from doing this.\n\n\n\n\nConclusion\nDale Carnegie’s “How to Win Friends and Influence People” provides a foundational perspective on the art of interaction and persuasion. While its age-old advice resonates with timeless truths about human nature, the modern reader must navigate its principles with a critical eye."
  }
]