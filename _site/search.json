[
  {
    "objectID": "links/linkedin.html",
    "href": "links/linkedin.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "Continue to ~ Linkedin ~\nThanks for stopping by!\n\n\n\nNamba Park, Osaka, Japan"
  },
  {
    "objectID": "links/spotify.html",
    "href": "links/spotify.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "Continue to ~ Spotify ~\nThanks for stopping by!\n\n\n\nNamba Park, Osaka, Japan"
  },
  {
    "objectID": "blog/Book-Reviews/blog.html",
    "href": "blog/Book-Reviews/blog.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "Book Review: Anxious People\n\n\n\n\n\n\n2/10\n\n\nContemporary Fiction\n\n\n\n\n\n\n\n\n\nJun 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBook Review: The Only Good Indians\n\n\n\n\n\n\n10/10\n\n\nHorror\n\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBook Review: How to Win Friends & Influence People\n\n\n\n\n\n\n03/10\n\n\nSelf-help\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBook Review: Carmageddon\n\n\n\n\n\n\n09/10\n\n\nSocial Science\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240301-BookReview-Carmageddon/index.html",
    "href": "blog/Book-Reviews/posts/20240301-BookReview-Carmageddon/index.html",
    "title": "Book Review: Carmageddon",
    "section": "",
    "text": "Rating: 9/10  Overview: Examines cars’ impact on society, illustrating the negative effects of car culture and challenging its dominance.\n\n\nIntroduction\nIn his compelling and thought-provoking book, Carmageddon: How Cars Make Life Worse and What to Do About It, Daniel Knowles invites readers to reconsider the pervasive dominance of automobiles in modern society. Through sharp analysis and persuasive arguments, Knowles explores the multifaceted ways in which car culture adversely affects our daily lives. Below, I share some of the insights and reflections that resonated with me long after turning the last page.\n\n\nCivil Rights and Public Transportation\n\nThe automotive industry’s exponential growth coincided with the era when people of color finally gained the right to sit anywhere on public transportation. This pivotal moment in civil rights history marks a complex intersection with the rise of car culture.\nIn the United States, the expansion of roads and highways was strategically planned in ways that often had deleterious effects on certain communities. These construction projects were intentionally routed to displace specific groups or sever their access to vital city resources, exacerbating social and economic divides.\nDisplacement due to these infrastructure projects frequently forced communities into areas lacking in essential public transportation services. This not only marginalized these groups further but also entrenched car dependency as a primary mode of mobility, despite its broader societal impacts.\nThe Albina Vision Trust in Portland is an exemplary organization actively working to address and rectify transportation policies, among other institutional decisions, that have historically led to the displacement of hundreds of families. Their efforts highlight a crucial movement towards reconciling past injustices with forward-thinking urban and transportation planning.\n\n\n\nCars and Classism\n\nCars since their origination have been a sign of the upper class. Even today you are seen as “less” if you do not own a car.\nA lot of jobs require people to have a drivers license or “valid form of transportation”, which means driving. Even though there are plenty of valid forms of transportation that are not driving.\nOn Class, Capitalism and Urban Planning in Who Framed Roger Rabbit , delves into an intriguing analysis of the film, which is based on the true story of the dismantling of trolley systems across the United States. It offers a compelling look at how the movie intertwines the narratives of class, capitalism, and urban planning, highlighting the profound impact of the automobile industry’s rise on public transit and, by extension, society at large.\nIn Kenneth Grahame’s classic novel, The Wind in the Willows, the character of Mr. Toad epitomizes the reckless and inconsiderate motorist, reflecting critiques of upper-class driving behaviors. For an intriguing dive into the real-world inspiration behind this memorable character, explore the article: Meet the Real-Life Inspiration Behind “Mr.Toad”.\n\n\n\nParking\n\nSociety’s preference for free parking over affordable housing is starkly evident in the increasing number of individuals living out of RVs and cars. This phenomenon highlights a troubling reality: in many areas, it’s more financially feasible to secure a parking spot for a vehicle than to afford living accommodations.\nUrban planning regulations often mandate a disproportionate allocation of parking spaces relative to the square footage of buildings, irrespective of the practical need for such extensive parking. This is particularly illogical for certain types of establishments, such as bars, where the excessive requirement for parking does not align with the nature of the business.\nThe prioritization of free parking spaces over the availability of public bathrooms underscores a misalignment in urban development priorities, where the convenience of car owners is often placed above basic public amenities.\nIdealized representations of car-centric cities frequently omit the extensive infrastructure devoted to parking. These depictions fail to acknowledge the visual, spatial, and environmental impact of parking lots and garages, presenting an unrealistically clean and uncluttered urban landscape.\nThe mandated quantity of parking in urban areas contributes to sprawling city layouts, with destinations spaced too far apart for practical walking. This sprawling effect contradicts one of the fundamental advantages of city living: the ability to navigate the urban environment on foot. By encouraging car dependency, current parking requirements undermine the walkability and, ultimately, the very essence of city life.\n\n\n\nTraffic\n\nTraffic congestion is inherently linked to the prevalence of cars. The solution to reducing traffic lies not in expanding road networks but in decreasing car usage.\nA shift towards public transportation, coupled with investments in its infrastructure, could significantly alleviate traffic congestion. Prioritizing transit development over the construction of more roads offers a sustainable path to reducing urban traffic.\nThe average driver spends a considerable amount of time stuck in traffic, a reality conspicuously absent from car advertisements. These commercials often portray an idealized driving experience, sidestepping the common frustration of traffic jams that many drivers face daily.\n\n\n\nThe Lethal Impact of Some Vehicles\n\nCars are usually designed to keep you safe in crashes. They have special features like engines that drop down and fronts that crumple to absorb crash impacts, protecting the people inside. But, there’s a big problem with large, lifted trucks and vans. Their height means they often skip right over those safety features in smaller cars. Instead of the front crumpling and absorbing the crash, these big vehicles can crash directly into where passengers sit, causing much worse injuries, and fatalities.\nCar manufacturers are aware that these big trucks and vans are more dangerous in accidents. Still, they keep selling them because they’re profitable, not just the vehicles themselves but also all the extra accessories they can sell.\nChoosing one of these big vehicles might feel safer because they’re so sturdy and high off the ground. But the truth is, they turn accidents into much more dangerous situations.\n\n\n\nCharity Car Clubs: A Strategic Image Shift\n\nAware of their controversial impact, car manufacturers have sought to align themselves with the emblematic “American Dream.” To cultivate a positive public image, they’ve initiated charity car clubs. These initiatives gather car owners to engage in charitable activities, all under the auspices of the car company’s brand.\nThis strategy is designed to forge a strong association between car ownership and membership in a commendable community. It intentionally crafts the narrative that owning a car from a particular brand elevates one’s social standing, embedding the vehicle not just as a means of transport but as a symbol of belonging to a virtuous circle.\n\n\n\nVehicles in Developing Nations\n\nThe United States, along with other developed nations, enforces rigorous environmental and safety standards for vehicles. Consequently, cars that no longer comply with these stringent regulations in their home countries are often exported to developing nations.\nThe implementation of import regulations on vehicles by developing countries can inadvertently widen the socioeconomic divide. Such policies may restrict access to affordable transportation for lower-income segments, exacerbating the gap between different social classes.\n\n\n\nCar Corporations and Public Health\n\nThe Volkswagen emissions scandal revealed the company cheated on emissions tests, significantly impacting environmental and public health standards.\nStudies indicate lead from gasoline significantly impaired the IQ of nearly half the U.S. population. The elimination of leaded gasoline has been a critical step in removing a major public health threat, marking a pivotal victory against environmental pollution caused by automotive fuels.\n\n\n\nLimitations of Electric Cars\n\nIn cities where the electrical grid relies on coal for power, the environmental benefits of electric cars are negated. The reliance on fossil fuels for electricity generation means that even zero-emission vehicles indirectly contribute to pollution.\nThe production of electric car batteries involves extensive mining operations due to their large size compared to smartphone batteries. This increased demand for raw materials significantly exacerbates the environmental impact associated with their extraction and processing.\nInvestments in infrastructure projects like the Las Vegas Convention Center Loop a one-way, one-lane tunnel that permits only a single car at a time, represents a questionable allocation of resources. This system pales in comparison to traditional subway systems, which offer higher capacity, efficiency, and speed at lower costs.\n\n\n\nGender Bias in Transporation\n\nPublic transportation systems often overlook the needs of women, despite their higher usage rates compared to men.\nMothers, in particular, face challenges when navigating buses or trains with children, highlighting a lack of family-friendly amenities.\nThe design and routing of many public transit options cater predominantly to the conventional nine-to-five work commute, neglecting the complex daily routines of individuals like stay-at-home moms who juggle school drop-offs, grocery shopping, and other errands.\nHistorically, crash test dummies were based on male body types, leading to vehicle safety features that are more optimized for men than for women.\nChild safety in vehicles largely falls to parents, as standard car seat designs cater to adults, not children, necessitating additional child-specific safety equipment.\n\n\n\nWalkable Cities\n\nWalkable cities emphasize pedestrian access, fostering healthier lifestyles by minimizing dependence on cars.\nShifting towards these urban designs offers notable environmental advantages, such as diminished air pollution and reduced greenhouse gas emissions.\nNotable examples include quaint college towns and places like Disney World’s Epcot, designed for easy pedestrian navigation across areas. Unlike the sprawling urban developments of today, these examples showcase the practicality and appeal of compact, walkable communities, underscoring the need for a return to more human-scale urban planning.\n\n\n\nConlusion\nIn “Carmageddon: How Cars Make Life Worse and What to Do About It,” Daniel Knowles navigates the intricate ways in which automobile dominance shapes modern society, spotlighting its profound impact on everything from civil rights and classism to urban planning and public health."
  },
  {
    "objectID": "blog/Technical-Blog/blog.html",
    "href": "blog/Technical-Blog/blog.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "Code to Content Lighting Talk 2\n\n\n\n\n\n\nTalks\n\n\nQuarto\n\n\n\n\n\n\n\n\n\nJun 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Test Data for Grants Claim Submissions\n\n\n\n\n\n\nR\n\n\nData Analysis\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCode to Content Lighting Talk\n\n\n\n\n\n\nTalks\n\n\nQuarto\n\n\n\n\n\n\n\n\n\nMay 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 14 Tidy Tuesday: Du Bois Visualization Challenge\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13 Tidy Tuesday: NCAA Men’s March Madness\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nReady4R: Crosstables\n\n\n\n\n\n\nReady4R\n\n\nR\n\n\nExploratory Data Analysis\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCode to Content\n\n\nThe Art and Impact of Data Blogging\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nRandi Bolt\n\n\n\n\n\n\n\n\n\n\n\n\nReady4R: skimr Package\n\n\n\n\n\n\nReady4R\n\n\nR\n\n\nExploratory Data Analysis\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 25 Tidy Tuesday: UFO Sightings Redux\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 24 Tidy Tuesday: SAFI Teaching Data\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Python Commands for Virtual Environments and Package Management\n\n\n\n\n\n\nWorkshop\n\n\nPython\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 22 Tidy Tuesday: Centenarians Data\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGun Law Scorecard (Article Review)\n\n\n\n\n\n\nArticle Review\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 17 Tidy Tuesday: London Marathon\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 Tidy Tuesday: Bob Ross Paintings\n\n\n\n\n\n\n#TidyTuesday\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nFlashcards\n\n\n\n\n\n\nOriginal\n\n\nMathematics\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nJan 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR and SQLite\n\n\n\n\n\n\nOriginal\n\n\nR\n\n\nSQL\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSportsObserveR - Part 2: Creating a Package in R\n\n\n\n\n\n\nOriginal\n\n\nNBA\n\n\nR\n\n\nPackage Building\n\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNBA Salaries - Part 1: Web-Scrapping\n\n\n\n\n\n\nOriginal\n\n\nNBA\n\n\nR\n\n\nData Visuals\n\n\nWeb-Scrapping\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Interview Questions - Part 1\n\n\n\n\n\n\nOriginal\n\n\nStatistics\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Links\n\n\n\n\n\n\nOriginal\n\n\nQuarto\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSportsObserveR - Part 1: Scrapping Functions\n\n\n\n\n\n\nOriginal\n\n\nR\n\n\nNBA\n\n\nWeb-Scrapping\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nProve P is Logically Equivalent to the Negation of the Negation of P\n\n\n\n\n\n\nMathematics\n\n\nProofs\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Rules! - Salt Lake City R User Group\n\n\n\n\n\n\nWorkshop\n\n\nR\n\n\nBayes\n\n\n\n\n\n\n\n\n\nAug 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nTidyverse Style Guide (Part 1)\n\n\n\n\n\n\nR4DS\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nReactable\n\n\n\n\n\n\nOriginal\n\n\nNBA\n\n\nR\n\n\nWeb-Scrapping\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nJul 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Stock Prices\n\n\n\n\n\n\nOriginal\n\n\nR\n\n\nData Analysis\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nJun 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Prep. Statistics\n\n\n\n\n\n\nSchool\n\n\nStatistics\n\n\nExam Prep\n\n\n\n\n\n\n\n\n\nJun 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Prep. Modern College Geometry\n\n\n\n\n\n\nSchool\n\n\nMathematics\n\n\nExam Prep\n\n\n\n\n\n\n\n\n\nMay 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPortland R User Group: Aggregate making tables with gt packag\n\n\n\n\n\n\nWorkshop\n\n\nPDXRUG\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nApr 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Spice\n\n\n\n\n\n\nWorkshop\n\n\nR\n\n\nData Visuals\n\n\nNLP\n\n\nTopic Model\n\n\n\n\n\n\n\n\n\nApr 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMidterm Prep. Group Theory\n\n\n\n\n\n\nSchool\n\n\nMathematics\n\n\nExam Prep\n\n\n\n\n\n\n\n\n\nMar 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMidterm Prep. Modern College Geometry\n\n\n\n\n\n\nSchool\n\n\nMathematics\n\n\nExam Prep\n\n\n\n\n\n\n\n\n\nMar 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio: Building a Blog with R\n\n\n\n\n\n\nWorkshop\n\n\nR\n\n\n\n\n\n\n\n\n\nFeb 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUO: ggplot2 Part 2\n\n\n\n\n\n\nWorkshop\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nFeb 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLearnGeom\n\n\n\n\n\n\nOriginal\n\n\nR\n\n\n\n\n\n\n\n\n\nJan 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLatex\n\n\n\n\n\n\nSchool\n\n\nMathematics\n\n\n\n\n\n\n\n\n\nJan 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDiagnostic For Leverage And Influence\n\n\n\n\n\n\nSchool\n\n\nLinear Regression\n\n\nR\n\n\n\n\n\n\n\n\n\nDec 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nTransformations and Weighting to Correct Model Inadequacies\n\n\n\n\n\n\nSchool\n\n\nLinear Regression\n\n\nR\n\n\n\n\n\n\n\n\n\nDec 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nProve H and I-H are Idempotent\n\n\n\n\n\n\nSchool\n\n\nLinear Regression\n\n\nProofs\n\n\n\n\n\n\n\n\n\nNov 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nSkewness\n\n\n\n\n\n\nSchool\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nNov 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nLet epsilon = min(x-a, b-x)\n\n\n\n\n\n\nSchool\n\n\nMathematics\n\n\nProofs\n\n\n\n\n\n\n\n\n\nOct 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nTree Diagrams\n\n\n\n\n\n\nOriginal\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nOct 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Art with Jasmines\n\n\n\n\n\n\nOriginal\n\n\nR\n\n\nGenerative Art\n\n\n\n\n\n\n\n\n\nSep 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAPI’s and tidycensus\n\n\n\n\n\n\nOriginal\n\n\nR\n\n\nAPI\n\n\n\n\n\n\n\n\n\nSep 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nR for Data Science - Ch.3: Data Visualisations\n\n\n\n\n\n\nR4DS\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nAug 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nEnable Emojis in Quarto\n\n\n\n\n\n\nOriginal\n\n\nQuarto\n\n\n\n\n\n\n\n\n\nAug 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nProving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition\n\n\n\n\n\n\nSchool\n\n\nMathematics\n\n\nProofs\n\n\n\n\n\n\n\n\n\nJul 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nProving If Two Angles Are Verticle, Then They Are Congruent\n\n\n\n\n\n\nSchool\n\n\nMathematics\n\n\nProofs\n\n\n\n\n\n\n\n\n\nJul 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics 451: Applied Statistics for Engineers and Scientists - Homework 1\n\n\n\n\n\n\nSchool\n\n\nStatistics\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nJun 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nBallet Data\n\n\n\n\n\n\nOriginal\n\n\nR\n\n\nData Visuals\n\n\n\n\n\n\n\n\n\nJun 21, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220725-Workshop-TidyverseP1/index.html",
    "href": "blog/Technical-Blog/posts/20220725-Workshop-TidyverseP1/index.html",
    "title": "Tidyverse Style Guide (Part 1)",
    "section": "",
    "text": "This is part 1 of my notes on the tidyverse style guide."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220725-Workshop-TidyverseP1/index.html#files",
    "href": "blog/Technical-Blog/posts/20220725-Workshop-TidyverseP1/index.html#files",
    "title": "Tidyverse Style Guide (Part 1)",
    "section": "2.1 Files",
    "text": "2.1 Files\n\nnames are meaningful and use letters, numbers, _, and/or -.\navoid special characters in files names.\nif more than 10 files then left pad with a zero (i.e. 00, 01, 02, 03,…)\nrename files instead of attempting 02a, 02b, and so on.\npay attention to capitalization.\nload all packages at once in the beginning of the file.\nuse - and = to break up file into readable chunks.\n\n\n\nShow code\n# Load data ---------------------------\n\n# Plot data ---------------------------"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220725-Workshop-TidyverseP1/index.html#syntax",
    "href": "blog/Technical-Blog/posts/20220725-Workshop-TidyverseP1/index.html#syntax",
    "title": "Tidyverse Style Guide (Part 1)",
    "section": "2.2 Syntax",
    "text": "2.2 Syntax\n\n2.2.1 Object Names\n\nVariables and function names should use lowercase letters, numbers and _ (no camel case).\nBase R uses dots in function and class names (data.frame).\nVariable names should be names and nouns.\nFunction names should be verbs.\nNames should be concise and meaningful.\nAvoid re-using names of common function and variables.\n\n\n\n2.2.2 Spacing\n\nPut a space after a common, never before.\nDo not put spaces inside or outside parentheses for regular functions : mean(x, na.rm = TRUE).\nPut a space before and after () when using if, for, or while.\nPlace a space after () used for function arguments : function(x) {}.\n\n\n\nShow code\n# if (debug) {\n#   show(x)\n# }\n\n\n\nThe embracing operator, { }, should always have inner spaces to help emphasize its special behavior.\nMost infix operators (==, +, -, &lt;-, etc.) should always be surrounded by spaces.\nExceptions being:\n\nThe operators with high precedence: ::, :::, $, @, [, [[, ^, unary -, unary +, and :.\nSingle-sided formulas when the right-hand side is a single identifier: call(!!xyz).\nWhen used in tidy evaluation !! (bang-bang) and !!! (bang-bang-bang) (because have precedence equivalent to unary -/+).\nThe helper operator: package?stats.\n\nadding extra spaces is ok if it improves alignment of = or &lt;-.\n\n\n\n2.2.3 Function Calls\n\nWhen you call a function omit the names of data arguemnts (such as x= because it is used so commonly).\nIf you override the default value of an argument, use the full name.\nAvoid assignments in function calls.\n\nException: function that capture side-effects: output &lt;- capture.output(x &lt;- f()).\n\n\n\n\n2.2.4 Control Flow\n\nFor {} brackets:\n\n{ should be the last character on the line.\nContents should be indented by two spaces.\n} should be the first character on the line.\nElse should be on the same line as } (if used).\n\n{} define the most important hierarchy of R code.\nAlways use && and || inside an if clause and never & and |.\nIf you want to rewrite a simple but lengthy if block:\n\n\n\nShow code\nif (x &gt; 10) {\n  message &lt;- \"big\"\n} else {\n  message &lt;- \"small\"\n}\n\n\nJust write it all on one line:\n\n\nShow code\nmessage &lt;- if (x &gt; 10) \"big\" else \"small\"\n\n\n\nit is okay to drop curly braces for very simple statements that fit on one line as long as they don’t have side-effects.\nFunction calls that affect control flow (like return(), stop() or continue) should always go in their own {} block.\nAvoid implicit type coercion (e.g. from numeric to logical) in if statements:\n\n\n\nShow code\n# Good\nif (length(x) &gt; 0) {\n  # do something\n}\n\n\nNULL\n\n\nShow code\n# Bad\nif (length(x)) {\n  # do something\n}\n\n\nNULL\n\n\n\nAvoid position-based switch() statements.\nEach element should go on its own line.\nProvided a fall-through arrow, unless you have previously validate the input.\n\n\n\nShow code\nswitch(x, \n  a = 0,\n  b = 1, \n  c = 2,\n  stop(\"Unknown `x`\", call. = FALSE)\n)\n\n\n[1] 0\n\n\n\n\n2.2.5 Long Lines\n\nLimit code to 80 characters per line. This fits comfortably on a printed page with a reasonably sized font.\n\n\n\nShow code\n# # Good\n# do_something_very_complicated(\n#   something = \"that\",\n#   requires = many,\n#   arguments = \"some of which may be long\"\n# )\n# \n# # Bad\n# do_something_very_complicated(\"that\", requires, many, arguments,\n#                               \"some of which may be long\"\n#                               )\n\n\n\nYou may also place several arguments on the same line if they are closely related to each other, e.g., strings in calls to paste() or stop().\n\n\n\n2.2.6 Semicolons\n\nDon’t put ; at the end of a line, and don’t use ; to put multiple commands on one line.\n\n\n\n2.2.7 Assignment\n\nUse &lt;-, not =, for assignment.\n\n\n\n2.2.8 Data\n\nUse \", not ', for quoting text. The only exception is when the text already contains double quotes and no single quotes.\nPrefer TRUE and FALSE over T and F.\n\n\n\n2.2.9 Comments\n\nEach line of a comment should begin with the comment symbol and a single space.\nIn data analysis code, use comments to record important findings and analysis decisions.\nIf you need comments to explain what your code is doing, consider rewriting your code to be clearer.\nIf you discover that you have more comments than code, consider switching to R Markdown."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220725-Workshop-TidyverseP1/index.html#functions",
    "href": "blog/Technical-Blog/posts/20220725-Workshop-TidyverseP1/index.html#functions",
    "title": "Tidyverse Style Guide (Part 1)",
    "section": "2.3 Functions",
    "text": "2.3 Functions\n\n2.3.1 Naming\n\nStrive to use verbs for function names.\n\n\n\n2.3.2 Long Lines\n\nPrefer function-indent style to double-indent style when it fits.\n\n\n\nShow code\n# Function-indent --------------------------------------\nlong_function_name &lt;- function(a = \"a long argument\",\n                               b = \"another argument\",\n                               c = \"another long argument\") {\n  # As usual code is indented by two spaces.\n}\n\n# Double-indent ----------------------------------------\nlong_function_name &lt;- function(\n    a = \"a long argument\",\n    b = \"another argument\",\n    c = \"another long argument\") {\n  # As usual code is indented by two spaces.\n}\n\n\n\nIf a function argument can’t fit on a single line, this is a sign you should rework the argument to keep it short and sweet.\n\n\n\n2.3.3 return()\n\nOnly use return() for early returns. Otherwise, rely on R to return the result of the last evaluated expression.\n\n\n\nShow code\n# Good\nfind_abs &lt;- function(x) {\n  if (x &gt; 0) {\n    return(x)\n  }\n  x * -1\n}\nadd_two &lt;- function(x, y) {\n  x + y\n}\n\n# Bad\nadd_two &lt;- function(x, y) {\n  return(x + y)\n}\n\n\n\nReturn statements should always be on their own line because they have important effects on the control flow.\nIf your function is called primarily for its side-effects (like printing, plotting, or saving to disk), it should return the first argument invisibly. This makes it possible to use the function as part of a pipe.\n\n\n\n2.3.4 Comments\n\nIn code, use comments to explain the “why” not the “what” or “how”.\nComments should be in sentence case, and only end with a full stop if they contain at least two sentences.\n\n\n\nShow code\n# Good -----------------------------------------\n\n# Objects like data frames are treated as leaves\n\n# Do not use `is.list()`. Objects like data frames \n# must be treated as leaves.\n\n# Bad -----------------------------------------\n\n# objects like data frames are treated as leaves\n\n# Objects like data frames are treated as leaves."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220725-Workshop-TidyverseP1/index.html#pipes",
    "href": "blog/Technical-Blog/posts/20220725-Workshop-TidyverseP1/index.html#pipes",
    "title": "Tidyverse Style Guide (Part 1)",
    "section": "2.4 Pipes",
    "text": "2.4 Pipes\n\n2.4.1 Introduction\n\nReserve pipes for a sequence of steps applied to one primary object.\nAvoid using pipes when there are meaningful intermediate objects that could be given informative names.\n\n\n\n2.4.2 Withespace\n\n%&gt;% should always have a space before it, and should usually be followed by a new line. After the first step, each line should be indented by two spaces.\n\n\n\n2.4.3 Long Lines\n\nIf the arguments to a function don’t all fit on one line, put each argument on its own line and indent.\n\n\n\n2.4.4 Short Pipes\n\nA one-step pipe can stay on one line, but unless you plan to expand it later on, you should consider rewriting it to a regular function call.\nSometimes it’s useful to include a short pipe as an argument to a function in a longer pipe. Carefully consider whether the code is more readable with a short inline pipe (which doesn’t require a lookup elsewhere) or if it’s better to move the code outside the pipe and give it an evocative name.\n\n\n\nShow code\n# Good\n# x %&gt;%\n#   select(a, b, w) %&gt;%\n#   left_join(y %&gt;% select(a, b, v), by = c(\"a\", \"b\"))\n# \n# # Better\n# x_join &lt;- x %&gt;% select(a, b, w)\n# y_join &lt;- y %&gt;% select(a, b, v)\n# left_join(x_join, y_join, by = c(\"a\", \"b\"))\n\n\n\n\n2.4.5 No Arguments\n\nmagrittr allows you to omit () on functions that don’t have arguments. Avoid this feature.\n\n\n\n2.4.6 Assignment\nThere are three acceptable forms of assignment:\n\nVariable name and assignment on separate lines:\n\n\n\nShow code\niris_long &lt;-\n  iris %&gt;%\n  gather(measure, value, -Species) %&gt;%\n  arrange(-value)\n\n\n\nVariable name and assignment on the same line:\n\n\n\nShow code\niris_long &lt;- iris %&gt;%\n  gather(measure, value, -Species) %&gt;%\n  arrange(-value)\n\n\n\nAssignment at the end of the pipe with -&gt;:\n\n\n\nShow code\niris %&gt;%\n  gather(measure, value, -Species) %&gt;%\n  arrange(-value) -&gt;\n  iris_long\n\n\n\nThe magrittr package provides the %&lt;&gt;% operator as a shortcut for modifying an object in place. Avoid this operator."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220725-Workshop-TidyverseP1/index.html#ggplot2",
    "href": "blog/Technical-Blog/posts/20220725-Workshop-TidyverseP1/index.html#ggplot2",
    "title": "Tidyverse Style Guide (Part 1)",
    "section": "2.5 ggplot2",
    "text": "2.5 ggplot2\n\n2.5.1 Introduction\n\nStyling suggestions for + used to separate ggplot2 layers are very similar to those for %&gt;% in pipelines.\n\n\n\n2.5.2 Whitespace\n\n+ should always have a space before it, and should be followed by a new line.\nIf you are creating a ggplot off of a dplyr pipeline, there should only be one level of indentation.\n\n\n\n2.5.3 Long lines\n\nIf the arguments to a ggplot2 layer don’t all fit on one line, put each argument on its own line and indent.\nggplot2 allows you to do data manipulation, such as filtering or slicing, within the data argument. Avoid this, and instead do the data manipulation in a pipeline before starting plotting.\n\n\nTo be continued …"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230609-Workshop-PythonCommands/index.html",
    "href": "blog/Technical-Blog/posts/20230609-Workshop-PythonCommands/index.html",
    "title": "Basic Python Commands for Virtual Environments and Package Management",
    "section": "",
    "text": "Explore basic Python commands for managing virtual environments and installing packages. Learn to set up isolated environments for clean and organized Python projects.\n\n\nIntroduction\nThe information provided here is based on the YouTube video titled “Python Tutorial: VENV (Mac & Linux) - How to Use Virtual Environments with the Built-In venv Module” by Corey Schafer.\nNote: Remember not to commit the virtual environment (venv) and include it in the .gitignore file. However, do commit the requirements.txt file for package version control.\nLet’s dive into the essential Python commands:\n\n\n1. Checking Installed Packages in the Global Environment\nTo view the list of packages installed on your machine in the global environment, execute the following command in your terminal:\n\npip3 list\n\n\n\n2. Creating a Virtual Environment\nTo create a virtual environment, use the following command:\n\npython3 -m venv project_name\n\n\n\n3. Creating a Virtual Environment in a Project Folder\nIt’s important to avoid placing your project files within the virtual environment. Instead, create the virtual environment in a project folder by executing the following command:\n\npython3 -m venv project_folder/project_name\n\n\n\n4. Activating the Virtual Environment\nTo activate the virtual environment, run the appropriate command based on your operating system:\n\nactivate virtual enviroment\n\n\n\n5. Checking the Python Version in the Virtual Environment\nTo determine the Python version being used within the virtual environment, use the following command:\n\nwhich python\n\n\n\n6. Checking Installed Packages in the Virtual Environment\nTo view the packages installed within the virtual environment, execute the following command:\n\npip list\n\n\n\n7. Installing Packages in the Virtual Environment\nTo install packages within the virtual environment, use the following command:\n\npip install package_name\n\n\n\n8. Creating a File of Packages and Versions\nTo create a file containing a list of installed packages and their versions, use the following command:\n\npip freeze &gt; requirements.txt\n\n\n\n9. Installing Packages from a Requirements File\nTo install packages from a requirements file, execute the following command:\n\npip install -r requirements.txt\n\n\n\n10. Deactivating the Virtual Environment\nTo deactivate the virtual environment and return to the global environment, use the following command:\n\ndeactivate \n\n\n\nConclusion\nThese basic Python commands provide a solid foundation for managing virtual environments and package installations. By utilizing virtual environments, you can isolate project dependencies and ensure consistent and reproducible development environments. Remember to create and commit a requirements.txt file for version control of your project’s dependencies, while excluding the virtual environment itself from version control."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220124-Original-LatexHacks/index.html",
    "href": "blog/Technical-Blog/posts/20220124-Original-LatexHacks/index.html",
    "title": "Latex",
    "section": "",
    "text": "Here is some short and simple latex.\n\n\nBasic Symbols :\n\n\\(\\sim\\) : \\sim\n\\(\\circ\\) : \\circ\n\\(\\square\\) : \\square\n\\(\\equiv\\) : \\equiv\n\\(\\cong\\) : \\cong\n\\(\\unlhd\\) : \\unlhd\n\\(\\div\\) : \\div\n\\(\\nless\\) : \\nless\n\\(\\ngtr\\) : ngtr\n\\(\\emptyset\\) : \\emptyset\n\\(\\subseteq\\) : \\subseteq\n\\(a\\choose b\\) : a\\choose b\n\\(\\underset{i\\in I}U\\) : \\underset{i\\in I}U\n\\(\\Leftrightarrow\\) : \\Leftrightarrow\n\\(\\langle\\rangle\\) : \\langle\\rangle\n\\(\\overrightarrow{\\rm AB}\\) : \\overrightarrow{\\rm AB}\n\\(\\underline{\\text{Underline Text}}\\) : \\underline{\\text{Underline Text}}\n\\(\\mathbb{R}\\) : \\mathbb{R}\n\nbb : blackboard bold\n\n\\(\\mathcal{F}\\) : \\mathcal{F}\n\\(\\mathscr{F}\\) : \\mathscr{F}\n\n\n\nGreek :\n\n\\(\\tau\\) : \\tau\n\\(\\rho\\) : \\rho\n\\(\\alpha\\) : \\alpha\n\\(\\beta\\) : \\beta\n\\(\\Gamma\\) : \\Gamma\n\\(\\epsilon\\) : \\epsilon\n\\(\\mathcal{E}\\) : \\mathcal{E}\n\\(\\varepsilon\\) : \\varepsilon\n\\(\\varphi\\) : \\varphi\n\n\n\nInline :\nLimits above and below sums and integrals\n\n\\(\\sum\\limits_{n}^{i}\\int_0^1\\) : \\limits\n\nMatrices and Matrix Equations\n\n\\(\\begin{smallmatrix} 1 & 0 \\\\ 0 & 1\\end{smallmatrix}\\) : \\begin{smallmatrix} 1 & 0 \\\\ 0 & 1\\end{smallmatrix}\n\n\\(I=[\\begin{smallmatrix} 1 & 0 \\\\ 0 & 1\\end{smallmatrix}]\\)\n\\((\\begin{smallmatrix} 1 & 1 \\\\ 1 & 0\\end{smallmatrix})(\\begin{smallmatrix} 1 & 1 \\\\ 0 & 1\\end{smallmatrix})\\ne(\\begin{smallmatrix} 1 & 1 \\\\ 0 & 1\\end{smallmatrix})(\\begin{smallmatrix} 1 & 1 \\\\ 1 & 0\\end{smallmatrix})\\)\n\n\n\n\nMultiple Lines :\nFunction\n\n\\(F(x)=\\begin{cases}1 & x\\geq 0\\\\0 & \\text{otherwise}\\end{cases}\\) : F(x)=\\begin{cases} . . . \\end{cases}\n\n1 & x \\geq 0 \\\\\n0 & \\text{otherwise}\n\n\nMatrix\n\n\\(F(x)=\\begin{bmatrix}1\\\\2\\\\3\\\\4\\\\5\\\\6\\\\\\end{bmatrix}\\) : F(x)=\\begin{bmatrix}1\\\\2\\\\3\\\\4\\\\5\\\\6\\\\\\end{bmatrix}\n\nSeries of Equalities\n\n\\(\\begin{equation}\\label{a}\\begin{split}x &= a+b+c\\\\&=1+2+3\\\\&=6\\end{split}\\end{equation}\\)\n\n: \\begin{equation}\\{label}\\begin{split}... \\end{split}\\end{equation}\n\nx &= a+b+c \\\\\n&= 1+2+3\n&= 6\n\n\n\nPotential Errors :\nSpelling\n\nEx: \\overlien{AB} should be \\overline{AB}\n\nlabel , table\n\n\nMore than two backslashes\n\nEx: Equation will work but \\end{equation} will show at the end. One of the lines has more than two backslashes at the end of at least one of the lines.\n\nSpace before final $\n\nEx: $\\angle ABC $ should be $\\angle ABC$\n\nMore $ on one side of equation than the other\n\nEx: $A^2+B^2=C^2$$ should be $A^2+B^2=C^2$\n\nClosing {}\n\nEx: $\\int\\limits_{1}^{2$ should be $\\int\\limits_{1}^{2}$\n\nUnderset on the wrong side\n\nEx: U\\underset{i\\in I} should be \\underset{i\\in I}U\n\n\n\nAdvice :\nDetextify\n\nIf you don’t know what a symbol is, draw it in Detextify here.\n\nGoogle Docs Equations Boxes\n\nMost latex backslashes work in google doc’s equation boxes. If I have to do a “quick” homework, and dont want to spend a lot of time formatting a pdf in R, I will use google docs and latex in the equations boxes.\n\nNote: This is how I began learning latex.\nWhy learn Latex?\n\nGenerally speaking it looks nicer, especially on reports, projects, and presentations.\nA lot of my peers and professors who wrote math by hand would have problems in their dominate writing hand. Typing math with latex helps spread that tension out to two hands.\nIt saves time in the long run, since updating a line in a typed document is a lot easier than re-writing an entire problem by hand. Not to mention the ability to use copy paste.\n\n\n\nBonus:\nColors\n\n\\(\\color{red}\\text{colored text}\\) : \\color{red}\\text{colored text}"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230123-Original-RandSQLite/index.html",
    "href": "blog/Technical-Blog/posts/20230123-Original-RandSQLite/index.html",
    "title": "R and SQLite",
    "section": "",
    "text": "Using a R package called SQLite, this post demonstrates how to answer data related questions with both R and SQL (Structured Query Language).\n\n\n0. Introduction\nThere are a handful of programming languages that data scientists use when querying, analyzing, and manipulating data. What I have found is that while R and Python have a lot more power in what they are capable of producing, SQL is used by a wider variety of roles to access and query data. So to get more practice using both SQL and R I pulled 10 questions and some data off a website called Learn SQL, and will be answering the following questions in both languages.\n\n0.0 Set-Up0.1 Packages0.2 Data0.3 Database\n\n\nContents:\n\n0.1 Packages\n0.2 Data\n0.3 Data base\n\n\n\nFor all my data queries and manipulation in R I will be using base R, dplyr, and magrittr.\n\n\nShow code\nlibrary(dplyr)\nlibrary(magrittr)\n\n\nTo create a SQL database, and run SQL queries in R chunks I will be using a package called RSQLite.\n\n\nShow code\n# install.packages(\"RSQLite\")\nlibrary(RSQLite)\n\n\nWarning: package 'RSQLite' was built under R version 4.3.2\n\n\n\n\nThis post will use three data sets that I copied from Learn SQL:\n\npatients: Which includes patient_id, first_name, last_name, gender, birth_date, city, province_id, allergies, height, and weight. Note I only copied the first 1000 entries.\n\n\n\nShow code\npatients &lt;- utils::read.csv('data/patients.csv')\n\n\n\nprovince_names: Which includes province_id, and province_name.\n\n\n\nShow code\nprovince_names &lt;- utils::read.csv(\"data/province_names.csv\")\n\n\n\nadmissions: which includes patient_id, admission_date, discharge_date, diagnosis, attending_doctor_id\n\n\n\nShow code\nadmissions &lt;- utils::read.csv(\"data/admissions.csv\")\n\n\n\n\nTo create a database use:\n\ndbConnect() to connect to a SQL data base called Hospital.db in the 00_data folder.\nSQLite() to connect to a SQLite database file.\n\n\n\nShow code\nhosp &lt;- RSQLite::dbConnect(RSQLite::SQLite(),\n                           \"data/Hospital.db\")\n\n\nTo define data within the database use:\n\ndbWriteTable() to create a data set within the hospital database first call the data base (hosp), define a name, and then define the data.\n\n\n\nShow code\nRSQLite::dbWriteTable(hosp,\n                      \"patients\",\n                      patients)\nRSQLite::dbWriteTable(hosp,\n                      \"province_names\",\n                      province_names)\nRSQLite::dbWriteTable(hosp,\n                      \"admissions\",\n                      admissions)\n\n\nVerify the three data sets are in the database using:\n\ndbListTables() to list the tables within the hosp database.\n\n\n\nShow code\nRSQLite::dbListTables(hosp)\n\n\n[1] \"admissions\"     \"patients\"       \"province_names\"\n\n\n\n\n\n\n\n1. Show the first ten rows of patients data.\n\n1.01.1 R1.2 SQL\n\n\nContents\n\n1.1 Solution in R\n1.2 Solution in SQL\n\n\n\nIn R use:\n\nhead() to view the first 10 rows of the patients data.\n\n\n\nShow code\nutils::head(patients, 10)\n\n\n   patient_id first_name  last_name gender birth_date            city\n1           1     Donald Waterfield      M 1963-02-12          Barrie\n2           2     Mickey     Baasha      M 1981-05-28          Dundas\n3           3       Jiji     Sharma      M 1957-09-05        Hamilton\n4           4      Blair       Diaz      M 1967-01-07        Hamilton\n5           5    Charles      Wolfe      M 2017-11-19         Orillia\n6           6        Sue     Falcon      F 2017-09-30            Ajax\n7           7     Thomas     ONeill      M 1993-01-31      Burlington\n8           8      Sonny    Beckett      M 1952-12-11 Port Hawkesbury\n9           9     Sister    Spitzer      F 1966-10-15         Toronto\n10         10     Cedric   Coltrane      M 1961-11-10         Toronto\n   province_id  allergies height weight\n1           ON       NULL    156     65\n2           ON      Sulfa    185     76\n3           ON Penicillin    194    106\n4           ON       NULL    191    104\n5           ON Penicillin     47     10\n6           ON Penicillin     43      5\n7           ON       NULL    180    117\n8           NS       NULL    174    105\n9           ON Penicillin    173     95\n10          ON       NULL    157     61\n\n\n\n\nIn R use:\n\ndbGetQuery() to run SQL commands from a given data base.\n\nIn SQL use:\n\nSELECT to select.\n* to include all columns.\nFROM to define the patients data for select to include all columns from.\nLIMIT to only show the top ten rows.\n\n\n\nShow code\nRSQLite::dbGetQuery(hosp, \n                    \"SELECT * \n                     FROM patients \n                     LIMIT 10\"\n                    )\n\n\n   patient_id first_name  last_name gender birth_date            city\n1           1     Donald Waterfield      M 1963-02-12          Barrie\n2           2     Mickey     Baasha      M 1981-05-28          Dundas\n3           3       Jiji     Sharma      M 1957-09-05        Hamilton\n4           4      Blair       Diaz      M 1967-01-07        Hamilton\n5           5    Charles      Wolfe      M 2017-11-19         Orillia\n6           6        Sue     Falcon      F 2017-09-30            Ajax\n7           7     Thomas     ONeill      M 1993-01-31      Burlington\n8           8      Sonny    Beckett      M 1952-12-11 Port Hawkesbury\n9           9     Sister    Spitzer      F 1966-10-15         Toronto\n10         10     Cedric   Coltrane      M 1961-11-10         Toronto\n   province_id  allergies height weight\n1           ON       NULL    156     65\n2           ON      Sulfa    185     76\n3           ON Penicillin    194    106\n4           ON       NULL    191    104\n5           ON Penicillin     47     10\n6           ON Penicillin     43      5\n7           ON       NULL    180    117\n8           NS       NULL    174    105\n9           ON Penicillin    173     95\n10          ON       NULL    157     61\n\n\n\n\n\n\n\n2. Show total patients admitted.\n\n2.02.1 R2.2 SQL\n\n\nContents\n\n2.1 Solution in R\n2.2 Solution in SQL\n\n\n\nIn R use:\n\ndata.frame() to define a column for total_admissions.\nnrow() to count the rows in the patients data which will equal the total_admissions.\n\n\n\nShow code\nbase::data.frame(\"total_admissions\" = base::nrow(patients))\n\n\n  total_admissions\n1             1000\n\n\n\n\nIn SQL use:\n\nSELECT to select.\nCOUNT(*) to count the total number of rows.\nAS to define that count as a new variable, total_admissions.\nFROM to define the patients data for select to count the total numbers of rows for, and define as total_admissions.\n\n\n\nShow code\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT COUNT(*) AS total_admissions \n                     FROM patients\"\n                    )\n\n\n  total_admissions\n1             1000\n\n\n\n\n\n\n\n3. Show first and last name as full_name.\n\n3.03.1 R3.2 SQL\n\n\nContents\n\n3.1 Solution in R\n3.2 Solution in SQL\n\n\n\nIn R use:\n\nhead() to show the first 10 rows of data.\ndata.frame() to define a data frame that includes full_name.\npaste0() to paste together the first_name, a space, and the last_name. This will equal the full_name.\n\n\n\nShow code\nutils::head(\n  base::data.frame(\n    full_name = base::paste0(patients$first_name, \n                             \" \", \n                             patients$last_name)), \n  10)\n\n\n           full_name\n1  Donald Waterfield\n2      Mickey Baasha\n3        Jiji Sharma\n4         Blair Diaz\n5      Charles Wolfe\n6         Sue Falcon\n7      Thomas ONeill\n8      Sonny Beckett\n9     Sister Spitzer\n10   Cedric Coltrane\n\n\n\n\nIn SQL use:\n\nSELECT to select.\n|| to concatenate first_name, space, and last name.\nAS to define the concatination as full_name.\nFROM to define the patients data for select to concatenate data from.\nLIMIT to show the first 10 rows of data.\n\n\n\nShow code\nRSQLite::dbGetQuery(hosp,\n  \"SELECT first_name || ' ' || last_name AS full_name \n   FROM patients \n   LIMIT 10\"\n   )\n\n\n           full_name\n1  Donald Waterfield\n2      Mickey Baasha\n3        Jiji Sharma\n4         Blair Diaz\n5      Charles Wolfe\n6         Sue Falcon\n7      Thomas ONeill\n8      Sonny Beckett\n9     Sister Spitzer\n10   Cedric Coltrane\n\n\n\n\n\n\n\n4. Show unique cities that are in province_id ‘NS’?\n\n4.04.1 R4.2 SQL\n\n\nContents\n\n4.1 Solution in R\n4.2 Solution in SQL\n\n\n\nIn R define the patients data then use:\n\n%&gt;% to pipe data.\nfilter() to filter for all province_id that is equal to “NS”.\nsummerise() to define unique_cites.\nunique() to remove duplicate elements of the city column which will be defined as unique_cites.\n\n\n\nShow code\npatients %&gt;% \n  filter(province_id == \"NS\") %&gt;%\n  summarise(unique_cities = unique(city)) \n\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n    unique_cities\n1 Port Hawkesbury\n2         Halifax\n\n\n\n\nIn SQL:\n\nSELECT to select.\nDISTINCT() to define city as the column to remove duplicates from.\nAS to define those cities as unique_cites.\nFROM to define the patients data for select to get unique_cites from.\nWHERE specifies a condition.\nIS is the condition that ‘NS’ is equal to province_id.\n\n\n\nShow code\ndbGetQuery(hosp,\n           \"SELECT DISTINCT(city) AS unique_cities \n            FROM patients \n            WHERE province_id IS 'NS'\"\n           )\n\n\n    unique_cities\n1 Port Hawkesbury\n2         Halifax\n\n\n\n\n\n\n\n5. Show the total number of male patients and the total number of female patients.\nDisplay the two results in the same row.\n\n5.05.1 R5.2 SQL\n\n\nContents\n\n5.1 Solution in R\n5.2 Solution in SQL\n\n\n\nIn R use:\n\ndata.frame() to create a data frame with two columns: male_count, and female_count.\nlength() to count the length of the input.\nwhich() to indicate which gender equals “M” or “F”, and counts that length accordingly.\n\n\n\nShow code\nbase::data.frame(\n  male_count = base::length(base::which(patients$gender == 'M')),\n  female_count = base::length(base::which(patients$gender == 'F'))\n  )\n\n\n  male_count female_count\n1        543          457\n\n\n\n\nIn SQL use:\n\nSELECT to select.\nCOUNT(*) to count all input values.\nFROM to define the patients data for select to count on.\nWHERE is a condition defined as gender = “M” or “F”.\nAS is defining the count as male_count, or female_count respectively.\n\n\n\nShow code\ndbGetQuery(hosp,\n           \"SELECT \n           (SELECT COUNT(*) FROM patients WHERE gender = 'M') AS male_count, \n           (SELECT COUNT(*) FROM patients WHERE gender = 'F') AS female_count\")\n\n\n  male_count female_count\n1        543          457\n\n\n\n\n\n\n\n6. Show all allergies ordered by popularity. Remove NULL values from the query.\n\n6.06.1 R6.2 SQL\n\n\nContents\n\n6.1 Solution in R\n6.2 Solution in SQL\n\n\n\nIn R define the patients data then use:\n\n%&gt;% pipe the data.\nfilter() to subset data to all allergies that aren’t “NULL”.\ngroup_by() to convert the table into one that is grouped by allergies.\nsummarise() to define total_diagnosis.\nn() to count the size of each group.\narrange() to define how the data is arranged.\ndesc() to define that the data is arranged in descending order by total_diagnosis.\n\n\n\nShow code\npatients %&gt;% \n  dplyr::filter(allergies != \"NULL\") %&gt;%\n  dplyr::group_by(allergies) %&gt;%\n  dplyr::summarise(total_diagnosis = dplyr::n()) %&gt;%\n  dplyr::arrange(dplyr::desc(total_diagnosis)) %&gt;%\n  utils::head(10)\n\n\n# A tibble: 10 × 2\n   allergies   total_diagnosis\n   &lt;chr&gt;                 &lt;int&gt;\n 1 Penicillin              230\n 2 Codeine                  58\n 3 Sulfa                    35\n 4 ASA                      16\n 5 Sulfa Drugs              13\n 6 Tylenol                  11\n 7 Wheat                    11\n 8 Peanuts                  10\n 9 Bee Stings                9\n10 Iodine                    9\n\n\n\n\nIn SQL:\n\nSELECT the allergies column.\nCOUNT(*) count all input.\nAS to define the count as total_diagnosis.\nFROM to define the patients data to select the allergies column from.\nWHERE to define a condition.\nIS NOT is the condition that says allergies cannot be ‘NULL’.\nGROUP BY groups the data by allergies.\nOrder BY to define how the data order is output.\nDESC is the given condition that the data is ordered in descending order by total_diagnosis.\nLIMIT to limit output to the first 10 rows.\n\n\n\nShow code\ndbGetQuery(hosp,\n           \"SELECT allergies,\n            COUNT(*) AS total_diagnosis\n            FROM patients\n            WHERE allergies IS NOT 'NULL'\n            GROUP BY allergies\n            ORDER BY total_diagnosis DESC\n            LIMIT 10\"\n           )\n\n\n     allergies total_diagnosis\n1   Penicillin             230\n2      Codeine              58\n3        Sulfa              35\n4          ASA              16\n5  Sulfa Drugs              13\n6        Wheat              11\n7      Tylenol              11\n8      Peanuts              10\n9       Iodine               9\n10  Bee Stings               9\n\n\n\n\n\n\n\n7. Display the total number of patients for each province. Order by descending.\n\n7.07.1 R7.1 SQL\n\n\nContents\n\n7.1 Solution in R\n7.2 Solution in SQL\n\n\n\nIn R use:\n\nmerge() to join the data sets province_names, and patients by “province_id”.\n%&gt;% to pipe the data.\ngroup_by() to group by province name.\nsummarise() to define the patient count.\nn() to count the number of patients in each province. This will equal patient_count.\narrange() to define how the data is arranged.\ndesc() to define that the data is arranged in descending order by patient_count.\n\n\n\nShow code\nbase::merge(province_names, patients, by = \"province_id\") %&gt;%\n  dplyr::group_by(province_name) %&gt;%\n  dplyr::summarise(patient_count = dplyr::n()) %&gt;%\n  dplyr::arrange(dplyr::desc(patient_count))\n\n\n# A tibble: 8 × 2\n  province_name             patient_count\n  &lt;chr&gt;                             &lt;int&gt;\n1 Ontario                             954\n2 Alberta                              14\n3 British Columbia                     11\n4 Nova Scotia                           9\n5 Manitoba                              7\n6 Newfoundland and Labrador             2\n7 Quebec                                2\n8 Saskatchewan                          1\n\n\n\n\nIn SQL use:\n\nSELECT select the province_name column.\nCOUNT(*) to count all input.\nAS to define count as patient_count.\nFROM to define the patients data AS pa to select province_name to count the number of patients from.\nJOIN to join the province_names data AS pr.\nON is the clause to join data based on pr.province_id = pa.province_id.\nGROUP BY to group the data by pr.province_id.\nORDER BY to define the order of the data output.\nDESC defines that the output data be arranged in order of descending patient_count.\n\n\n\nShow code\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT province_name,\n                     COUNT(*) AS patient_count\n                     FROM patients pa\n                     JOIN province_names pr ON pr.province_id = pa.province_id\n                     GROUP BY pr.province_id\n                     ORDER BY patient_count DESC\")\n\n\n              province_name patient_count\n1                   Ontario           954\n2                   Alberta            14\n3          British Columbia            11\n4               Nova Scotia             9\n5                  Manitoba             7\n6                    Quebec             2\n7 Newfoundland and Labrador             2\n8              Saskatchewan             1\n\n\n\n\n\n\n\n8. Show the provinces that have more patients identified as ‘M’ than ‘F’. Must only show full province_name.\n\n8.08.1 R8.2 SQL\n\n\nContents\n\n8.1 Solution in R\n8.2 Solution in SQL\n\n\n\nIn R use,\n\nmerge() to join the data sets province_names, and patients by “province_id”.\ngroup_by() to group by province name.\ncount() to count the number of “M” and “F” patients for each province.\nslice() to remove certain rows based on a given criteria.\nwhich.max() to determine which province has a greater number of male patients.\nsummarise() to define only province_name in the output.\n\n\n\nShow code\nbase::merge(province_names, patients, by = \"province_id\") %&gt;%\n  dplyr::group_by(province_name) %&gt;%\n  dplyr::count(gender == \"M\", gender == \"F\") %&gt;%\n  dplyr::slice(base::which.max(n)) %&gt;%\n  dplyr::summarise(province_name = province_name)\n\n\n# A tibble: 8 × 1\n  province_name            \n  &lt;chr&gt;                    \n1 Alberta                  \n2 British Columbia         \n3 Manitoba                 \n4 Newfoundland and Labrador\n5 Nova Scotia              \n6 Ontario                  \n7 Quebec                   \n8 Saskatchewan             \n\n\n\n\nIn SQL use,\n\nSELECT to select pr.province_name column.\nFROM to define the patients data to select pr.province_name from.\nAS to define patients data as pa.\nJOIN to join pa.province data with pr.province_name data.\nAS to define province_name data as pr.\nON to join pa.patients and pr.province_names data by province_id.\nGROUP BY to group data by province_id.\nHAVING to define a clause to filter the data where the number of “M” patients is greater than “F” patients.\nCOUNT() to count the given input.\nCASE to go through the condition of gender = “M” or when gender = “F”\nWHEN to preform the count when the condition is true.\nTHEN 1 END to add a 1 to the count when the case is met.\n\n\n\nShow code\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT pr.province_name\n                     FROM patients AS pa\n                     JOIN province_names AS pr ON pa.province_id = pr.province_id\n                     GROUP BY pr.province_id\n                     HAVING\n                     COUNT( CASE WHEN gender = 'M' THEN 1 END) &gt;\n                     COUNT( CASE WHEN gender = 'F' THEN 1 END)\")\n\n\n              province_name\n1                   Alberta\n2          British Columbia\n3                  Manitoba\n4 Newfoundland and Labrador\n5               Nova Scotia\n6                   Ontario\n7                    Quebec\n8              Saskatchewan\n\n\n\n\n\n\n\n9. Each admission costs $50 for patients without insurance, and $10 for patients with insurance. All patients with an even patient_id have insurance.\nGive each patient a ‘Yes’ if they have insurance, and a ‘No’ if they don’t have insurance. Add up the admission_total cost for each has_insurance group.\n\n9.09.1 R9.2 SQL\n\n\nContents\n\n9.1 Solution in R\n9.2 Solution in SQL\n\n\n\nIn R use,\n\n%&gt;% to pipe the data.\nmutate() to mutate the data to include has_insurance, and cost_after_insurance information.\ncase_when() to define a case where if the patient id is odd then they don’t have insurance, and if they are even they do have insurance.\ngroup_by() to group the data by has_insurance.\nsummarize() to define cost_after_insurance.\nsum() to add up the cost for all those with and without insurance.\n\n\n\nShow code\npatients %&gt;%\n  dplyr::mutate(\n    has_insurance = dplyr::case_when(\n    patient_id %%2==1 ~ \"Yes\",\n    patient_id %%2!=1 ~ \"No\"\n  ),cost_after_insurance = dplyr::case_when(\n    has_insurance == \"Yes\" ~ 10,\n    has_insurance == \"No\" ~ 50\n  )) %&gt;%\n  group_by(has_insurance)  %&gt;%\n  summarise(cost_after_insurance = base::sum(cost_after_insurance))\n\n\n# A tibble: 2 × 2\n  has_insurance cost_after_insurance\n  &lt;chr&gt;                        &lt;dbl&gt;\n1 No                           25000\n2 Yes                           5000\n\n\n\n\nIn SQL use,\n\nSELECT to select.\nCASE to define a case.\nWHEN to select when a patient_id is even.\nTHEN to set insurance to “Yes” if patient_id is even.\nELSE to set has_insurance to “No” when patient_id is not even.\nEND to end the case.\nAS to define the first case values as has_insurance.\nSUM() to add up input.\nCASE to define another case.\nWHEN to select when a patient has an even id.\nTHEN to set cost_after_insurance to 10 if the patient id is even.\nELSE to set the cost_after_insurance to 50 if the patient id is not even.\nEND to end the case.\nAS to define the second case values as cost_after_insurance.\nFROM to define the patients data to select and figure out insurance costs for.\nGROUP BY to group data by has_insurance.\n\n\n\nShow code\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT \n                        CASE WHEN patient_id % 2 = 0 Then 'Yes'\n                        ELSE 'No' \n                        END as has_insurance,\n                     SUM(\n                        CASE WHEN patient_id % 2 = 0 Then 10\n                        ELSE 50 \n                        END\n                         ) AS cost_after_insurance\n                     FROM patients \n                     GROUP BY has_insurance;\")\n\n\n  has_insurance cost_after_insurance\n1            No                25000\n2           Yes                 5000\n\n\n\n\n\n\n\n10. We are looking for a specific patient. Pull all columns for the patient who matches the following criteria:\n\nFirst_name contains a ‘b’ after the first two letters.\nIdentifies their gender as ‘F’\nTheir weight would be between 50 kg and 70 kg\nTheir patient_id is an odd number\nThey are from the city ‘Burlington’\n\n\n10.010.1 R10.2 SQL\n\n\nContents\n\n10.1 Solution in R\n10.2 Solution in SQL\n\n\n\nIn R define the patients data then use,\n\n%&gt;% to pipe the data.\nfilter to filter the database on first name, gender, weight, patient_id, and city.\ngrepl to select first_names where the 3rd letter is b.\n\n\n\nShow code\npatients %&gt;%\n  dplyr::filter(base::grepl(\"^.{2}[b]\", first_name),\n         gender == \"F\",\n         weight &gt; 50 & weight &lt; 80,\n         patient_id %%2==1,\n         city == \"Burlington\")\n\n\n  patient_id first_name last_name gender birth_date       city province_id\n1        695    Sabrina    Hettie      F 2000-11-25 Burlington          ON\n   allergies height weight\n1 Penicillin    160     51\n\n\n\n\nIn SQL use:\n\nSELECT to select all * columns.\nFROM to select all columns from the patients data.\nWhere defines multiple conditions.\nLIKE is a condition where first_name has the third letter equal to a lower case b.\nAND to define multiple conditions such as gender, patient_id, and city.\nBETWEEN to define weight is greater than 50, but less than 70.\n\n\n\nShow code\nRSQLite::dbGetQuery(hosp,\n                    \"SELECT *\n                     FROM patients\n                     WHERE\n                      first_name LIKE '__b%'\n                      AND gender = 'F'\n                      AND weight BETWEEN 50 AND 70\n                      AND patient_id % 2 = 1\n                      AND city = 'Burlington';\")\n\n\n  patient_id first_name last_name gender birth_date       city province_id\n1        695    Sabrina    Hettie      F 2000-11-25 Burlington          ON\n   allergies height weight\n1 Penicillin    160     51"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220425-Workshop-AggregateMakingTableswithgt/index.html",
    "href": "blog/Technical-Blog/posts/20220425-Workshop-AggregateMakingTableswithgt/index.html",
    "title": "Portland R User Group: Aggregate making tables with gt packag",
    "section": "",
    "text": "Ted Landeras led this event for Portland R user Group where we watched Rich Lannone|| Making Beautiful Tables with {gt}|| RStudio, and other examples."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220425-Workshop-AggregateMakingTableswithgt/index.html#example-1",
    "href": "blog/Technical-Blog/posts/20220425-Workshop-AggregateMakingTableswithgt/index.html#example-1",
    "title": "Portland R User Group: Aggregate making tables with gt packag",
    "section": "Example 1",
    "text": "Example 1\n\n\nShow code\nhead(rock)\n\n\n  area    peri     shape perm\n1 4990 2791.90 0.0903296  6.3\n2 7002 3892.60 0.1486220  6.3\n3 7558 3930.66 0.1833120  6.3\n4 7352 3869.32 0.1170630  6.3\n5 7943 3948.54 0.1224170 17.1\n6 7979 4010.15 0.1670450 17.1\n\n\n\n\nShow code\nrock %&gt;% # Get 'rock' data\n  head(5) %&gt;% # First 5 lines only\n  gt() # Make a table, it just works.\n\n\n\n\n\n\n\n\n\narea\nperi\nshape\nperm\n\n\n\n\n4990\n2791.90\n0.0903296\n6.3\n\n\n7002\n3892.60\n0.1486220\n6.3\n\n\n7558\n3930.66\n0.1833120\n6.3\n\n\n7352\n3869.32\n0.1170630\n6.3\n\n\n7943\n3948.54\n0.1224170\n17.1"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220425-Workshop-AggregateMakingTableswithgt/index.html#example-2",
    "href": "blog/Technical-Blog/posts/20220425-Workshop-AggregateMakingTableswithgt/index.html#example-2",
    "title": "Portland R User Group: Aggregate making tables with gt packag",
    "section": "Example 2",
    "text": "Example 2\n\n\nShow code\nhead(BOD)\n\n\n  Time demand\n1    1    8.3\n2    2   10.3\n3    3   19.0\n4    4   16.0\n5    5   15.6\n6    7   19.8\n\n\n\n\nShow code\nBOD %&gt;% # Get the data...\ngt() %&gt;% # use 'gt' to make an awesome table...\n  tab_header( \n    title = \"BOD Table Woooooo!\", # ...with this title\n    subtitle = \"Hooray gt!\") %&gt;% # and this subtitle\n  fmt_number( # A column (numeric data)\n    columns = vars(Time), # What column variable? BOD$Time\n    decimals = 2 # With two decimal places\n    ) %&gt;% \n  fmt_number( # Another column (also numeric data)\n    columns = vars(demand), # What column variable? BOD$demand\n    decimals = 1 # I want this column to have one decimal place\n  ) %&gt;% \n  cols_label(Time = \"Time (hours)\", demand = \"Demand (mg/L)\") # Update labels\n\n\n\n\n\n\n\n\n\nBOD Table Woooooo!\n\n\nHooray gt!\n\n\nTime (hours)\nDemand (mg/L)\n\n\n\n\n1.00\n8.3\n\n\n2.00\n10.3\n\n\n3.00\n19.0\n\n\n4.00\n16.0\n\n\n5.00\n15.6\n\n\n7.00\n19.8"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220425-Workshop-AggregateMakingTableswithgt/index.html#example-3",
    "href": "blog/Technical-Blog/posts/20220425-Workshop-AggregateMakingTableswithgt/index.html#example-3",
    "title": "Portland R User Group: Aggregate making tables with gt packag",
    "section": "Example 3",
    "text": "Example 3\n\n\nShow code\nBOD %&gt;% # Get the data...\ngt() %&gt;% # use 'gt' to make an awesome table...\n  tab_header( \n    title = \"BOD Table Woooooo!\", # ...with this title\n    subtitle = \"Hooray gt!\") %&gt;% # and this subtitle\n  fmt_number( # A column (numeric data)\n    columns = vars(Time), # What column variable? BOD$Time\n    decimals = 2 # With two decimal places\n    ) %&gt;% \n  fmt_number( # Another column (also numeric data)\n    columns = vars(demand), # What column variable? BOD$demand\n    decimals = 1 # I want this column to have one decimal place\n  ) %&gt;% \n  cols_label(Time = \"Time (hours)\", demand = \"Demand (mg/L)\") # Update labels\n\n\n\n\n\n\n\n\n\nBOD Table Woooooo!\n\n\nHooray gt!\n\n\nTime (hours)\nDemand (mg/L)\n\n\n\n\n1.00\n8.3\n\n\n2.00\n10.3\n\n\n3.00\n19.0\n\n\n4.00\n16.0\n\n\n5.00\n15.6\n\n\n7.00\n19.8"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220425-Workshop-AggregateMakingTableswithgt/index.html#example-4",
    "href": "blog/Technical-Blog/posts/20220425-Workshop-AggregateMakingTableswithgt/index.html#example-4",
    "title": "Portland R User Group: Aggregate making tables with gt packag",
    "section": "Example 4",
    "text": "Example 4\n\n\nShow code\ntooth_length &lt;- ToothGrowth %&gt;% \n  group_by(supp, dose) %&gt;% \n  summarize(\n    mean_len = mean(len)\n  ) %&gt;% \n  as_tibble() \n\n# A gt table: \ntooth_length %&gt;% # Take tooth_length\n  gt() %&gt;% # Make a gt table with it\n  tab_header(\n    title = \"A title just like that\", # Add a title\n    subtitle = \"(with something below it!)\" # And a subtitle\n  ) %&gt;%\n  fmt_passthrough( # Not sure about this but it works...\n    columns = vars(supp) # First column: supp (character)\n  ) %&gt;% \n  fmt_number(\n    columns = vars(mean_len), # Second column: mean_len (numeric)\n    decimals = 2 # With 4 decimal places\n  ) %&gt;% \n  fmt_number(\n    columns = vars(dose), # Third column: dose (numeric)\n    decimals = 2 # With 2 decimal places\n  ) %&gt;% \n  data_color( # Update cell colors...\n    columns = vars(supp), # ...for supp column!\n    colors = scales::col_factor( # &lt;- bc it's a factor\n      palette = c(\n        \"green\",\"cyan\"), # Two factor levels, two colors\n      domain = c(\"OJ\",\"VC\")# Levels\n  )\n  ) %&gt;% \n  data_color( # Update cell colors...\n    columns = vars(dose), # ...for dose column \n    colors = scales::col_numeric( # &lt;- bc it's numeric\n      palette = c(\n        \"yellow\",\"orange\"), # A color scheme (gradient)\n      domain = c(0.5,2) # Column scale endpoints\n  )\n  ) %&gt;% \n  data_color( # Update cell colors...\n    columns = vars(mean_len), # ...for mean_len column\n    colors = scales::col_numeric(\n      palette = c(\n        \"red\", \"purple\"), # Overboard colors! \n      domain = c(7,27) # Column scale endpoints\n  )\n  ) %&gt;% \n  cols_label(supp = \"Supplement\", dose = \"Dosage (mg/d)\", mean_len = \"Mean Tooth Length\") %&gt;% # Make the column headers\n  tab_footnote(\n    footnote = \"Baby footnote test\", # This is the footnote text\n    locations = cells_column_labels(\n      columns = vars(supp) # Associated with column 'supp'\n      )\n    ) %&gt;% \n    tab_footnote(\n    footnote = \"A second footnote\", # Another line of footnote text\n    locations = cells_column_labels( \n      columns = vars(dose) # Associated with column 'dose'\n      )\n    )\n\n\n\n\n\n\n\n\n\nA title just like that\n\n\n(with something below it!)\n\n\nSupplement1\nDosage (mg/d)2\nMean Tooth Length\n\n\n\n\nOJ\n0.50\n13.23\n\n\nOJ\n1.00\n22.70\n\n\nOJ\n2.00\n26.06\n\n\nVC\n0.50\n7.98\n\n\nVC\n1.00\n16.77\n\n\nVC\n2.00\n26.14\n\n\n\n1 Baby footnote test\n\n\n2 A second footnote"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221107-Original-SportsObserveR/index.html",
    "href": "blog/Technical-Blog/posts/20221107-Original-SportsObserveR/index.html",
    "title": "SportsObserveR - Part 1: Scrapping Functions",
    "section": "",
    "text": "In this tutorial I will be creating functions to scrape NBA data. The goal here is to prepare these functions to use in a package for future analysis."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221107-Original-SportsObserveR/index.html#about-the-data",
    "href": "blog/Technical-Blog/posts/20221107-Original-SportsObserveR/index.html#about-the-data",
    "title": "SportsObserveR - Part 1: Scrapping Functions",
    "section": "0_1. About-The-Data",
    "text": "0_1. About-The-Data\nI will be scrapping data from Basketball Reference which gets thier data updated regularly by a handful of contributors and sources. The main reasons I like using this data is because it’s reliable, updated regularly, and similar sites exist for other non-NBA Sports (such as: WNBA, Baseball, Football, and others) if I wanted to expand my research outside the NBA."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221107-Original-SportsObserveR/index.html#package-installs",
    "href": "blog/Technical-Blog/posts/20221107-Original-SportsObserveR/index.html#package-installs",
    "title": "SportsObserveR - Part 1: Scrapping Functions",
    "section": "0_2. Package Installs",
    "text": "0_2. Package Installs\nThe packages I will be using are rvest to scrape the data and magrittr to pipe it. To install these packages, copy the code below and remove the first comment hash (command - shift - c).\n\n\nShow code\n## install packages\n# install.packages(\"rvest\",  \"magrittr\")\n\n\nThen load:\n\n\nShow code\n# load packages \nlibrary(rvest) \nlibrary(magrittr)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221107-Original-SportsObserveR/index.html#team-statistics",
    "href": "blog/Technical-Blog/posts/20221107-Original-SportsObserveR/index.html#team-statistics",
    "title": "SportsObserveR - Part 1: Scrapping Functions",
    "section": "1_1. Team Statistics:",
    "text": "1_1. Team Statistics:\nThe first function I’m creating scrapes team statistics, which will need the user to input the teams url slug, the year that team attended or attends the NBA playoffs, and the stats_tb or statistics table that corresponds to what is shown on Basketball Reference. Currently not all tables work, but it should work for: #per_game, #totals, #per_36_minutes, and #advanced.\n\n\nShow code\nscrape_team_data &lt;- function(slug, year, stats_tb){\n    \"\n  A function that returns a data frame of team statistics. \n  \n  @param slug is string of three letters that represents the teams url. \n  @param year is a string that corresponds to the NBA finals.\n  @param stats_tb is a string that corresponds to the statistics table on BasketBall Reference such as #per_game, #totals, #per_36_minutes, and #advanced\n  \n  @return a df of team statistics\n  \"\n  # define team page URL\n  url &lt;- base::paste0(\"https://www.basketball-reference.com/teams/\",\n                slug,\"/\", year, \".html\")\n  \n  # Read stats table\n  stats_tb &lt;- url %&gt;%\n  read_html %&gt;%\n  html_node(stats_tb) %&gt;% \n  html_table()\n  \n  # Rename Column 2 to Name \n  base::names(stats_tb)[2] &lt;- \"Name\"\n  \n  # Replace NA values with 0 (for stat functions)\n  stats_tb[base::is.na(stats_tb)] &lt;- 0\n  \n  # make data frame\n  df &lt;- base::data.frame(stats_tb)\n  base::return(df)\n  }\n\n\n\nExamples\n\nA. Current Blazers Roster\n\n\nShow code\nzers_roster &lt;- scrape_team_data(\"POR\",\"2022\",\"#roster\")\nutils::head(zers_roster)\n\n\n  No.              Name Pos  Ht  Wt         Birth.Date Var.7 Exp\n1  21    Keljin Blevins  SF 6-4 200  November 24, 1995    us   1\n2   4    Greg Brown III  SF 6-7 205  September 1, 2001    us   R\n3  33  Robert Covington  PF 6-7 209  December 14, 1990    us   8\n4  34 Jarron Cumberland  SG 6-5 205 September 22, 1997    us   R\n5  18         Kris Dunn  PG 6-3 205     March 18, 1994    us   5\n6  16         CJ Elleby  SF 6-6 200      June 16, 2000    us   1\n                       College\n1 Southern Miss, Montana State\n2                        Texas\n3              Tennessee State\n4                   Cincinnati\n5                   Providence\n6             Washington State\n\n\n\n\nB. 1997 Chicago Bulls Total Statistics\n\n\nShow code\nbulls_totals &lt;- scrape_team_data(\"CHI\", \"1998\", \"#totals\")\nutils::head(bulls_totals)\n\n\n  Rk           Name Age  G GS   MP  FG  FGA   FG. X3P X3PA  X3P. X2P X2PA  X2P.\n1  1 Michael Jordan  34 82 82 3181 881 1893 0.465  30  126 0.238 851 1767 0.482\n2  2  Dennis Rodman  36 80 66 2856 155  360 0.431   4   23 0.174 151  337 0.448\n3  3     Ron Harper  34 82 82 2284 293  665 0.441  16   84 0.190 277  581 0.477\n4  4     Toni Kukoč  29 74 52 2235 383  841 0.455  63  174 0.362 320  667 0.480\n5  5    Luc Longley  29 58 58 1703 277  609 0.455   0    0 0.000 277  609 0.455\n6  6 Scottie Pippen  32 44 44 1652 315  704 0.447  61  192 0.318 254  512 0.496\n   eFG.  FT FTA   FT. ORB DRB  TRB AST STL BLK TOV  PF  PTS\n1 0.473 565 721 0.784 130 345  475 283 141  45 185 151 2357\n2 0.436  61 111 0.550 421 780 1201 230  47  18 147 238  375\n3 0.453 162 216 0.750 107 183  290 241 108  48  91 181  764\n4 0.493 155 219 0.708 121 206  327 314  76  37 154 149  984\n5 0.455 109 148 0.736 113 228  341 161  34  62 130 206  663\n6 0.491 150 193 0.777  53 174  227 254  79  43 109 116  841\n\n\nHere we can see when Michael Jordan won his 6th ring with the Chicago Bulls he was also the leagues leading point scorer with 2,357 total points that season. Dennis Rodman was also a league leader that season in rebounds collecting a total of 1,201 rebounds."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221107-Original-SportsObserveR/index.html#player-statistics",
    "href": "blog/Technical-Blog/posts/20221107-Original-SportsObserveR/index.html#player-statistics",
    "title": "SportsObserveR - Part 1: Scrapping Functions",
    "section": "1_2. Player Statistics",
    "text": "1_2. Player Statistics\nThe second function will scrape player statistics. The user will need to input the players name, and the stats_tb or statistics table that corresponds to what is shown on Basketball Reference. Currently not all tables work, but it should work for: #per_game, #totals, #per_36_minutes, and #advanced.\n\n\nShow code\nscrape_player_data &lt;- function(name, stats_tb){\n  \"\n  A function that returns a data frame of player statistics. \n  \n  @param name is a string that represnets an NBA players name\n  @param stats_tb is a string that corresponds to the statistics table on BasketBall Reference such as #per_game, #totals, #per_36_minutes, and #advanced\n  \n  @return a df of player statistics\n  \"\n  # make name lower case\n  lower_case_name &lt;- base::tolower(name)\n\n  # split name \n  split_name &lt;- base::strsplit(lower_case_name, \" +\")[[1]]\n\n  # define first and last name\n  first_name &lt;- split_name[[1]]\n  last_name &lt;- split_name[[2]]\n  \n  # first letter of last name\n  letter &lt;- base::substr(last_name, 1,1)\n  \n  # first five letters of last name \n  last_5 &lt;- base::substr(last_name, 1, 5)\n  \n  # first two letters of first name\n  first_2 &lt;- base::substr(first_name, 1,2)\n  \n  # define team page URL\n  url &lt;- base::paste0(\"https://www.basketball-reference.com/players/\",letter ,\"/\",last_5,first_2,\"01.html\")\n  \n  # Read stats table\n  stats_tb &lt;- url %&gt;%\n  read_html %&gt;%\n  html_node(stats_tb) %&gt;% \n  html_table()\n  \n  # Rename Column 2 to Name \n  names(stats_tb)[2] &lt;- \"Name\"\n  \n  # Replace NA values with 0 (for stat functions)\n  stats_tb[base::is.na(stats_tb)] &lt;- 0\n  \n  # make list a dataframe\n  df &lt;- base::data.frame(stats_tb)\n  \n  base::return(df)\n  }\n\n\n\nExamples\n\nC. Allen Iverson Per Game Stats\n\n\nShow code\nai_per_game &lt;- scrape_player_data(\"Allen Iverson\", \"#per_game\")\nhead(ai_per_game)\n\n\n   Season Name  Tm  Lg Pos  G GS   MP   FG  FGA  FG. X3P X3PA X3P. X2P X2PA\n1 1996-97   21 PHI NBA  PG 76 74 40.1  8.2 19.8 .416 2.0  6.0 .341 6.2 13.8\n2 1997-98   22 PHI NBA  PG 80 80 39.4  8.1 17.6 .461 0.9  2.9 .298 7.2 14.7\n3 1998-99   23 PHI NBA  SG 48 48 41.5  9.1 22.0 .412 1.2  4.1 .291 7.9 17.9\n4 1999-00   24 PHI NBA  SG 70 70 40.8 10.4 24.8 .421 1.3  3.7 .341 9.1 21.0\n5 2000-01   25 PHI NBA  SG 71 71 42.0 10.7 25.5 .420 1.4  4.3 .320 9.4 21.2\n6 2001-02   26 PHI NBA  SG 60 59 43.7 11.1 27.8 .398 1.3  4.5 .291 9.8 23.4\n  X2P. eFG.  FT  FTA  FT. ORB DRB TRB AST STL BLK TOV  PF  PTS\n1 .448 .467 5.0  7.2 .702 1.5 2.6 4.1 7.5 2.1 0.3 4.4 3.1 23.5\n2 .494 .486 4.9  6.7 .729 1.1 2.6 3.7 6.2 2.2 0.3 3.1 2.5 22.0\n3 .440 .439 7.4  9.9 .751 1.4 3.5 4.9 4.6 2.3 0.1 3.5 2.0 26.8\n4 .435 .446 6.3  8.9 .713 1.0 2.8 3.8 4.7 2.1 0.1 3.3 2.3 28.4\n5 .441 .447 8.2 10.1 .814 0.7 3.1 3.8 4.6 2.5 0.3 3.3 2.1 31.1\n6 .419 .422 7.9  9.8 .812 0.7 3.8 4.5 5.5 2.8 0.2 4.0 1.7 31.4\n                 Awards\n1          MVP-17,ROY-1\n2                      \n3            MVP-4,NBA1\n4         MVP-7,NBA2,AS\n5 MVP-1,NBA1,AS,DPOY-11\n6         MVP-9,NBA2,AS\n\n\nNotice that when Allen Iverson won the NBA’s MVP in 2001 he was putting up about 31 points a game.\n\n\nD. Kareem Abdul-Jabbar Totals\n\n\nShow code\nkaj_totals &lt;- scrape_player_data(\"Kareem Abdul-Jabbar\", \"#totals\")\nutils::head(kaj_totals)\n\n\n   Season Name  Tm  Lg Pos  G GS   MP   FG  FGA   FG. X3P X3PA X3P.  X2P X2PA\n1 1969-70   22 MIL NBA   C 82  0 3534  938 1810 0.518   0    0    0  938 1810\n2 1970-71   23 MIL NBA   C 82  0 3288 1063 1843 0.577   0    0    0 1063 1843\n3 1971-72   24 MIL NBA   C 81  0 3583 1159 2019 0.574   0    0    0 1159 2019\n4 1972-73   25 MIL NBA   C 76  0 3254  982 1772 0.554   0    0    0  982 1772\n5 1973-74   26 MIL NBA   C 81  0 3548  948 1759 0.539   0    0    0  948 1759\n6 1974-75   27 MIL NBA   C 65  0 2747  812 1584 0.513   0    0    0  812 1584\n   X2P.  eFG.  FT FTA   FT. ORB DRB  TRB AST STL BLK TOV  PF  PTS Var.31\n1 0.518 0.518 485 743 0.653   0   0 1190 337   0   0   0 283 2361      0\n2 0.577 0.577 470 681 0.690   0   0 1311 272   0   0   0 264 2596      0\n3 0.574 0.574 504 732 0.689   0   0 1346 370   0   0   0 235 2822      0\n4 0.554 0.554 328 460 0.713   0   0 1224 379   0   0   0 208 2292      0\n5 0.539 0.539 295 420 0.702 287 891 1178 386 112 283   0 238 2191      0\n6 0.513 0.513 325 426 0.763 194 718  912 264  65 212   0 205 1949      0\n  Trp.Dbl\n1       0\n2       1\n3       1\n4       2\n5       3\n6       1"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221107-Original-SportsObserveR/index.html#box-scores",
    "href": "blog/Technical-Blog/posts/20221107-Original-SportsObserveR/index.html#box-scores",
    "title": "SportsObserveR - Part 1: Scrapping Functions",
    "section": "1_3. Box Scores",
    "text": "1_3. Box Scores\nThe last function still needs a bit of work, but will pull box scores of all the NBA games on a given day. The user will need to enter the game_day or day of the games they want box scores for.\nNote: Ideally this function would return a list with each game being its own df, but for now it only prints one data frame that includes all games played on that date. There also seem to be issues when only one game is played, or it is the first game of the season (see examples below), but for now those issues are manageable.\n\n\nShow code\nbox_scores &lt;- function(game_day){\n  \"\n  A function that returns a data frame of box scores. \n  \n  @param game_day is a string that represents the date in the form Y-M-D\n  \n  @return a df of box scores from that day.\n  \"\n  # split by dash\n  split_date &lt;- base::strsplit(game_day, \"-\")\n  \n  # year - month - day \n  year &lt;- split_date[[1]][[1]]\n  month &lt;- split_date[[1]][[2]]\n  day &lt;- split_date[[1]][[3]]\n  \n  #url\n  url &lt;- base::paste0(\"https://www.basketball-reference.com/boxscores/?month=\",\n                month ,\"&day=\", day,\"&year=\", year)\n  \n  # read url\n  html &lt;- read_html(url)\n  \n  # extract all the 'div\" items from the html as tables\n  div &lt;- html %&gt;% \n    html_elements(\"div\") %&gt;% \n    html_table()\n  \n  #remove empties\n  div &lt;- div[base::sapply(div, function(i) dim(i)[1]) &gt; 0]\n  \n  # only keep rows == 7\n  div &lt;- div[base::sapply(div, function(i) nrow(i)[1]) == 7]\n  \n  # empty list\n  my_vec &lt;- base::list()\n  \n  #for loop\n  for(i in 1:base::length(div)) {        \n  my_out &lt;- div[[i]][3:5,] \n  my_vec &lt;- c(my_vec, my_out)\n  df &lt;- base::data.frame(my_vec)\n  }\n  \n  df &lt;- df[-1,]\n  \n  base::return(df)\n}\n\n\n\nExample\n\nE. Box Scores for 10-19-2022 (works correctly)\n\n\nShow code\noct_19 &lt;- box_scores(\"2022-10-19\")\noct_19\n\n\n       X1 X2 X3 X4 X5        X1.1 X2.1 X3.1 X4.1 X5.1    X1.2 X2.2 X3.2 X4.2\n2 Houston 20 30 30 27 New Orleans   32   26   40   32 Orlando   28   27   28\n3 Atlanta 26 33 25 33    Brooklyn   14   36   28   30 Detroit   17   40   34\n  X5.2       X1.3 X2.3 X3.3 X4.3 X5.3     X1.4 X2.4 X3.4 X4.4 X5.4 X6    X1.5\n2   26 Washington   36   24   27   27 New York   23   23   33   29  4 Chicago\n3   22    Indiana   25   27   25   30  Memphis   25   36   24   23  7   Miami\n  X2.5 X3.5 X4.5 X5.5          X1.6 X2.6 X3.6 X4.6 X5.6    X1.7 X2.7 X3.7 X4.7\n2   28   31   37   20 Oklahoma City   22   30   35   21  Dallas   32   30   19\n3   33   26   27   22     Minnesota   35   30   22   28 Phoenix   24   21   31\n  X5.7       X1.8 X2.8 X3.8 X4.8 X5.8        X1.9 X2.9 X3.9 X4.9 X5.9     X1.10\n2   24   Portland   32   19   33   31   Charlotte   38   30   30   31 Cleveland\n3   31 Sacramento   23   32   29   24 San Antonio   22   25   28   27   Toronto\n  X2.10 X3.10 X4.10 X5.10  X1.11 X2.11 X3.11 X4.11 X5.11\n2    22    35    27    21 Denver    30    23    27    22\n3    28    23    25    32   Utah    37    38    19    29\n\n\n\n\nF. Box scores for the first day of the ’22/’23 NBA season (issues)\n\n\nShow code\noct_18 &lt;- box_scores(\"2022-10-18\")\noct_18\n\n\n            X1 X2 X3 X4 X5         X1.1 X2.1 X3.1 X4.1 X5.1                X1.2\n2 Philadelphia 29 34 25 29    LA Lakers   22   30   19   38 Philadelphia 76ers*\n3       Boston 24 39 35 28 Golden State   25   34   32   32  Western Conference\n  X2.2 X3.2 X4.2 X5.2    X6    X7   X8   X9 X10 X11 X12  X13 X14 X15  X16 X17\n2    0    1 .000  1.0 117.0 126.0 &lt;NA&gt; &lt;NA&gt;  NA  NA  NA &lt;NA&gt;  NA  NA &lt;NA&gt;  NA\n3    W    L W/L%   GB  PS/G  PA/G &lt;NA&gt; &lt;NA&gt;  NA  NA  NA &lt;NA&gt;  NA  NA &lt;NA&gt;  NA\n  X18 X19 X20 X21 X22  X23  X24  X25  X26  X27  X28  X29  X30  X31 X32 X33 X34\n2  NA  NA  NA  NA  NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;  NA  NA  NA\n3  NA  NA  NA  NA  NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;  NA  NA  NA\n   X35 X36 X37  X38 X39 X40 X41 X42 X43 X44\n2 &lt;NA&gt;  NA  NA &lt;NA&gt;  NA  NA  NA  NA  NA  NA\n3 &lt;NA&gt;  NA  NA &lt;NA&gt;  NA  NA  NA  NA  NA  NA\n\n\nIssue: For the first game of the season there is an are NA tables that are being pulled in.\nG. First game of the 1992 NBA Finals AKA Michael Jordan’s famous Shrug (issues)\n\n\nShow code\nfinals_92_g1 &lt;- box_scores(\"1992-6-3\")\nfinals_92_g1\n\n\n        X1 X2 X3 X4 X5     X1.1 X2.1 X3.1 X4.1 X5.1\n2 Portland 30 21 17 21 Portland   30   21   17   21\n3  Chicago 33 33 38 18  Chicago   33   33   38   18\n\n\nIssue: For days where only one game is played the one game is printed twice in the data frame."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20211018-Original-TreeDiagrams/index.html",
    "href": "blog/Technical-Blog/posts/20211018-Original-TreeDiagrams/index.html",
    "title": "Tree Diagrams",
    "section": "",
    "text": "In this post I wanted to create a tree diagram based off my Statistics-461 Notes.\n\n\n1. Set-Up\nThis example is from W3-D5 Example 1 of my Statistics-461 Notes, and uses the data.tree package which can create a multiple node object.\n\n\nShow code\nlibrary(data.tree)\n# install.packages(\"DiagrammeR\")\nlibrary(DiagrammeR)\n\n\n\n\n2. Creating a Node Object\nAll trees are constructed by tying together Node object, so to start I will create a new Node object for example 1.\n\n\nShow code\nex1 &lt;- data.tree::Node$new(\"Example 1\")\n\n\nFor example 1 we suppose that 1% of the population uses a certain drug. So next I want to AddChild to ex1 to show those who use the drug (d) and do not use the drug (dc, where c means compliment).\n\n\nShow code\nd &lt;- ex1$AddChild(\"Uses Drug\", p = 0.01)\ndc &lt;- ex1$AddChild(\"Does Not Use Drug\", p = 0.99)\n\n\nNow let t be tests positive for the disease. The drug manufacturer claims that \\(P(T|D^C)=0.015\\) and \\(P(T^C|D)=0.005\\). Which means that:\n\\(P(T^C|D^C)=1-P(T|D^C)=1-0.015=0.985\\) and\n\\(P(T|D)=1-P(T^C|D)=1-0.005=0.995\\)\nSo lets add another layer of nodes to example 1.\n\n\nShow code\nt &lt;- d$AddChild(\"Positive Test\", p = 0.995)\ntc &lt;- d$AddChild(\"Negative Test\", p = 0.005)\nt &lt;- dc$AddChild(\"Positive Test\", p = 0.015)\ntc&lt;- dc$AddChild(\"Negative Test\", p = 0.985)\n\n\nAnd then print what information we have.\n\n\nShow code\nbase::print(ex1, 'p')\n\n\n              levelName     p\n1 Example 1                NA\n2  ¦--Uses Drug         0.010\n3  ¦   ¦--Positive Test 0.995\n4  ¦   °--Negative Test 0.005\n5  °--Does Not Use Drug 0.990\n6      ¦--Positive Test 0.015\n7      °--Negative Test 0.985\n\n\nIf the probability column shows as a percentages we can use the SetFormat() function to set the decimal to 3 places.\n\n\nShow code\ndata.tree::SetFormat(ex1, \"p\", formatFun = data.tree::FormatFixedDecimal(3))\n\n\nAnd print the information we have.\n\n\nShow code\nbase::print(ex1, 'p')\n\n\n              levelName     p\n1 Example 1                NA\n2  ¦--Uses Drug         0.010\n3  ¦   ¦--Positive Test 0.995\n4  ¦   °--Negative Test 0.005\n5  °--Does Not Use Drug 0.990\n6      ¦--Positive Test 0.015\n7      °--Negative Test 0.985\n\n\n\n\n3. Conditional Probability\nGiven a positive test, we can find the probability that a person actually uese the drug with the following equation:\n\\(\\begin{equation}\\label{a}\\begin{split}P(D|T) &= \\frac{P(T|D)\\times P(D)}{[P(T|D)\\times P(D)]+[P(T|D^C)\\times P(D^C)]}\\\\&=\\frac{(0.995)(0.01)}{(0.995\\times 0.01)+(0.015\\times 0.99)}\\\\&=\\frac{199}{496}\\\\&\\approx 0.4012\\end{split}\\end{equation}\\)\n\n\n4. Plotting a Tree Diagram\nLastly we can use the plot() function to print a Tree Diagram:\n\n\nShow code\nbase::plot(ex1)\n\n\n\n\n\n\nTo visualize the tree diagram from left to right instead of top to bottom we can use the SetGraphStyle() function as shown below.\n\n\nShow code\ndata.tree::SetGraphStyle(ex1, rankdir = \"LR\")\nbase::plot(ex1)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230425-TidyTuesday-LondonMarathon/index.html",
    "href": "blog/Technical-Blog/posts/20230425-TidyTuesday-LondonMarathon/index.html",
    "title": "Week 17 Tidy Tuesday: London Marathon",
    "section": "",
    "text": "In this week’s #TidyTuesday submission I delve into London Marathon data, unveiling the number of winners by nationality throughout the marathon’s history.\n\nLondon Marathon\nThe data this week comes from Nicola Rennie’s LondonMarathon R package. This is an R package containing two data sets scraped from Wikipedia (1 November 2022) on London Marathon winners, and some general data. How the dataset was created, and some analysis, is described in Nicola’s post “Scraping London Marathon data with {rvest}”. Thank you for putting this dataset together @nrennie!\n\n\n\nCode\nThis week I wanted to add flags for each nationality, so two more functions are added to this weeks workflow:\n\nflags(): which reads in each flags .pgn, and saves them as a list.\nadd_flag(): reads in the list of flags, and overlays the images on the graph.\n\nAdditionally are the three key functions: cleaning, visualizing, and styling, as well as the index.R file which integrates data loading, function execution, and the generation of of a data visualization.\n\nCleaningVisualizingStylingFlagsAdd Flagsindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\nlibrary(dplyr)\nlibrary(magrittr)\n\n#### Cleaning Function ####\nclean &lt;- function(data){\n  clean_data &lt;- data %&gt;%\n    dplyr::group_by(Nationality) %&gt;%\n    dplyr::summarise(\n      nat_winners = dplyr::n()) %&gt;%\n    dplyr::arrange(\n      dplyr::desc(nat_winners))\n  return(clean_data)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nlibrary(ggplot2)\n\n#### Load Colors ####\ngeom_bar_color &lt;- \"#DD733A\"\n\n#### Visual Function ####\nvis &lt;- function(df){\n  vis &lt;- ggplot2::ggplot(\n    df,\n    ggplot2::aes(\n      x = nat_winners,\n      y = stats::reorder(\n        Nationality, \n        nat_winners))) +\n    ggplot2::geom_bar(\n      stat = \"identity\",\n      fill = geom_bar_color)\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nlibrary(ggplot2)\n\n#### Load Colors ####\nbackground_color &lt;- \"#596D78\"\ntitle_color &lt;- \"#ABB3B8\"\ngraph_color &lt;- \"#ABB3B8\"\nbonus_1 &lt;- \"#4F555B\"\nbonus_2 &lt;- \"#B4AE59\"\n\n#### Style Function ####\nstyle &lt;- function(vis){\n  sty &lt;- vis +\n    # labels\n    ggplot2::labs(\n      title = \"Nationality of London Marathon Winners\",\n      subtitle = \"\",\n      caption = \"Graph by Randi Bolt \\n Data from #TidyTuesday\",\n      x = \"Number of Winners\",\n      y = \"\") +\n    # values next to bars\n    ggplot2::geom_text(\n      ggplot2::aes(label = nat_winners,\n                   hjust = 1.2)) +\n    ggplot2::theme(\n      # title\n      plot.title = element_text(\n        size = 18,\n        face = \"bold\",\n        color = title_color,\n        hjust = .5),\n      # caption \n      plot.caption = element_text(\n        color = title_color),\n      # graph background\n      panel.background = element_rect(\n        fill = graph_color),\n      # x and y axis labels\n      axis.title = element_text(\n        size = 14, \n        color = title_color),\n      # axis ticks text\n      axis.text = element_text(\n        siz = 10,\n        color = title_color),\n      # background\n      plot.backgroun = ggplot2::element_rect(\n        fill = background_color\n      )\n    )\n  return(sty)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(png)\n\n#### Load Flags ####\nuk &lt;-  readPNG(\"flags/uk.png\")   \nkenya &lt;-  readPNG(\"flags/kenya.png\") \nusa &lt;-  readPNG(\"flags/usa.png\")\nswi &lt;-  readPNG(\"flags/switzerland.png\")\nethi &lt;-  readPNG(\"flags/ethiopia.png\")\nnor &lt;-  readPNG(\"flags/norway.png\")\nire &lt;-  readPNG(\"flags/ireland.png\")\nmex &lt;-  readPNG(\"flags/mexico.png\")\ngermany &lt;-  readPNG(\"flags/germany.png\") \nport &lt;-  readPNG(\"flags/portugal.png\")\nitaly &lt;-  readPNG(\"flags/italy.png\")\ncan &lt;-  readPNG(\"flags/canada.png\")\nsweden &lt;-  readPNG(\"flags/sweden.png\")\njapan &lt;-  readPNG(\"flags/japan.png\")\nfran &lt;-  readPNG(\"flags/france.png\")\ndenmark &lt;-  readPNG(\"flags/denmark.png\")\naus &lt;-  readPNG(\"flags/australia.png\")\npol &lt;- readPNG(\"flags/poland.png\")\nneth &lt;- readPNG(\"flags/netherlands.png\")\nmor &lt;- readPNG(\"flags/morocco.png\")\nspain &lt;- readPNG(\"flags/spain.png\")\nsu &lt;- readPNG(\"flags/soviet_union.png\")\nchina &lt;- readPNG(\"flags/china.png\")\nbel &lt;- readPNG(\"flags/belgium.png\")\n\n#### Flag List ####\nflag_list &lt;- list(\n  uk, kenya, usa,\n  swi, ethi, nor,\n  ire, mex, germany,\n  port, italy, can,\n  sweden, japan, fran,\n  denmark, aus, pol,\n  neth, mor, spain,\n  su, china, bel)\n\n#### Save Flags ####\nsaveRDS(flag_list, file=\"data/flag_list.RData\")\n\n\n\n\n\n\nShow Code\n#### Load Packages #### \nlibrary(ggplot2)\nlibrary(grid)\n\n#### Load Data ####\nflags_list &lt;- readRDS(\"data/flag_list.RData\")\n\n#### Add Flags ####\nadd_flags &lt;- function(styled_vis){\n  for (i in 1:length(flags_list)){\n    flag_new &lt;- styled_vis + \n      # add static annotation\n      ggplot2::annotation_custom(\n        # cycles through list and stacking flags on graph\n        grid::rasterGrob(\n          flags_list[[i]], \n          width=1, height=1),\n        xmin = -2, xmax = -.1,\n        ymin = 24.55-i, ymax = 25.45-i)\n    styled_vis &lt;- flag_new\n  }\n  return(flag_new)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(tidyverse) \nbase::library(png)\n#### Load Data ####\nremotes::install_github(\"nrennie/LondonMarathon\")\ndata(winners, package = \"LondonMarathon\")\n\n#### Clean Data ####\ndata &lt;- clean(winners)\n\n#### Create Data Visual ####\ndata_vis &lt;- vis(data)\n\n#### Style Data Visual ####\nsty_vis &lt;- style(data_vis)\n\n#### Add Flags ####\nsty_vis_with_flags &lt;- add_flags(sty_vis)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\")\n\n\n\n\nWeek 17 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nDownload images from wikimedia with utils::download.file(url = \"\", destfile = \"\", mode = \"wb).\nAdd images to data visual with ggplot2::annotation_custion() and grid::rasterGrob.\nggplot2::labs() reads markdown such as newline, \\n."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/codetocontent.html#agenda",
    "href": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/codetocontent.html#agenda",
    "title": "Code to Content",
    "section": "Agenda",
    "text": "Agenda\n\n\nAdventure Awaits\nReasons to Embark on Your Expedition\nBlogging Blueprint\nBuilding Your Narrative With Quarto\nCurated Counsel\nAdvice for Smooth Sailing\nDive Deeper\nResourceful Links"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/codetocontent.html#adventure-awaits",
    "href": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/codetocontent.html#adventure-awaits",
    "title": "Code to Content",
    "section": "Adventure Awaits",
    "text": "Adventure Awaits\nReasons to Embark on Your Expedition\n\n\nJoy in Expression and Self Discovery\nOwn Your Narrative\nContinuous Growth and Learning\nReveal Fresh Perspectives\nData Advocacy and Empowerment\nEmbedded in R Culture"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/codetocontent.html#blogging-blueprint",
    "href": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/codetocontent.html#blogging-blueprint",
    "title": "Code to Content",
    "section": "Blogging Blueprint",
    "text": "Blogging Blueprint\nBuilding Your Narrative With Quarto\n\n\nKnowledge Acquisition, Ideation,\nand Planning\nSetup, Version Control, and Deployment\nConfigure, Personalize, and Post\nAesthetics, Ownership, and Analysis"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/codetocontent.html#curated-council",
    "href": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/codetocontent.html#curated-council",
    "title": "Code to Content",
    "section": "Curated Council",
    "text": "Curated Council\nAdvice for Smooth Sailing\n\n\nPrioritize Accessibility\nSelect the Right Tool for the Job\nAvoid Duplicating Other Profiles\nConsider Your Audience"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/codetocontent.html#dive-deeper",
    "href": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/codetocontent.html#dive-deeper",
    "title": "Code to Content",
    "section": "Dive Deeper",
    "text": "Dive Deeper\nResources\n\n\nQuarto &gt; Websites &gt; Creating a Blog\nCreating a blog with Quarto in 10 steps\nBea Milz - Blog (Repo)\nCreate Your Data Science Portfolio with Quarto\nDeepsha Menghani - Blog (Repo)\nStyle Your Quarto Blog without Knowing a lot of HTML/CSS\nAlbert Rapp - Blog\nQuarto/RMarkdown - What’s Different?\nTed Laderas - Blog (Repo)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240402-TidyTuesday-dubois/index.html",
    "href": "blog/Technical-Blog/posts/20240402-TidyTuesday-dubois/index.html",
    "title": "Week 14 Tidy Tuesday: Du Bois Visualization Challenge",
    "section": "",
    "text": "For TidyTuesday, I recreated W.E.B. Du Bois’s charts on African American conditions using R and Tidyverse for a modern analysis.\n\nDu Bois Visualization Challenge 2024\n\n\n\n\n\n\n\n\n\n\n\nRandi’s Chart (Rendition)\nDu Bois’s Chart (Original)\n\n\n\nThe challenge this year pays homage to the profound data visualization legacy of W.E.B. Du Bois, inviting participants to reinterpret the groundbreaking visualizations he presented at the 1900 Paris Exposition with contemporary tools. Anchored by the vibrant colors of the Pan-African Flag, the challenge provides a unique opportunity to explore the intersection of history, culture, and data science.\nParticipants are encouraged to channel the essence of Plate 37 from Du Bois’s series, ‘A Series Of Statistical Charts Illustrating The Conditions Of Descendants Of Former African Slaves Now Resident In The United States,’ using the Pan-African Flag’s colors as a palette. This endeavor not only celebrates Du Bois’s legacy but also deepens our understanding of the historical and current conditions of African American communities.\n\nThe goal of the challenge is to celebrate the data visualization legacy of W.E.B Du Bois by recreating the visualizations from the 1900 Paris Exposition using modern tools.\n\nFor comprehensive details and to join the challenge, visit the GitHub repository!\n\n\nCode\nThis code follows a streamlined structure organized into four distinct phases: Set-Up, Clean, Graph, and Save.\n\nSet-UpCleanGraphSaveLinks\n\n\n\n\nShow Code\n#### Packages ####\n# tidyverse: A collection of data-related packages.\n# showtext: Use various fonts. \n# forcats: Add factors to categorical variables. \n# ggtext: Used markdown on graph text. \n# ggrepel: Overlapping labels on graph. \nbase::library(tidyverse)\nbase::library(showtext)\nbase::library(forcats)\nbase::library(ggtext)\nbase::library(ggrepel)\n\n#### Data ####\n# dubois: TidyTuesday data of occupation percentages\ndubois &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-04-02/dubois_week10.csv')\n\n#### Fonts ####\n# font_add_google(): Search Google Fonts. \n# showtext_auto(): Turn showtext on for graphics. \nsysfonts::font_add_google(\"Courier Prime\", \"font\")\nshowtext::showtext_auto()\n\n#### Colors ####\n# col1: Text \n# col2: Background\n# colors: Colors for Pie Chart\ncol1 &lt;- \"green\"\ncol2 &lt;- \"black\"\ncolors &lt;- c(\"#ce1d40\", \"#a59faa\", \"#e4bdb0\", \"#d1bea6\", \"#9b8f7d\", \"#ecb95b\")\n\n#### Text ####\n# title_text\n# subtitle_text\n# caption_text\ntitle_text &lt;- \"A SERIES OF STATISTICAL CHARTS. ILLUSTRA-&lt;br&gt;TING THE CONDITION OF THE DESCENDANTS OF FOR-&lt;br&gt;MER AFRICAN SLAVES NOW RESIDENT IN THE UNITED&lt;br&gt;STATES OF AMERICA.\"\nsubtitle_text &lt;- \"&lt;br&gt; &lt;span style='color:red;'&gt;UNE SÉRIE DE CARTES ET DIAGRAMMES STATISTIQUES MONTRANT LA&lt;br&gt;CONDITION PRÉSENTE DES DESCENDANTS DES ANCIENS ESCLAVES AFRI-&lt;br&gt;CANS ACTUELLMENT ÉTABLIS DANS LES ETATS UNIS D´ AMÉRIQUE.&lt;/span&gt; &lt;br&gt; &lt;br&gt; THE UNIVERSITY WAS FOUNDED IN 1867. &lt;br&gt; IT HAS INSTRUCED 6000 NEGROS STUDENTS. &lt;br&gt; &lt;br&gt; &lt;span style='color:red;'&gt;L´UNIVERSITÉ A ÉTÉ FONDÉE EN 1867. &lt;br&gt; ELLE A DONNÉ L´ INSTRUCTION A'6000 ÉTUDIANTS NEGRES.&lt;/span&gt;&lt;br&gt; &lt;br&gt; IT HAS GRADUATED 330 NEGROES AMOUNG WHOM ARE: &lt;br&gt; &lt;br&gt; &lt;span style='color:red;'&gt; ELLE A DÉLIVRÉ DES DIPLOMES A 330 NÉGRES DONT:&lt;/span&gt;\"\ncaption_text &lt;- \"Randi Bolt \\nApril 2024 \\n#TidyTuesday \\nDu Bois\"\n\n\nSet-Up\n\nPackages\nData\nFonts\nColors\nText\n\n\n\n\n\nShow Code\n#### Clean Data ####\n# Add Columns\n# 1. Calculate the cumulative sum of 'Percentage' column in reverse order.\n# 2. Calculate the position for labels. \n# 3. Replace NA values in 'pos' with half of the 'Percentage' value.\ndubois &lt;- dubois |&gt;\n  dplyr::mutate(\n    # Calculate the cumulative sum of 'Percentage' column in reverse order.\n    csum = base::rev(base::cumsum(base::rev(Percentage))),\n    # Calculate the position for labels.\n    pos = Percentage/2 + dplyr::lead(csum, 1),\n    # Replace NA values in 'pos' with half of the 'Percentage' value.\n    pos = dplyr::if_else(base::is.na(pos), Percentage/2, pos))\n\n\nClean\n\nCalculate the cumulative sum of ‘Percentage’ column in reverse order.\nCalculate the position for labels.\nReplace NA values in ‘pos’ with half of the ‘Percentage’ value.\n\n\n\n\n\nShow Code\n#### Pie Chart ####\n# Data: dubois (modified with position column)\n# Aesthetics: x = blank, y = Percentage, fill = fct_inorder(Occupation)\n# Geometry: width = 1\n# Polarize Coordinate: theta = y, start = 1.5, directional = 1 (clockwise)\n# Define fill color and order\n# Add labels outside of pie chart. \n# Legend title\n# Define labels\n# Remove Grid from Pie Chart\n# Theme: title, subtitle, caption, legend, background, plot margins\npie &lt;- ggplot2::ggplot(\n  # Data\n  dubois, \n  # Aesthetics\n  aes(\n    x = \"\" , \n    y = Percentage, \n    fill = fct_inorder(Occupation))\n  ) +\n  # Geometry\n  geom_col(\n    width = 1) +\n  # Polarize Coordinates\n  coord_polar(\n    theta = \"y\", \n    start = 1.5, \n    direction = 1) +\n  # Define fill color and order\n  scale_fill_manual(\n    values = colors,\n    breaks = c(\n      \"Teachers\", \"Ministers\", \"Government Service\", \n      \"Business\", \"Other Professions\", \"House Wives\"\n      )\n  ) +\n  # Add Labels Outside Pie Chart \n  ggrepel::geom_label_repel(\n    # Data\n    data = dubois,\n    # Aesthetics\n    aes(\n      y = pos, \n      label = paste0(Percentage, \"%\")\n      ),\n    size = 4.5, \n    nudge_x = 1, \n    show.legend = FALSE,\n    colour = \"black\"\n    ) +\n  # Legend Title\n  guides(\n    fill = guide_legend(title = \"\")\n    ) +\n  # Labels\n  labs(\n    title = title_text,\n    subtitle = subtitle_text,\n    caption = caption_text\n  ) +\n  # Remove Grid from Pie Chart\n  theme_void(\n  ) +\n  # Theme\n  theme(\n    # Title\n    plot.title = element_markdown(\n      size = 11, \n      hjust = 0.5,\n      lineheight = 0.9,\n      family = \"font\",\n      face = \"bold\",\n      color = col1),\n    plot.title.position = \"plot\",\n    # Subtitle\n    plot.subtitle = element_markdown(\n      size = 9, \n      hjust = 0.5,\n      lineheight = 0.9,\n      family = \"font\", \n      color = col1\n    ),\n    # Caption\n    plot.caption = element_text(\n      size = 6,\n      family = \"font\",\n      color = col1,\n      hjust = 1\n    ),\n    # Legend\n    legend.position = \"left\",\n    legend.title = element_blank(),\n    legend.text = element_text(\n      size = 8,\n      family = \"font\",\n      color = col1),\n    legend.key = element_blank(),\n    legend.background = element_blank(),\n    # Background\n    plot.background = element_rect(fill = col2, color = NA),\n    # Plot Margins\n    plot.margin = margin(t = 20, r = 20, b = 20, l = 20))\n\n\nGraph\n\nData: dubois (modified with position column)\nAesthetics: x = blank, y = Percentage, fill = fct_inorder(Occupation)\nGeometry: width = 1\nPolarize Coordinate: theta = y, start = 1.5, directional = 1 (clockwise)\nDefine fill color and order\nAdd labels outside of pie chart.\nLegend title\nDefine labels\nRemove Grid from Pie Chart\nTheme: title, subtitle, caption, legend, background, plot margins\n\n\n\n\n\nShow Code\n#### Save ####\nggplot2::ggsave(\n  \"plot.png\", \n  width = 5.25, \n  height = 6, \n  units = \"in\",\n  dpi = 100)\n\n\nSave plot as plot.png!\n\n\n\nR for Data Science’s #TidyTuesday Repo\nRandi Bolt’s #TidyTuesday Repo\n\n\n\n\n\nQuick Notes\n\nThe ggrepel() package includes the geom_label_repel() function, which is utilized to place chart labels outside of pie chart segments, ensuring that values are displayed clearly without overlapping the chart itself.\nThe pos variable, defined when cleaning, is calculated as a position for placing labels by first determining the midpoint of each item’s Percentage value and then adjusting this position based on the cumulative sum of percentages below it. This adjustment is made by adding half of an item’s own percentage to the cumulative sum of percentages of all items below it, shifted upwards by one position (dplyr::lead(csum, 1)), ensuring labels are centered appropriately on or near their respective segments. In cases where this calculation results in NA (specifically for the last item due to the lead function shifting values), pos is set to half of the item’s own percentage, ensuring every item has a defined position for its label.\nThe coord_polar() function customization options include:\n\nSpecifying the \\(\\theta\\) component based on either the x or y aesthetic.\nAdjusting the starting offset from the default 12 o’clock position, expressed in radians.\nSetting the direction parameter to 1 for clockwise orientation or -1 for counterclockwise orientation.\n\nThe plot.title.position() argument aligns the plot’s title centrally over the entire plot area, rather than just centering it above the pie chart, for improved layout balance and title visibility.\nDue to the integrated design of geom_label_repel() within the ggrepel package, altering the color of the connecting lines between the chart and the percentage values would concurrently change the text color of the percentage values. Opting to maintain these lines in black provided a visually appealing contrast, deemed superior to alternative colors such as white. This choice also streamlined the visualization process, circumventing the more time-consuming task of manually positioning percentage labels."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220620-School-FinalPrepStatistics/index.html",
    "href": "blog/Technical-Blog/posts/20220620-School-FinalPrepStatistics/index.html",
    "title": "Final Prep. Statistics",
    "section": "",
    "text": "Notes consist of method of distribution function (CDF), Jacobian (uni-variate and bivariate), MGF (uni-variate and bivariate), and Normal Sample.\n\n\n1. Method of Distribution Function (CDF)\n\nGiven the density function \\(f(y)\\) the distribution function \\[F_Y(y)=\\int_{\\text{lower}}^{y}f(t)dt=F_Y(t)|_{t=\\text{lower}}^{t=y}=F_Y(y)\\]\nFor \\(U=\\color{red}\\text{FUNCTION WITH Y}\\) , when y = lower, u = \\(\\color{yellow}\\text{MIN}\\) and when y = upper, u = \\(\\color{orange}\\text{MAX}\\).\nBy definition of the CDF, the CDF of U is,\n\n\\(\\begin{equation}\\label{a}\\begin{split}F_U(u) &= P(U\\leq u)\\\\&=P(\\color{red}\\text{FUNCTION WITH Y}\\color{black}\\leq u)\\\\&=P(Y\\leq\\color{blue}\\text{FUNCTION WITH u}\\color{black})\\\\&= F_Y(\\color{blue}\\text{FUNCTION WITH u}\\color{black})\\\\&=\\color{purple}\\text{NEW FUNCTION WITH u}\\end{split}\\end{equation}\\)\n\\(F_U(u)=\\begin{cases}0 & u&lt;\\text{MIN}\\\\ \\color{purple}\\text{NEW FUNCTION WITH u}\\color{black} & \\color{yellow}\\text{MIN}\\color{black}\\leq u\\leq \\color{orange}\\text{MAX}\\color{black}\\\\1 & \\color{orange}\\text{MAX}\\color{black}\\leq u\\end{cases}\\)\n\nSince \\(f_U(u)=F'_U(u)\\),\n\n\\(f_U(u)=\\frac{d}{du}(F_U(u))=\\color{green}\\text{DERIVATIVE OF NEW FUNCTION WITH u}\\)\nThe complete pdf of U is,\n\\[f_U(u)=\\begin{cases}\\color{green}\\text{DERIVATIVE OF NEW FUNCTION WITH u}\\color{black} & \\color{yellow}\\text{MIN}\\color{black}&lt;u&lt;\\color{orange}\\text{MAX}\\color{black}\\\\0 & \\text{otherwise}\\end{cases}\\]\n\n\n2. Jacobian (Univariate)\n\nGraph U (Y,u) to verify g is monotone and increasing.\n\\(U=g(Y)\\) where \\(g(y)=U\\). Since g is a monotinc increasing function we can apply the Transformation Method.\nThe inverse of g is \\(h(u)=g^{-1}(y)=\\)🥚 for \\(\\color{yellow}\\text{MIN}\\) \\(\\leq u\\leq\\) \\(\\color{orange}\\text{MAX}\\), and \\(h'(u)=\\) 🐤.\nThe pdf of U is then, \\(f_U(u)=f_Y(h(u))\\cdot |h'(u)|=f_Y(\\)🥚\\()\\cdot|\\)🐤\\(|=\\color{white}\\text{A Solution}\\)\n\nAnd the complete pdf is,\n\\[f_U(u)=\\begin{cases}\\color{white}\\text{A Solution}\\color{black} & \\color{yellow}\\text{MIN}\\color{black}&lt;u&lt;\\color{orange}\\text{MAX}\\color{black}\\\\0 & \\text{otherwise}\\end{cases}\\]\nNote:\n\n\\(y^2\\) is monotonic on \\(0\\leq y\\)\n\n\n\n3. MGF\n\n\nThe mgf of Y is \\(M_Y(t)=\\color{white}\\text{Moment Generating Function of Y}\\color{black}=E_Y(t)=E(e^{ty})\\).\nBy definition of the mgf, the mgf of U is,\n\n\\(\\begin{equation}\\label{b}\\begin{split}M_U(t) &= E(e^{tU})\\\\&=\\text{something with the form of }E(e^{ty})\\\\&=\\color{white}\\text{something similar to known distribution}\\end{split}\\end{equation}\\)\n\nTherefore, mgf U has the form of \\(\\underline{\\text{a known distribution}}\\) with parameters \\(\\color{yellow}\\text{Parameter}\\) and \\(\\color{orange}\\text{Parameter}\\).\n\nNote: (For Bivariate)\n\nLet \\(Y_i\\) be the ith \\(\\underline{\\quad\\quad\\quad}\\) for \\(i=1,2\\).\n\n\n\n4. Jacobian (bivariate)\n\nThe pdf of \\(Y_1\\) is \\(f(y_1)=\\)🌗 and the pdf of \\(Y_2\\) is \\(f(y_2)=\\)🌓.\nSince \\(Y_1\\) and \\(Y_2\\) are independent, their joint pdf is \\(f_{Y_1,Y_2}(y_1,y_2)=f(y_1)f(y_2)=\\)🌗🌓=🌝\nLet \\(U_1=\\)🏃\\(=h_1(y_1,y_2)\\) and \\(U_2=\\)🌎\\(=h_2(y_1,y_2)\\). Then, \\(y_1=...=\\)⛵\\(=h_1^{-1}(u_1,u_2)\\) and \\(y_2=...=\\)🌊\\(=h_2^{-1}(u_1,u_2)\\).\nThen the Jacobian is, \\(J=\\text{det}\\begin{bmatrix}\\frac{\\partial y_1}{\\partial u_1} & \\frac{\\partial y_1}{\\partial u_2}\\\\ \\frac{\\partial y_2}{\\partial u_1} & \\frac{\\partial y_2}{\\partial u_2}\\end{bmatrix}=\\) ⚓\nRecall: det\\(|\\begin{smallmatrix}a & b \\\\ c & d\\end{smallmatrix}|=(ad)-(bc)\\)\nTherefore the joint pdf of \\(U_1\\) and \\(U_2\\) is \\(f_{U_1,U_2}=(u_1,u_2)=f_{Y_1,Y_2}(h_1^{-1}(u_1,u_2),h_2^{-1}(u_1,u_2))\\times|J|=\\)🌜⛵🌊🌛\\(\\times\\)|⚓|\n\n\n\n5. Normal Sample\nFind the probability that the sample of \\(n=\\)🐝 will be within X=🍯 of the population mean. Given \\(\\mu=\\)🌻 and \\(\\sigma^2=\\)🌿\n\nLet \\(\\overline{Y}\\) be the mean of \\(\\underline{\\quad\\text{     }\\quad}\\) of 🐝 \\(\\underline{\\quad\\text{     }\\quad}\\).\nWe want to find \\(P(|\\overline{Y}-\\mu|\\leq X)=P(|\\overline{Y}-\\)🌻\\(|\\leq\\)🍯\\()=P(-\\)🍯\\(\\leq \\overline{Y}-\\)🌻\\(\\leq\\)🍯\\()\\).\nSince the population is normally distributed with mean \\(=\\mu\\) and variance \\(\\frac{\\sigma^2}{\\sqrt{n}}=\\)🌿/\\(\\sqrt{}\\)🐝=🌾.\n\nThen, \\(P(-Z\\leq\\overline{Y}-\\mu\\leq Z)=P(\\)-🍯/🌾\\(\\leq [\\overline{Y}\\)-🌻]/🌾\\(\\leq\\)🍯/🌾)=P(🍄\\(\\leq Z\\leq\\) 🌸)\n\nOn Ti Calculator normalcdf(🍄,🌸,0,1)\n\nNote:\n\nVariance = 4 \\(\\rightarrow\\sqrt{4}=\\sigma^2\\)\nStandard Deviation = 4 \\(\\rightarrow 4=\\sigma^2\\)\n\\(Z=\\frac{\\overline{Y}-\\mu}{\\frac{\\sigma^2}{\\sqrt{n}}}=\\frac{\\sqrt{n}(\\overline{Y}-\\mu)}{\\sigma^2}\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230620-TidyTuesday-UFOSightingsRedux/index.html",
    "href": "blog/Technical-Blog/posts/20230620-TidyTuesday-UFOSightingsRedux/index.html",
    "title": "Week 25 Tidy Tuesday: UFO Sightings Redux",
    "section": "",
    "text": "Explore the mystery of UFOs in my Week 25 #TidyTuesday submission, uncovering patterns in Oregon sightings under 5 minutes, classified by shape.\n\nUFO Sightings Redux\nThe data this week comes from the National UFO Reporting Center, cleaned and enriched with data from sunrise-sunset.org by Jon Harmon.\nIf this dataset looks familiar, that’s because we used a version of it back in 2019. The new version adds the last several years of data, adds information about time-of-day, and cleans up some errors in the original dataset. We’d love to see visualizations describing the differences between the 2019 dataset and this new dataset!\n\nThe National UFO Reporting Center was founded in 1974 by noted UFO investigator Robert J. Gribble. The Center’s primary function over the past five decades has been to receive, record, and to the greatest degree possible, corroborate and document reports from individuals who have been witness to unusual, possibly UFO-related events. Throughout its history, the Center has processed over 170,000 reports, and has distributed its information to thousands of individuals.\n\n\n\n\nCode\nThis weeks code retains its core components of cleaning, visualizing, and styling, while the index.R file handles data loading, file execution, and the generation of a data visualization.\n\nCleaningVisualizeStyleindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\n# magrittr: %&gt;% pipe function. \n# dplyr: data cleaning functions.\n# tidyr: data manipulation functions.\nbase::library(dplyr)\nbase::library(magrittr)\nbase::library(tidyr)\n\n#### Cleaning Function ####\nclean &lt;- function(df){\n  # Filter date for Oregon\n  or_df &lt;- df %&gt;%\n    dplyr::filter(state == c(\"OR\")) %&gt;%\n    dplyr::reframe(\n      shape = shape, \n      duration_seconds = duration_seconds,\n      duration_minutes = duration_seconds/60)\n  # remove nas \n  or_df$duration_seconds &lt;- stats::na.omit(or_df$duration_seconds)\n  # Subset for all data 5 minutes or less\n  clean_df &lt;- \n    base::subset(or_df, \n                 duration_seconds &lt; 301) %&gt;%\n    tidyr::replace_na(list(shape = c(\"not available\")))\n    \n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(ggplot2)\n\n#### Visual Function ####\nvis &lt;- function(clean_df){\n  vis &lt;- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(x = shape,\n                 y = duration_minutes,\n                 fill = shape,\n                 color = shape)) +\n    ggplot2::geom_boxplot(outlier.shape = NA) +\n    ggplot2::geom_jitter(\n      na.rm = TRUE, \n      color = \"#d2ff46\", \n      alpha = 0.7)\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# ggplot2: graphing functions.\n# showtext: font functions. \nbase::library(\"ggplot2\")\nbase::library(\"showtext\")\n\n#### Load Fonts ####\nsysfonts::font_add_google(\"Orbitron\", \"font\")\nshowtext::showtext_auto()\n\n#### Load Colors ####\ncol1 &lt;- \"#d2ff46\"\ncol2 &lt;- \"#0d0d0d\"\ncol3 &lt;- \"white\"\n\n#### Axis Labels ####\naxis_labs &lt;- c(\"changing\", \"orb\", \"formation\", \"oval\", \"unknown\",\n               \"cigar\", \"rectangle\", \"circle\", \"egg\", \"sphere\", \n               \"other\", \"light\", \"triangle\", \"teardrop\", \"cross\",\n               \"diamond\", \"cylinder\", \"disk\", \"fireball\", \"not available\", \n               \"flash\", \"chevron\", \"cone\")\n\n#### Style Function ####\nsty &lt;- function(vis){\n  sty &lt;- vis +\n    # labs\n    ggplot2::labs(\n      title = \"Shapes in Oregon: Short Sightings\",\n      subtitle = \"\\nThe following box plot displays the distribution of UFO sightings lasting less than 5 minutes in Oregon, categorized by \\ntheir reported shape. Among this subset of sightings, Cylinder, Disk, Fireball, Not Available, Flash, Chevron, and Cone \\nwere more frequently reported with durations of less than two minutes. In contrast, the Changing UFO shape was \\nobserved for a broader duration range, spanning from 2 to 5 minutes. \\n\\nThe asterisk (*) denotes the mean duration in minutes.\\n \",\n      caption = \"Randi Bolt - June 2023 \\n#TidyTuesday: UFO Sightings Redux\",\n      x = \"Shape\",\n      y = \"Duration in Minutes\")  +\n    # x and y axis\n    ggplot2::scale_y_continuous(limits = c(0,5)) + \n    ggplot2::scale_x_discrete(limits = axis_labs) +\n    # variable color and fill colors\n    ggplot2::scale_color_viridis_d() +\n    ggplot2::scale_fill_viridis_d(alpha = 0.6) +\n    # mean point\n    ggplot2::stat_summary(\n      fun = mean, \n      geom = \"point\", \n      shape = 8, \n      size = 4) +\n    # theme\n    ggplot2::theme(\n      # labs \n      plot.title = element_text(\n        size = 30,\n        family = \"font\",\n        face = \"bold\",\n        hjust = 0,\n        color = col1),\n      plot.subtitle = element_text(\n        size = 18,\n        family = \"font\",\n        hjust = 0,\n        color = col1),\n      plot.caption = element_text(\n        size = 12,\n        family = \"font\",\n        hjust = 1,\n        color = col1),\n      axis.title = element_text(\n        size = 24, \n        family = \"font\",\n        color = col1),\n      axis.text = element_text(\n        size = 18, \n        family = \"font\",\n        color = col1),\n      axis.text.x = element_text(\n        angle = 55,\n        vjust = .7), \n      plot.background = element_rect(fill = col2),\n      panel.background = element_rect(fill = col2),\n      panel.grid.major.x = element_blank(),\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      legend.position = \"none\") \n  return(sty)\n}\n\n\n\n\n\n\nShow Code\n#### Week ####\nweek &lt;- c(\"25\")\n\n#### Load Packages ####\n# tidyverse: A collection of data-related packages.\nbase::library(tidyverse)\n\n#### Load Data ####\n# tt_data: ufo_sightings\ntt_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-20/ufo_sightings.csv')\n\n#### Clean Data ####\nclean_data &lt;- clean(tt_data)\n\n#### Create Data Visual ####\nvis_data &lt;- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis &lt;- sty(vis_data)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\",\n  dpi = 150)\n\n\n\n\nWeek 25 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nI utilized tidyr::replace_na() in the cleaning function to replace missing values (NA) with the string ‘not available’.\nIn the visualizing function, I opted to use ggplot2::geom_jitter() instead of ggplot2::geom_point() due to the nature of the data. Since the data points represent estimates of sighting durations, and they are mostly in minutes rather than specific seconds, using geom_point() would result in straight lines of dots across the box plots. Therefore, I decided to apply jittering using geom_jitter() to add some randomness to the point positions, which I deemed acceptable for visualizing the estimated sighting durations.\nIn the styling function, I utilized ggplot2::scale_x_discrete(limits = axis_labs) to define the order of the x-axis labels. Note that if you misspell a word, the data will not appear on your graph.\nAs the styling function grows longer, I find myself considering the idea of splitting it up. One approach I might explore is separating the labels into their own distinct function. This way, I can assess whether this change significantly improves the readability of the styling function.\nThis visual raises some concerns, and I acknowledge the need to allocate more time for statistical analysis in the future. The following issues can be identified with this visual.\n\n\nLack of statistical significance in the arrangement of UFO shapes along the x-axis: The order was chosen based on aesthetic and readability considerations, rather than any statistical criteria.\nAbsence of statistical analysis for outlier elimination: Since the data was limited to sightings lasting 5 minutes or less, no statistical analysis was conducted to identify and eliminate outliers."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220718-Original-2022NBAFinalsReactable/index.html",
    "href": "blog/Technical-Blog/posts/20220718-Original-2022NBAFinalsReactable/index.html",
    "title": "Reactable",
    "section": "",
    "text": "This post uses the r package reactable to look at 2022 NBA Finals contenders: Golden State Warriors and Boston Celtics.\n\n\n1. Set Up\nThis post will use three packages:\n\nrvest: to harvest the data.\ndplyr: to join and tidy data.\nreactable: to make interactive tables.\n\n\n\nShow code\nlibrary(rvest) \nlibrary(dplyr)\nlibrary(reactable)\n\n\n\n\n2. Havest the Data with rvest\nUsing data from Basketball Reference and the rvest package I can harvest current data without having to save CSV’s. For this table I will be pulling in the final two teams which competed in the 2022 NBA Finals:\n\nGolden State Warriors\nBoston Celtics\n\n\n\nShow code\n# team name\ngsw &lt;- \"Golden State Warriors\"\nbc &lt;- \"Boston Celtics\"\n\n# team slug\ngsw_slug &lt;- \"GSW\"\nbc_slug &lt;- \"BOS\"\n\n# url\ngsw_url &lt;- base::paste0(\"https://www.basketball-reference.com/teams/\",\n                        gsw_slug,\"/2022.html\")\nbc_url &lt;- base::paste0(\"https://www.basketball-reference.com/teams/\",\n                       bc_slug,\"/2022.html\")\n\n\nThere is a lot of data available on Basketball Reference, but for this table I will only be looking at each teams 2022 total stats.\n\n\nShow code\n# harvest data\ngsw_ttl_stat &lt;- gsw_url %&gt;%\n  read_html %&gt;%\n  html_node(\"#totals\") %&gt;% \n  html_table()\n\nbc_ttl_stat &lt;- bc_url %&gt;%\n  read_html %&gt;%\n  html_node(\"#totals\") %&gt;% \n  html_table()\n# look at data\nutils::head(gsw_ttl_stat)\n\n\n# A tibble: 6 × 28\n     Rk Player   Age     G    GS    MP    FG   FGA `FG%`  `3P` `3PA` `3P%`  `2P`\n  &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n1     1 Andre…    26    73    73  2330   475  1019 0.466   157   399 0.393   318\n2     2 Jorda…    22    76    51  2283   474  1058 0.448   211   580 0.364   263\n3     3 Steph…    33    64    64  2211   535  1224 0.437   285   750 0.38    250\n4     4 Kevon…    25    82    80  1732   208   364 0.571     0     1 0       208\n5     5 Otto …    28    63    15  1396   193   416 0.464    80   216 0.37    113\n6     6 Draym…    31    46    44  1329   135   257 0.525    16    54 0.296   119\n# ℹ 15 more variables: `2PA` &lt;int&gt;, `2P%` &lt;dbl&gt;, `eFG%` &lt;dbl&gt;, FT &lt;int&gt;,\n#   FTA &lt;int&gt;, `FT%` &lt;dbl&gt;, ORB &lt;int&gt;, DRB &lt;int&gt;, TRB &lt;int&gt;, AST &lt;int&gt;,\n#   STL &lt;int&gt;, BLK &lt;int&gt;, TOV &lt;int&gt;, PF &lt;int&gt;, PTS &lt;int&gt;\n\n\nShow code\nutils::head(bc_ttl_stat)\n\n\n# A tibble: 6 × 28\n     Rk Player   Age     G    GS    MP    FG   FGA `FG%`  `3P` `3PA` `3P%`  `2P`\n  &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n1     1 Jayso…    23    76    76  2731   708  1564 0.453   230   651 0.353   478\n2     2 Marcu…    27    71    71  2296   300   718 0.418   119   360 0.331   181\n3     3 Jayle…    25    66    66  2220   576  1217 0.473   166   464 0.358   410\n4     4 Al Ho…    35    69    69  2005   266   569 0.467    89   265 0.336   177\n5     5 Grant…    23    77    21  1875   205   432 0.475   106   258 0.411    99\n6     6 Rober…    24    61    61  1804   271   368 0.736     0     1 0       271\n# ℹ 15 more variables: `2PA` &lt;int&gt;, `2P%` &lt;dbl&gt;, `eFG%` &lt;dbl&gt;, FT &lt;int&gt;,\n#   FTA &lt;int&gt;, `FT%` &lt;dbl&gt;, ORB &lt;int&gt;, DRB &lt;int&gt;, TRB &lt;int&gt;, AST &lt;int&gt;,\n#   STL &lt;int&gt;, BLK &lt;int&gt;, TOV &lt;int&gt;, PF &lt;int&gt;, PTS &lt;int&gt;\n\n\n\n\n3. Tidy Data\nTo tidy the data I want to do 3 things:\n\nRename column 2 to ‘Name’.\nAdd a column with the team name.\nJoin two tables into one.\n\n\n\nShow code\n# remane \nbase::names(gsw_ttl_stat)[2] &lt;- \"Name\"\nbase::names(bc_ttl_stat)[2] &lt;- \"Name\"\n\n# add column\ngsw_ttl_stat$Team &lt;- gsw\nbc_ttl_stat$Team &lt;- bc\n\n# merge tables\ntotal_stats &lt;- dplyr::full_join(gsw_ttl_stat, bc_ttl_stat)\n\n#view data  \nutils::head(total_stats)\n\n\n# A tibble: 6 × 29\n     Rk Name     Age     G    GS    MP    FG   FGA `FG%`  `3P` `3PA` `3P%`  `2P`\n  &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n1     1 Andre…    26    73    73  2330   475  1019 0.466   157   399 0.393   318\n2     2 Jorda…    22    76    51  2283   474  1058 0.448   211   580 0.364   263\n3     3 Steph…    33    64    64  2211   535  1224 0.437   285   750 0.38    250\n4     4 Kevon…    25    82    80  1732   208   364 0.571     0     1 0       208\n5     5 Otto …    28    63    15  1396   193   416 0.464    80   216 0.37    113\n6     6 Draym…    31    46    44  1329   135   257 0.525    16    54 0.296   119\n# ℹ 16 more variables: `2PA` &lt;int&gt;, `2P%` &lt;dbl&gt;, `eFG%` &lt;dbl&gt;, FT &lt;int&gt;,\n#   FTA &lt;int&gt;, `FT%` &lt;dbl&gt;, ORB &lt;int&gt;, DRB &lt;int&gt;, TRB &lt;int&gt;, AST &lt;int&gt;,\n#   STL &lt;int&gt;, BLK &lt;int&gt;, TOV &lt;int&gt;, PF &lt;int&gt;, PTS &lt;int&gt;, Team &lt;chr&gt;\n\n\nShow code\ntotal_stats\n\n\n# A tibble: 47 × 29\n      Rk Name    Age     G    GS    MP    FG   FGA `FG%`  `3P` `3PA` `3P%`  `2P`\n   &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n 1     1 Andr…    26    73    73  2330   475  1019 0.466   157   399 0.393   318\n 2     2 Jord…    22    76    51  2283   474  1058 0.448   211   580 0.364   263\n 3     3 Step…    33    64    64  2211   535  1224 0.437   285   750 0.38    250\n 4     4 Kevo…    25    82    80  1732   208   364 0.571     0     1 0       208\n 5     5 Otto…    28    63    15  1396   193   416 0.464    80   216 0.37    113\n 6     6 Dray…    31    46    44  1329   135   257 0.525    16    54 0.296   119\n 7     7 Dami…    29    63     5  1256   169   383 0.441    63   187 0.337   106\n 8     8 Gary…    29    71    16  1248   212   344 0.616    43   120 0.358   169\n 9     9 Jona…    19    70    12  1185   236   460 0.513    50   149 0.336   186\n10    10 Nema…    33    71     0  1143   160   342 0.468    54   149 0.362   106\n# ℹ 37 more rows\n# ℹ 16 more variables: `2PA` &lt;int&gt;, `2P%` &lt;dbl&gt;, `eFG%` &lt;dbl&gt;, FT &lt;int&gt;,\n#   FTA &lt;int&gt;, `FT%` &lt;dbl&gt;, ORB &lt;int&gt;, DRB &lt;int&gt;, TRB &lt;int&gt;, AST &lt;int&gt;,\n#   STL &lt;int&gt;, BLK &lt;int&gt;, TOV &lt;int&gt;, PF &lt;int&gt;, PTS &lt;int&gt;, Team &lt;chr&gt;\n\n\n\n\n4. Reactable\nNow to make a simple reactable I will do 7 things:\n\nGroup by “Team” name.\nDefine column names.\nInclude boarders around the table and every cell.\nInclude highlight rows that are hovered over.\nMake filterable.\nMake Searchable.\nHave the two teams be the minimum number of rows initally shown.\n\n\n\nShow code\nreactable(\n  total_stats,\n  groupBy = \"Team\",\n  columns = list(\n    Rk = colDef(name = \"Rank\"),\n    G = colDef(name = \"Games\"),\n    MP = colDef(name = \"Minutes Played\"),\n    FG = colDef(name = \"Field Goals\"),\n    `3P` = colDef(name = \"3 Point Goals\"),\n    `2P` = colDef(name = \"2 Point Goals\"),\n    FT = colDef(name = \"Free Throws\"),\n    AST = colDef(name = \"Assists\"),\n    STL = colDef(name = \"Steals\"),\n    BLK = colDef(name = \"Blocks\"),\n    PTS = colDef(name = \"Points\"),\n    TOV = colDef(name = \"Turnovers\"),\n    PF = colDef(name = \"Personal Fouls\")\n  ),\n  bordered = TRUE,\n  highlight = TRUE,\n  filterable = TRUE,\n  searchable = TRUE,\n  minRows = 2\n  )\n\n\n\n\n\n\n\n\n5. Sources\nNBA Analytics Tutorial: Using R to Display Player Career Stats\nReactable"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230613-TidyTuesday-SAFITeachingData/index.html",
    "href": "blog/Technical-Blog/posts/20230613-TidyTuesday-SAFITeachingData/index.html",
    "title": "Week 24 Tidy Tuesday: SAFI Teaching Data",
    "section": "",
    "text": "Explore a rich data set focused on understanding the role of farmer-led irrigation in Africa with my week 24 #TidyTuesday submission. In this post my visual delves into the relationship between livestock ownership and the number of rooms in a household.\n\nSAFI Data\nThe data this week comes from the SAFI (Studying African Farmer-Led Irrigation) survey, a subset of the data used in the Data Carpentry Social Sciences workshop. So, if you’re looking how to learn how to work with this data, lessons are already available! Data is available through Figshare.\nCITATION: Woodhouse, Philip; Veldwisch, Gert Jan; Brockington, Daniel; Komakech, Hans C.; Manjichi, Angela; Venot, Jean-Philippe (2018): SAFI Survey Results. doi:10.6084/m9.figshare.6262019.v1\n\nSAFI (Studying African Farmer-Led Irrigation) is a currently running project which is looking at farming and irrigation methods. This is survey data relating to households and agriculture in Tanzania and Mozambique. The survey data was collected through interviews conducted between November 2016 and June 2017 using forms downloaded to Android Smartphones. The survey forms were created using the ODK (Open Data Kit) software via an Excel spreadsheet. The collected data is then sent back to a central server. The server can be used to download the collected data in both JSON and CSV formats. This is a teaching version of the collected data that we will be using. It is not the full dataset.\n\n\nThe survey covered such things as; household features (e.g. construction materials used, number of household members), agricultural practices (e.g. water usage), assets (e.g. number and types of livestock) and details about the household members.\n\n\nThe basic teaching dataset used in these lessons is a subset of the JSON dataset that has been converted into CSV format.\n\n\n\n\nCode\nThis week in the index.R file I tried using the tidytuesdayR package and the variable week &lt;- c(24) in attempt to streamline updating the index file for future weeks. This project retains its core components of cleaning, visualizing, and styling, while the index.R file handles data loading, file execution, and the generation of a data visualization.\n\nCleaningVisualizeStyleindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(dplyr)\nbase::library(magrittr)\n\n#### Cleaning Function ####\nclean &lt;- function(df){\n  # extract data from list\n  extracted_df &lt;- df[[1]]\n  clean_df &lt;- extracted_df %&gt;% \n    dplyr::count(rooms, liv_count)\n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(ggplot2)\n\n#### Visual Function ####\nvis &lt;- function(clean_df){\n  vis &lt;- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(x = rooms, y = liv_count, fill = n)) +\n    geom_tile()\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(\"ggplot2\")\nbase::library(\"showtext\")\n\n#### Load Fonts ####\nsysfonts::font_add_google(\"Judson\", \"font\")\nshowtext::showtext_auto()\n\n#### Load Colors ####\ncol1 &lt;- \"white\"\ncol2 &lt;- \"#3E3D53\"\ncol3 &lt;- \"#2C6E63\"\ncol4 &lt;- \"#FCE100\"\n\n#### Style Function ####\nsty &lt;- function(vis){\n  sty &lt;- vis +\n    # labs\n    ggplot2::labs(\n      title = \"Relationship Between Live Stock Ownership and Number of Rooms in Household\",\n      subtitle = \"This subset of the SAFI (Studying African Farmer-Led Irrigation) data shows that of the 131 people surveyed most lived in a house with 1 room,\\n and had 1 cattle. There is no obvious evidence to indicate that the number of rooms in a home is related to the number of live stock owned.\",\n      caption = \"Randi Bolt - June 2023 \\n#TidyTuesday: SAFI Data - June 2017\",\n      x = \"Rooms\",\n      y = \"Live Stock\",\n      fill = \"Surveyed\")  +\n    # add numbers in boxes\n    ggplot2::geom_text(\n      ggplot2::aes(label = n),\n      color = col1,\n      size = 12,\n      family = \"font\") + \n    # scale color\n    ggplot2::scale_fill_gradient(low = col3, high = col4) +\n    # Axis Breaks \n    ggplot2::scale_x_continuous(\n      breaks = seq(1,8,1)) + \n    ggplot2::scale_y_continuous(\n      breaks = seq(1,5,1)) + \n # theme\n    ggplot2::theme(\n      plot.title = element_text(\n        size = 30,\n        family = \"font\",\n        face = \"bold\",\n        hjust = 0,\n        color = col1),\n      plot.subtitle = element_text(\n        size = 18,\n        family = \"font\",\n        hjust = 0,\n        color = col1),\n      plot.caption = element_text(\n        size = 12,\n        family = \"font\",\n        hjust = 1,\n        color = col1),\n      axis.title = element_text(\n        size = 24, \n        family = \"font\",\n        color = col1),\n      axis.text = element_text(\n        size = 18, \n        family = \"font\",\n        color = col1),\n      legend.title = element_text(\n        size = 24,\n        family = \"font\",\n        color = col1),\n      legend.text = element_text(\n        size = 16,\n        family = \"font\",\n        color = col1),\n      plot.background = element_rect(fill = col2),\n      panel.background = element_rect(fill = col2),\n      panel.grid.major.x = element_blank(),\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      legend.background = element_rect(fill = col2))\n  return(sty)\n}\n\n\n\n\n\n\nShow Code\nweek &lt;- c(\"24\")\n#### Load Packages ####\nbase::library(tidyverse)\nbase::library(tidytuesdayR)\n\n#### Load Data ####\ntt_data &lt;- tidytuesdayR::tt_load(2023, week = base::as.integer(week))\n\n\n\n    Downloading file 1 of 1: `safi_data.csv`\n\n\nShow Code\n#### Clean Data ####\nclean_data &lt;- clean(tt_data)\n\n#### Create Data Visual ####\nvis_data &lt;- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis &lt;- sty(vis_data)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\",\n  dpi = 150)\n\n\n\n\nWeek 24 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nI used ggplot2::geom_tile() to create the heat map graph.\nI used ggplot2::scale_fill_gradient() to assign a “high” color and “low” color, but it might make the graph easier to read if I picked 5 colors instead.\nI would have liked white horizontal lines on this graph, but was only getting the vertical lines to show. Will need to do more research on this in future weeks.\nWhen I use the showtext package to assign fonts to the text on my graphs and go to save them as a .png file, if I don’t update the dpi = value (dots per inch) then I have huge spaces in between lines of text. I found that the larger the dpi value the more space there is between lines of text. 150 dpi seems to be a good dpi for this graph.\nI am still debating using the TidyTuesdayR package. It is nice to have the link to the .csv file avaialble in the code."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html",
    "href": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "",
    "text": "This is a “presentation” style proof for Math-338: Modern College Geometry, and looks at congruence of two vertical angles."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#suppose-two-angles-are-verticle.",
    "href": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#suppose-two-angles-are-verticle.",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "Suppose two angles are verticle.",
    "text": "Suppose two angles are verticle.\nProve that they are congruent."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#vertical-angles",
    "href": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#vertical-angles",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "1.1 Vertical Angles",
    "text": "1.1 Vertical Angles\nIn class we defined vertical angles as being “across from each other”.\n\n\n\n\n\nFig. 1:  Vertical angles are shown in pink.\n\n\n\n\nWe can see a simple example of this in Fig. 1 where a pair of pink angles \\(\\angle AOB\\) and \\(\\angle COD\\) are vertical to each other.\nNote that \\(\\angle AOC\\) and \\(\\angle BOD\\) are vertical angles as well."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#congruence",
    "href": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#congruence",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "1.2 Congruence",
    "text": "1.2 Congruence\n\\(x\\cong y\\) if there is an \\(\\underline{\\text{isometry}}\\) that superimposes x onto y.\n\nIsometry is a map that preserves distance and angles\n\ntranslation (move without turning)\nrotation (moving about a fixed point)\nreflection (mirror)\ncombination\n\n\n\n\n\n\n\nFig. 2:  Two congruent triangles\n\n\n\n\nIn Fig. 2 we see the two triangles are congruent, and would only need a translation isometry or two to map \\(\\triangle ABC\\) onto \\(\\triangle A_1B_1C_1\\)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#supplementary-angles",
    "href": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#supplementary-angles",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "1.3 Supplementary Angles",
    "text": "1.3 Supplementary Angles\nWe defined supplementary angles as angles whose measurement adds up to \\(180^\\circ\\).\n\n\n\n\n\nFig. 3: Supplementary angles are shown in pink and orange.\n\n\n\n\nIn Fig.3 we can clearly see that \\(m\\angle AOC\\) shown in pink and \\(m\\angle AOB\\) in orange adds to a straight line or \\(180^\\circ\\). We can also see three other pairs of supplementary angles:\n\n\\(m\\angle AOB+m\\angle BOD=180^\\circ\\)\n\\(m\\angle BOD+m\\angle COD=180^\\circ\\)\n\\(m\\angle COD+m\\angle AOC=180^\\circ\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#axioms-of-angle-measure",
    "href": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#axioms-of-angle-measure",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "2.1 Axioms of Angle Measure",
    "text": "2.1 Axioms of Angle Measure\n\nRight angle measures \\(90^\\circ\\)\n\\(m\\angle ABC=m\\angle CBA\\)\nIf D is the interior of \\(\\angle ABC\\), then \\(m\\angle ABC=m\\angle ABD+m\\angle BDC\\)\nThere exists a unique ray that is the angle bisector of \\(\\angle ABC\\)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#congruence-and-angle-measure-theorem",
    "href": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#congruence-and-angle-measure-theorem",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "2.2 Congruence and Angle Measure theorem",
    "text": "2.2 Congruence and Angle Measure theorem\n\\(\\angle ABC\\cong \\angle DEF \\Leftrightarrow m\\angle ABC=m\\angle DEF\\)\n\nIf two angles are congruent then the measure of those two angles is the same.\nIf the measure of two angles is the same, then those two angles are congruent.\n\n(Note that the measure for angles that will be used on the proof will be in degrees. )"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#supplementary-interior-angle-theorem",
    "href": "blog/Technical-Blog/posts/20210719-School-Quiz1/index.html#supplementary-interior-angle-theorem",
    "title": "Proving If Two Angles Are Verticle, Then They Are Congruent",
    "section": "2.3 Supplementary Interior Angle Theorem",
    "text": "2.3 Supplementary Interior Angle Theorem\nIf two lines are parallel then the supplementary interior angles add to \\(180^\\circ\\).\n\n\n\n\n\nFig. 4:  Supplementary interior angles are shown in red.\n\n\n\n\nIn Fig. 4 the supplementary interior angles \\(\\angle AEF\\) and \\(\\angle CFE\\) are shown in red and add up to \\(180^\\circ\\)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20211220-School-TransformationsandWeightingtoCorrectModelInadequacies/index.html",
    "href": "blog/Technical-Blog/posts/20211220-School-TransformationsandWeightingtoCorrectModelInadequacies/index.html",
    "title": "Transformations and Weighting to Correct Model Inadequacies",
    "section": "",
    "text": "This post covers examples from Chapter 5 of Introduction to Linear Regression Analysis.\n\n\n1. Set Up\nFor this analysis I will be using four packages:\n\nmagrittr: for piping (%&gt;%)\ndplyr: to arrange the data\nMASS: to use the boxcox function\nlatex2exp: to put latex on graphs\n\n\n\nShow code\nlibrary(latex2exp) \nlibrary(magrittr) \nlibrary(dplyr) \nlibrary(MASS) \n\n\n\n\n2. Example 5.1: The Electric Utility Data\nAn electric utility company is interested in developing a model relating peak-hour demand \\((y)\\) to total energy usage during the month \\((x)\\).\nTo start lets look at the data.\n\n\nShow code\nex51 &lt;- utils::read.csv(\"data/ex5-1.csv\")\nex51\n\n\n    X Customer x_.kWh. y_.kW.\n1   1        1     679   0.79\n2   2        2     292   0.44\n3   3        3    1012   0.56\n4   4        4     493   0.79\n5   5        5     582   2.70\n6   6        6    1156   3.64\n7   7        7     997   4.73\n8   8        8    2189   9.50\n9   9        9    1097   5.34\n10 10       10    2078   6.85\n11 11       11    1818   5.84\n12 12       12    1700   5.21\n13 13       13     747   3.25\n14 14       14    2030   4.43\n15 15       15    1643   3.16\n16 16       16     414   0.50\n17 17       17     354   0.17\n18 18       18    1276   1.88\n19 19       19     745   0.77\n20 20       20     435   1.39\n21 21       21     540   0.56\n22 22       22     874   1.56\n23 23       23    1543   5.28\n24 24       24    1029   0.64\n25 25       25     710   4.00\n26 26       26    1434   0.31\n27 27       27     837   4.20\n28 28       28    1748   4.88\n29 29       29    1381   3.48\n30 30       30    1428   7.58\n31 31       31    1255   2.63\n32 32       32    1777   4.99\n33 33       33     370   0.59\n34 34       34    2316   8.19\n35 35       35    1130   4.79\n36 36       36     463   0.51\n37 37       37     770   1.74\n38 38       38     724   4.10\n39 39       39     808   3.94\n40 40       40     790   0.96\n41 41       41     783   3.29\n42 42       42     406   0.44\n43 43       43    1242   3.24\n44 44       44     658   2.14\n45 45       45    1746   5.71\n46 46       46     468   0.64\n47 47       47    1114   1.90\n48 48       48     413   0.51\n49 49       49    1787   8.33\n50 50       50    3560  14.94\n51 51       51    1495   5.11\n52 52       52    2221   3.85\n53 53       53    1526   3.93\n\n\nRight away we can see that for each customer there is a value x_.kWH for Kilowatt hour which corresponds to energy usage during the month, and y_.kW for kilowatt which would then be peak-hour demand. The plot of this is shown below.\n\n\nShow code\nbase::plot(ex51$x_.kWh., \n           ex51$y_.kW.,\n           xlab = \"Usage\",\n           ylab = \"Demand\")\n\n\n\n\n\nScatter diagram of the energy demand (kW) versus energy usage (kWh)\n\n\n\n\nAs a starting point a simple linear regression model is assumed. Lets look at the summary to get an equation for the least-squares fit, and analyze variability.\n\n\nShow code\nlm51 &lt;- stats::lm(ex51$y_.kW. ~ ex51$x_.kWh., data = ex51)\nsummary(lm51)\n\n\n\nCall:\nstats::lm(formula = ex51$y_.kW. ~ ex51$x_.kWh., data = ex51)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1399 -0.8275 -0.1934  1.2376  3.1522 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.8313037  0.4416121  -1.882   0.0655 .  \nex51$x_.kWh.  0.0036828  0.0003339  11.030 4.11e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.577 on 51 degrees of freedom\nMultiple R-squared:  0.7046,    Adjusted R-squared:  0.6988 \nF-statistic: 121.7 on 1 and 51 DF,  p-value: 4.106e-15\n\n\nFrom our summary our we can extrapolate our least-squares fit is: \\(\\hat y=-0.83130+0.00368x\\)\nFor this model \\(R^2=0.7046\\); that is about 70% of the variability in demand is accounted for by the straight-line fit to energy usage. The summary statistics do not reveal any obvious problems with this model.\nBelow this model is plotted with a red line.\n\n\nShow code\nbase::plot(ex51$x_.kWh.,\n           ex51$y_.kW.,\n           xlab = \"Usage\",\n           ylab = \"Demand\")\ngraphics::abline(lm51, col = \"red\")\n\n\n\n\n\nScatter diagram of the energy demand (kW) versus energy usage (kWh) with Simple Linear Model\n\n\n\n\nFrom visual inspection we can see the points on the far left side of the graph are much closer to the best fit line than those in the middle and right side of the graph. We might want to apply a transformation to this model, so lets look at the Studentized Residual also known as r student.\n\n\nShow code\nbase::plot(stats::fitted(lm51),\n           stats::rstudent(lm51),\n           ylab=latex2exp::TeX(r'($t_i$)'),\n           xlab=latex2exp::TeX(r'($\\hat{y}_i$)'),\n           pch = 16);graphics::abline(0, 0,lty = 2)\n\n\n\n\n\nPlot of R-Student vs. fitted values\n\n\n\n\nFrom this graph we can see that the residuals form an outward-opening funnel, indicating that the error variance is increasing as energy consumption increases. A transformation may be helpful in correcting this model inadequacy. To select the form of the transformation, note that the response variable y may be viewed as a “count” of the number of kilowatts used by a customer during a particular hour. The simplest probabilistic model for count data is the Poisson distribution. This suggests regressing \\(y^*=\\sqrt{y}\\) on x as a variance-stabilizing transformation.\n\n\nShow code\nex51$ystar &lt;- base::sqrt(ex51$y_.kW.)\nlm51T &lt;- stats::lm(ex51$ystar ~ ex51$x_.kWh., data = ex51)\nbase::summary(lm51T)\n\n\n\nCall:\nstats::lm(formula = ex51$ystar ~ ex51$x_.kWh., data = ex51)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.39185 -0.30576 -0.03875  0.25378  0.81027 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.822e-01  1.299e-01   4.481 4.22e-05 ***\nex51$x_.kWh. 9.529e-04  9.824e-05   9.699 3.61e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.464 on 51 degrees of freedom\nMultiple R-squared:  0.6485,    Adjusted R-squared:  0.6416 \nF-statistic: 94.08 on 1 and 51 DF,  p-value: 3.614e-13\n\n\nThe resulting least-squares fit is: \\(\\hat y^*=0.5822+0.0009529x\\)\n\n\nShow code\nbase::plot(stats::fitted(lm51T),\n           stats::rstudent(lm51T),\n           ylab=latex2exp::TeX(r'($t_i$)'),\n           xlab=latex2exp::TeX(r'($\\hat{y}^*_i$)'),\n           pch = 16);graphics::abline(0, 0,lty = 2)\n\n\n\n\n\n\n\n\n\nThe impression from examining this plot is that the variance is stable; consequently, we conclude that the transformed model is adequate.\nNote that there is one suspiciously large residual (customer 26) and one customer whose energy usage is somewhat large (customer 50). The effect of these two points on the fit should be studied further before the model is released for use.\n\n\n3. Example 5.2: The Windmill Data\nA research engineer is investigating the use of a windmill to generate electricity. He has collected data on the DC Output from his windmill and the corresponding wind velocity.\n\n\nShow code\nex52 &lt;- utils::read.csv(\"data/ex5-2.csv\")\nutils::head(ex52)\n\n\n  X ObservationNumber_i WindVelocity_xi_mph DCOutput_yi\n1 1                   1                 5.0       1.582\n2 2                   2                 6.0       1.822\n3 3                   3                 3.4       1.057\n4 4                   4                 2.7       0.500\n5 5                   5                10.0       2.236\n6 6                   6                 9.7       2.386\n\n\nThe data is plotted below.\n\n\nShow code\nbase::plot(ex52$WindVelocity_xi_mph,\n           ex52$DCOutput_yi,\n           xlab = \"Wind Velocity, X\",\n           ylab = \"DC Output, Y\")\n\n\n\n\n\n\n\n\n\nInspection of the scatter diagram indicates that the relationship between DC output \\((y)\\) and wind velocity \\((x)\\) may be nonlinear. However, we initially fit a straight-line model to the data, and look at the summary statistics.\n\n\nShow code\nlm52 &lt;- stats::lm(ex52$DCOutput_yi ~ ex52$WindVelocity_xi_mph, data = ex52)\nbase::summary(lm52)\n\n\n\nCall:\nstats::lm(formula = ex52$DCOutput_yi ~ ex52$WindVelocity_xi_mph, \n    data = ex52)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.59869 -0.14099  0.06059  0.17262  0.32184 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.13088    0.12599   1.039     0.31    \nex52$WindVelocity_xi_mph  0.24115    0.01905  12.659 7.55e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2361 on 23 degrees of freedom\nMultiple R-squared:  0.8745,    Adjusted R-squared:  0.869 \nF-statistic: 160.3 on 1 and 23 DF,  p-value: 7.546e-12\n\n\nThe summary statistics for this model are \\(R^2=0.8745\\), and \\(F_0=160.26\\) (the P-value is &lt;0.0001), and he regression model is: \\(\\hat y=0.1309+0.2411x\\), shown in red below.\n\n\nShow code\nbase::plot(ex52$WindVelocity_xi_mph,\n           ex52$DCOutput_yi,\n           xlab = \"Wind Velocity, x\",\n           ylab = \"DC Output, Y\")\ngraphics::abline(lm52, col = \"red\")\n\n\n\n\n\n\n\n\n\nBelow we can extract the fitted and residual values from our linear model, and then arrange them in order of increasing wind speed.\n\n\nShow code\nex52$fitted &lt;- stats::fitted(lm52)\nex52$resid &lt;- stats::resid(lm52)\nex52 %&gt;% dplyr::arrange(-dplyr::desc(ex52$WindVelocity_xi_mph)) \n\n\n    X ObservationNumber_i WindVelocity_xi_mph DCOutput_yi    fitted       resid\n1  25                  25                2.45       0.123 0.7216899 -0.59868986\n2   4                   4                2.70       0.500 0.7819771 -0.28197708\n3  11                  11                2.90       0.653 0.8302069 -0.17720685\n4   8                   8                3.05       0.558 0.8663792 -0.30837918\n5   3                   3                3.40       1.057 0.9507813  0.10621871\n6  16                  16                3.60       1.137 0.9990111  0.13798894\n7  24                  24                3.95       1.144 1.0834132  0.06058683\n8  23                  23                4.10       1.194 1.1195855  0.07441450\n9  13                  13                4.60       1.562 1.2401599  0.32184007\n10  1                   1                5.00       1.582 1.3366195  0.24538052\n11 20                  20                5.45       1.501 1.4451365  0.05586353\n12 14                  14                5.80       1.737 1.5295386  0.20746142\n13  2                   2                6.00       1.822 1.5777683  0.24423165\n14 10                  10                6.20       1.866 1.6259981  0.24000188\n15 12                  12                6.35       1.930 1.6621705  0.26782955\n16 19                  19                7.00       1.800 1.8189172 -0.01891722\n17 15                  15                7.40       2.088 1.9153768  0.17262323\n18 17                  17                7.85       2.179 2.0238938  0.15510624\n19  9                   9                8.15       2.166 2.0962384  0.06976158\n20 18                  18                8.80       2.112 2.2529852 -0.14098518\n21 21                  21                9.10       2.303 2.3253298 -0.02232985\n22  7                   7                9.55       2.294 2.4338468 -0.13984684\n23  6                   6                9.70       2.386 2.4700192 -0.08401917\n24  5                   5               10.00       2.236 2.5423638 -0.30636383\n25 22                  22               10.20       2.310 2.5905936 -0.28059360\n\n\nThe residuals show a distinct pattern, that is, they move systematically from negative to positive and back to negative again as wind speed increases.\n\n\nShow code\nbase::plot(stats::fitted(lm52),\n           stats::resid(lm52),\n           ylab=TeX(r'($e_i$)'),\n           xlab=TeX(r'($\\hat{y}_i$)'),\n           pch = 16);graphics::abline(0, 0,lty = 2)\n\n\n\n\n\n\n\n\n\nThis residual plot indicates model inadequacy and implies that the linear relationship has not captured all of the information in the wind speed variable. Note that the curvature was apparent in the earlier scatter diagram, but is greatly amplified in the residual plot\nClearly some other model form must be considered. We might initially consider using a quadratic model such as: \\(y=\\beta_0+\\beta_1x+\\beta_2x^2+\\epsilon\\) to account for the curvature. However since the quadratic model will eventually bend downward as wind speed increases, it would not be appropriate for these data. A more reasonable model for windmill data that incorporates an upper asymptote would be: \\(y=\\beta_0+\\beta_1(\\frac{1}{x})+\\epsilon\\).\n\n\nShow code\nex52$xstar &lt;- 1/ex52$WindVelocity_xi_mph\nlm52T &lt;- stats::lm(ex52$DCOutput_yi ~ ex52$xstar, data = ex52)\nbase::summary(lm52T)\n\n\n\nCall:\nstats::lm(formula = ex52$DCOutput_yi ~ ex52$xstar, data = ex52)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.20547 -0.04940  0.01100  0.08352  0.12204 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.9789     0.0449   66.34   &lt;2e-16 ***\nex52$xstar   -6.9345     0.2064  -33.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09417 on 23 degrees of freedom\nMultiple R-squared:   0.98, Adjusted R-squared:  0.9792 \nF-statistic:  1128 on 1 and 23 DF,  p-value: &lt; 2.2e-16\n\n\nThe fitted regression model is \\(\\hat y=2.9789-6.9345x'\\)\nThe summary statistics for this model are \\(R^2=0.98\\), and \\(F_0=1128\\) (the p value is &lt;0.0001).\n\n\nShow code\nbase::plot(stats::fitted(lm52T),\n           stats::rstudent(lm52T),\n           ylab=TeX(r'($t_i$)'),\n           xlab=TeX(r'($\\hat{y}_i$)'),\n           pch = 16);graphics::abline(0, 0,lty = 2)\n\n\n\n\n\n\n\n\n\nThis plot does not reveal any serious problems.\n\n\n4. Example 5.3: The Electic Utility Data\nWe use the Box-Cox procedure to select a variance-stabilizing transformation. The values of \\(SS_{Res}(\\lambda)\\) for various values are shown in the table.\n\n\nShow code\nboxcoxResult = MASS::boxcox(ex51$y_.kW. ~ ex51$x_.kWh., data = ex51, lambda = seq(-2,2,0.125))\n\n\n\n\n\n\n\n\n\nThe Box-Cox graph shows most of the data is below the 95% confidence interval.\n\n\nShow code\nlambda &lt;- boxcoxResult$x[which.max(boxcoxResult$y)]\nlambda\n\n\n[1] 0.5454545\n\n\nWhere \\(\\lambda\\approx\\) 0.5454545 could be used as an appropriate exponent to use to transform the data into a “normal shape.”"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210823-Original-Emojis/index.html",
    "href": "blog/Technical-Blog/posts/20210823-Original-Emojis/index.html",
    "title": "Enable Emojis in Quarto",
    "section": "",
    "text": "This post explains two ways to insert emojis into a quarto blog.1"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210823-Original-Emojis/index.html#footnotes",
    "href": "blog/Technical-Blog/posts/20210823-Original-Emojis/index.html#footnotes",
    "title": "Enable Emojis in Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nQuarto Documentation - Content Editing↩︎"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html",
    "href": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "",
    "text": "The problems on this page are from Probability & Statistics for Engineering & Sciences 9th Edition by Jay L. Devore - Duxbury Publisher, and the work is mine.\nWarning: package 'ggplot2' was built under R version 4.3.2"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#consider-the-strength-data-for-beams.",
    "href": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#consider-the-strength-data-for-beams.",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.10 Consider the strength data for beams.",
    "text": "1.10 Consider the strength data for beams.\n\na. Construct a stem-and leaf display of the data. What appears to be a representative strength value? Do the observations appear to be highly concentrated about the representative value or rather spread out?\n\n\nShow code\nbeam &lt;- c(5.9, 7.2, 7.3, 6.3, 8.1, 6.8, 7.0, 7.6, 6.8, 6.5, 7.0, 6.3, 7.9, 9.0,\n         8.2, 8.7, 7.8, 9.7, 7.4, 7.7, 9.7, 7.8, 7.7, 11.6, 11.3, 11.8, 10.7)\nstem(beam)\n\n\n\n  The decimal point is at the |\n\n   5 | 9\n   6 | 33588\n   7 | 00234677889\n   8 | 127\n   9 | 077\n  10 | 7\n  11 | 368\n\n\nA representative strength value would be 7.7, as more observations are concentrated around this value than any other.\n\n\nb. Does the display appear to be reasonably symmetric about a representative value, or would you describe its shape in some other way?\n\n\n\n\n\n\n\n\n\nNo, I would argue that the data has a slight positive skewed to the right. I say this because the higher frequency values (in blues) seem to be to the left of the representative value (indicated by the red line), which is to the left of the mean (indicated by the orange line).\n\n\nc. Do there appear to be any outlying strength values?\nYes there seems to be a small density of values outlying around 11.\n\n\nd. What proportion of strength observations in this sample exceed 10 MPa?\n\n\n[1] \"14.81 %\"\n\n\nAn unlikely 14.81% of values exceed 10 MPa."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#the-accompanying-specific-gravity-values-for-various-wood-types-used-in-construction-appeared-in-the-article-bolted-connection-design-values-based-on-european-yield-model-j.-of-structural-engr.-1993-2169-2186",
    "href": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#the-accompanying-specific-gravity-values-for-various-wood-types-used-in-construction-appeared-in-the-article-bolted-connection-design-values-based-on-european-yield-model-j.-of-structural-engr.-1993-2169-2186",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.12 The accompanying specific gravity values for various wood types used in construction appeared in the article “Bolted Connection Design Values Based on European Yield Model” (J. of Structural Engr., 1993: 2169-2186):",
    "text": "1.12 The accompanying specific gravity values for various wood types used in construction appeared in the article “Bolted Connection Design Values Based on European Yield Model” (J. of Structural Engr., 1993: 2169-2186):\n\n\nShow code\nwood.g &lt;- c(.31, .35, .36, .36, .37, .38, .40, .40, .40,\n            .41, .41, .42, .42, .42, .42, .42, .43, .44,\n            .45, .46, .46, .47, .48, .48, .48, .51, .54,\n            .54, .55, .58, .62, .66, .66, .67, .68, .75)\n\n\n\nConstruct a stem-and-leaf display using repeated stems (see the previous exercise), and comment on any interesting features of the display.\n\n\n\n  The decimal point is at the |\n\n  0 | 344444444444444444\n  0 | 555555555566677778\n\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  3 | 156678\n  4 | 0001122222345667888\n  5 | 14458\n  6 | 26678\n  7 | 5\n\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n  3 | 1\n  3 | 56678\n  4 | 000112222234\n  4 | 5667888\n  5 | 144\n  5 | 58\n  6 | 2\n  6 | 6678\n  7 | \n  7 | 5\n\n\nLooking at the three stem-and-leaf displays there are a few interesting features about the gravity for various wood types used in construction data that stands out. Dropping the last digit in the data, the first stem-and-leaf display shows us that there’s an even number of values that are &gt;0.5 and 0.5that appear most dense around 0.4. The second and third stem-and-leaf displays show us that these values are most dense about 0.42, with a small blip around 6.6 and obvious outlier for value 0.75."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#the-article-cited-in-example-1.2-also-gave-the-accompanying-strengths-observations-for-cylinders",
    "href": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#the-article-cited-in-example-1.2-also-gave-the-accompanying-strengths-observations-for-cylinders",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.16 The article cited in Example 1.2 also gave the accompanying strengths observations for cylinders:",
    "text": "1.16 The article cited in Example 1.2 also gave the accompanying strengths observations for cylinders:\n\n\nShow code\ncylinders &lt;- c(6.1, 5.8, 7.8, 7.1, 7.2, 9.2, 6.6, 8.3, 7.0, 8.3,\n           7.8, 8.1, 7.4, 8.5, 8.9, 9.8, 9.7, 14.1, 12.6, 11.2)\n\n\n\na. Construct a comparative stem-and-leaf display (see the previous exercise) of the beam and cylinder data and then answer the questions in parts (b)-(d) of Exercise 10 for the observations on cylinders.\n\n\n\n  The decimal point is at the |\n\n   5 | 8\n   6 | 16\n   7 | 012488\n   8 | 13359\n   9 | 278\n  10 | \n  11 | 2\n  12 | 6\n  13 | \n  14 | 1\n\n\n\nDoes the display appear to be reasonably symmetric about a representative value, or would you describe its shape in some other way?\n\n\n\ncylinders\n 5.8  6.1  6.6    7  7.1  7.2  7.4  8.1  8.5  8.9  9.2  9.7  9.8 11.2 12.6 14.1 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 7.8  8.3 \n   2    2 \n\n\n[1] 8.575\n\n\n[1] 8.2\n\n\nThis data of cylinders also appears to have a slight positive skew, shown by mean (orange line) to the right of the median (red line) and mode (yellow line). However the mean is a lot closer to the median and mode in this than in the previous graph because there appear to bee less extreme observations.\n\nDo there appear to be any outlying strength values?\n\n14.1 appears to be an outlying strength value, as well as 11.2 and 12.6. Without these three values the mean and median are almost identical, shown below in cylinders.clean.\n\n\n [1] 6.1 5.8 7.8 7.1 7.2 9.2 6.6 8.3 7.0 8.3 7.8 8.1 7.4 8.5 8.9 9.8 9.7\n\n\n[1] 7.858824\n\n\n[1] 7.8\n\n\n\nWhat proportion of strength observations in this sample exceed 10 MPa?\n\n\n\n[1] \"15 %\"\n\n\nAbout 15% of the data observations exceed 10 Mpa.\n\n\nb. In what ways are the two sides of the display similar? Are there any obvious differences between the beam observations and the cylinder observations?\nBoth displays show relatively normal distributions with slight skews to the right. Similarly both the beam and cylinders have approximately 15% of the data outside the normal distribution. An obvious difference between the two is that the beam data was more skewed than the cylinder. A reason for this may be that the range of values for the beam is smaller than the range of values for cylinders, so the beam mean is more sensitive to outlying data."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#exposure-to-microbial-products-especially-endotoxin-may-have-an-impact-on-vulnerability-to-allergic-diseases.-the-article-dust-sampling-methods-for-endotoxin-an-essential-but-underestimated-issue-indoor-air-2006-20-27-considered-various-issues-associated-with-determining-endotoxin-concentration.-the-following-data-on-concentration-eumg-in-settled-dust-for-one-sample-of-urban-homes-and-another-of-farm-homes-was-kindly-supplied-by-the-authors-of-the-cited-article.",
    "href": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#exposure-to-microbial-products-especially-endotoxin-may-have-an-impact-on-vulnerability-to-allergic-diseases.-the-article-dust-sampling-methods-for-endotoxin-an-essential-but-underestimated-issue-indoor-air-2006-20-27-considered-various-issues-associated-with-determining-endotoxin-concentration.-the-following-data-on-concentration-eumg-in-settled-dust-for-one-sample-of-urban-homes-and-another-of-farm-homes-was-kindly-supplied-by-the-authors-of-the-cited-article.",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.34 Exposure to microbial products, especially endotoxin, may have an impact on vulnerability to allergic diseases. The article “Dust Sampling Methods for Endotoxin – An Essential, But Underestimated Issue” (Indoor Air, 2006: 20-27) considered various issues associated with determining endotoxin concentration. The following data on concentration (EU/mg) in settled dust for one sample of urban homes and another of farm homes was kindly supplied by the authors of the cited article.",
    "text": "1.34 Exposure to microbial products, especially endotoxin, may have an impact on vulnerability to allergic diseases. The article “Dust Sampling Methods for Endotoxin – An Essential, But Underestimated Issue” (Indoor Air, 2006: 20-27) considered various issues associated with determining endotoxin concentration. The following data on concentration (EU/mg) in settled dust for one sample of urban homes and another of farm homes was kindly supplied by the authors of the cited article.\n\n\nShow code\nurban &lt;- c(6.0, 5.0, 11.0, 33.0, 4.0, 5.0, 80.0, 18.0, 35.0, 17.0, 23.0)\nfarm &lt;- c(4.0, 14.0, 11.0, 9.0, 9.0, 8.0, 4.0, 20.0, 5.0, 8.9, 21.0, 9.2, 3.0, 2.0, 0.3 )\n\n\n\na. Determine the sample mean for each sample. How do they compare?\n\n\nShow code\nmean(urban)\n\n\n[1] 21.54545\n\n\nShow code\nmean(farm)\n\n\n[1] 8.56\n\n\nThe mean endotoxin concentration is greater in urban homes than farm homes.\n\n\nb. Determine the sample median for each sample. How do they compare? Why is the median for the urban sample so different from the mean for that sample?\n\n\nShow code\nmedian(urban)\n\n\n[1] 17\n\n\nShow code\nmedian(farm)\n\n\n[1] 8.9\n\n\nThe median endotoxin concentration is greater in urban homes than farm homes.\nThe urban sample’s median is so different from the mean, because the mean is more sensitive to outliers in the data (such as 80 EU/mg) than the median is.\n\n\nc. Calculate the trimmed mean for each sample by deleting the smallest and largest observation. What are the corresponding trimming percentages? How do the values of these trimmed means compare to the corresponding means and medians?\nTo start I will sort the data to see what are the largest and smallest observations for both.\n\n\nShow code\nsort(urban)\n\n\n [1]  4  5  5  6 11 17 18 23 33 35 80\n\n\nShow code\nsort(farm)\n\n\n [1]  0.3  2.0  3.0  4.0  4.0  5.0  8.0  8.9  9.0  9.0  9.2 11.0 14.0 20.0 21.0\n\n\nFrom the sorted data I can see the lowest and highest values for urban are c(4,80), and for farm they are c(0.3,21). These values are removed from the urban.trim and farm.trim shown below.\n\n\nShow code\nurban.trim &lt;- c(6.0, 5.0, 11.0, 33.0, 5.0, 18.0, 35.0, 17.0, 23.0)\nfarm.trim &lt;- c(4.0, 14.0, 11.0, 9.0, 9.0, 8.0, 4.0, 20.0, 5.0, 8.9, 9.2, 3.0, 2.0)\n\n\nTo find the corresponding trimming percentages I subtracted the sum of the subtracted values from the sums of the urban and farm data sets respectively, and then divide each by the length of each data set respectively.\n\n\nShow code\n(sum(urban)-84)/length(urban)\n\n\n[1] 13.90909\n\n\nShow code\n(sum(farm)-21.3)/length(farm)\n\n\n[1] 7.14\n\n\nThe two values removed from the urban dataset is about 14% of the data, slightly more then the 7.14% variation of the farm data set.\nNow to look at the trimmed means.\n\n\nShow code\nmean(urban.trim)\n\n\n[1] 17\n\n\nShow code\nmean(farm.trim)\n\n\n[1] 8.238462\n\n\nThe trimmed mean of the urban data is closer to the median of the urban data, whereas the trimmed mean for the farm data is farther away from the mean and median of the untrimmed data. After this analysis it would seem that trimming the first data set may be appropriate, whereas trimming the second may lead to misleading data. Looking at the stem-and-leaf displays are helpful in visualizing distributions and outliers.\n\n\nShow code\nstem(urban, scale = 2)\n\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  0 | 4556\n  1 | 178\n  2 | 3\n  3 | 35\n  4 | \n  5 | \n  6 | \n  7 | \n  8 | 0\n\n\nShow code\nstem(farm, scale = .5)\n\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  0 | 02344589999\n  1 | 14\n  2 | 01"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#exercise-34-presented-the-following-data-on-endotoxin-concentration-in-settled-dust-both-for-a-sample-of-urban-homes-and-for-a-sample-of-farm-homes",
    "href": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#exercise-34-presented-the-following-data-on-endotoxin-concentration-in-settled-dust-both-for-a-sample-of-urban-homes-and-for-a-sample-of-farm-homes",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.48 Exercise 34 presented the following data on endotoxin concentration in settled dust both for a sample of urban homes and for a sample of farm homes:",
    "text": "1.48 Exercise 34 presented the following data on endotoxin concentration in settled dust both for a sample of urban homes and for a sample of farm homes:\n\n\nShow code\nurban\n\n\n [1]  6  5 11 33  4  5 80 18 35 17 23\n\n\nShow code\nfarm\n\n\n [1]  4.0 14.0 11.0  9.0  9.0  8.0  4.0 20.0  5.0  8.9 21.0  9.2  3.0  2.0  0.3\n\n\n\na. Determine the value of the sample standard deviation for each sample, interpret these values, and then contrast variability in the two samples.\n\n\nShow code\nsd(urban)\n\n\n[1] 22.29961\n\n\nShow code\nsd(farm)\n\n\n[1] 6.087669\n\n\nAnother way we could find the standard deviation given the hint and the following equation given in class :\n[s^2=_{i=1}^{n}(x_i - x)^2] [=[_{i=1}{n}x_i2 - ]]\n\n\nShow code\n# hint\nsigma_x_i.u &lt;- 237.0\nsigma_x_i.f &lt;- 128.4\nsigma_x_i_2.u &lt;- 10079\nsigma_x_i_2.f &lt;- 1617.94\nn.u &lt;- length(urban)\nn.f &lt;- length(farm)\nconstant.u &lt;- 1/(n.u-1)\nconstant.f &lt;- 1/(n.f-1)\nsv.u &lt;- constant.u*(sigma_x_i_2.u - (sigma_x_i.u)^2/n.u)\nsv.f &lt;- constant.f*(sigma_x_i_2.f - (sigma_x_i.f)^2/n.f)\nsqrt(sv.u)\n\n\n[1] 22.29961\n\n\nShow code\nsqrt(sv.f)\n\n\n[1] 6.087669\n\n\n\n\nb. Compute the fourth spread for each sample and compare. Do the fourth spreads convey the same message about variability that the standard deviations do? Explain.\nThe quickest way to do this in r is with quantile().\n\n\nShow code\nquantile(urban)\n\n\n  0%  25%  50%  75% 100% \n 4.0  5.5 17.0 28.0 80.0 \n\n\nShow code\nquantile(farm)\n\n\n  0%  25%  50%  75% 100% \n 0.3  4.0  8.9 10.1 21.0 \n\n\nExplanation : coming soon\nAnother way to find the forth spread is by first computing the upper fourth and lower fourth . To do that I first sorted the data, and then split it in half. Notice that because n for both data sets is odd, I figure out the middle value that is included in both, before I create the new data sets.\n\n\nShow code\n# sort \nsorted.u &lt;- sort(urban)\nsorted.f &lt;- sort(farm)\nsorted.u\n\n\n [1]  4  5  5  6 11 17 18 23 33 35 80\n\n\nShow code\nsorted.f\n\n\n [1]  0.3  2.0  3.0  4.0  4.0  5.0  8.0  8.9  9.0  9.0  9.2 11.0 14.0 20.0 21.0\n\n\nShow code\n# find included value \ninclude.u &lt;- as.integer(n.u/2)+1\ninclude.f &lt;- as.integer(n.f/2)+1\nsorted.u[include.u]\n\n\n[1] 17\n\n\nShow code\nsorted.f[include.f]\n\n\n[1] 8.9\n\n\nShow code\n# upper and lower forth \nurban.lower &lt;- c(4,  5,  5,  6, 11, 17)\nurban.upper &lt;- c(17, 18, 23, 33, 35, 80)\nfarm.lower &lt;- c(0.3,  2.0,  3.0,  4.0,  4.0,  5.0,  8.0,  8.9)\nfarm.upper &lt;- c(8.9,  9.0,  9.0,  9.2, 11.0, 14.0, 20.0, 21.0)\n\n\nNow that the data is sorted, the middle values of 17 and 8.9 have been found, and the data has been split into two chunks I can compute the minimum,lower forth median, median, upper forth median, and the largest value.\n\n\nShow code\nmin(urban)\n\n\n[1] 4\n\n\nShow code\nmedian(urban.lower)\n\n\n[1] 5.5\n\n\nShow code\nmedian(urban)\n\n\n[1] 17\n\n\nShow code\nmedian(urban.upper)\n\n\n[1] 28\n\n\nShow code\nmax(urban)\n\n\n[1] 80\n\n\nShow code\nmin(farm)\n\n\n[1] 0.3\n\n\nShow code\nmedian(farm.lower)\n\n\n[1] 4\n\n\nShow code\nmedian(farm)\n\n\n[1] 8.9\n\n\nShow code\nmedian(farm.upper)\n\n\n[1] 10.1\n\n\nShow code\nmax(farm)\n\n\n[1] 21\n\n\n\nThe authors of the cited article also provided endotoxin concentrations in dust bag dust:\n\n\n\nShow code\nurban.bag &lt;- c(34.0, 49.0, 13.0, 33.0, 24.0, 24.0, 35.0, 104.0, 34.0, 40.0, 38.0, 1.0)\nfarm.bag &lt;- c(2.0, 64.0, 6.0, 17.0, 35.0, 11.0, 17.0, 13.0, 5.0, 27.0, 23.0,\n              28.0, 10.0, 13.0, 0.2)\nquantile(urban.bag)\n\n\n   0%   25%   50%   75%  100% \n  1.0  24.0  34.0  38.5 104.0 \n\n\nShow code\nquantile(farm.bag)\n\n\n  0%  25%  50%  75% 100% \n 0.2  8.0 13.0 25.0 64.0 \n\n\nConstruct a comparative boxplot (as did the cited paper) and compare and contrast the four samples.\n\n\nShow code\npar(mfrow = c(1,2))\nboxplot(urban)\nboxplot(urban.bag)\n\n\n\n\n\n\n\n\n\nShow code\npar(mfrow = c(1,2))\nboxplot(farm)\nboxplot(farm.bag)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#the-article-a-thin-film-oxygen-uptake-test-for-the-evaluation-of-automotive-crankcase-lubricants-lubric.-engr.-1984-75-83-reported-the-following-data-on-oxidation-induction-time-min-for-various-commercial-oils",
    "href": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#the-article-a-thin-film-oxygen-uptake-test-for-the-evaluation-of-automotive-crankcase-lubricants-lubric.-engr.-1984-75-83-reported-the-following-data-on-oxidation-induction-time-min-for-various-commercial-oils",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.51 The article “A Thin-Film Oxygen Uptake Test for the Evaluation of Automotive Crankcase Lubricants” (Lubric. Engr., 1984: 75-83) reported the following data on oxidation-induction time (min) for various commercial oils:",
    "text": "1.51 The article “A Thin-Film Oxygen Uptake Test for the Evaluation of Automotive Crankcase Lubricants” (Lubric. Engr., 1984: 75-83) reported the following data on oxidation-induction time (min) for various commercial oils:\n\n\nShow code\noxi.induct.time.min &lt;- c(87, 103, 130, 160, 180, 195, 132, 145, 211, 105, 145,\n                         153, 152, 138, 87, 99, 93, 119, 129)\n\n\n\na. Calculate the sample variance and the standard deviation.\n\n\nShow code\noxi.var &lt;- var(oxi.induct.time.min)\noxi.sd &lt;- sd(oxi.induct.time.min)\noxi.var\n\n\n[1] 1264.766\n\n\nShow code\noxi.sd\n\n\n[1] 35.56355\n\n\n\n\nb. If the observations were re-expressed in hours, what would be the resulting values of the sample variance and sample standard deviation? Answer without actually performing the re-expression.\nThe standard deviation has the same units as the data values (minutes) so in hours the standard deviation would be 35.56/60 (or a little over half an hour) whereas the variance is the standard deviation squared, so the values would be converted 1264.766/60^2.\n\n\nShow code\noxi.var/60^2\n\n\n[1] 0.3513239\n\n\nShow code\noxi.sd/60\n\n\n[1] 0.5927258\n\n\nShow code\n# verification \noxi.induct.time.hour &lt;- oxi.induct.time.min/60\nvar(oxi.induct.time.hour)\n\n\n[1] 0.3513239\n\n\nShow code\nsd(oxi.induct.time.hour) \n\n\n[1] 0.5927258"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#observations-on-burst-strength-lbin2-were-obtained-both-for-test-nozzle-welds-proper-procedures-are-the-key-to-welding-radioactive-waste-canisters-welding-j.-auud.-1997-61-67",
    "href": "blog/Technical-Blog/posts/20210628-School-Stat451Hw1/index.html#observations-on-burst-strength-lbin2-were-obtained-both-for-test-nozzle-welds-proper-procedures-are-the-key-to-welding-radioactive-waste-canisters-welding-j.-auud.-1997-61-67",
    "title": "Statistics 451: Applied Statistics for Engineers and Scientists - Homework 1",
    "section": "1.60 Observations on burst strength (lb/in2) were obtained both for test nozzle welds (“Proper Procedures Are the Key to Welding Radioactive Waste Canisters,” Welding J., Auud. 1997: 61-67)",
    "text": "1.60 Observations on burst strength (lb/in2) were obtained both for test nozzle welds (“Proper Procedures Are the Key to Welding Radioactive Waste Canisters,” Welding J., Auud. 1997: 61-67)\n\n\nShow code\nTest &lt;-c(7200, 6100, 7300, 7300, 8000, 7400,\n         7300, 7300, 8000, 6700, 8300)         \nCannister &lt;- c(5250, 5625, 5900, 5900, 5700, 6050,\n               5800, 6000, 5875, 6100, 5850, 6600)\n\n\n\na. Construct a comparative boxplot and comment on interesting features (the cited article did not include such a picture, but the authors commented that they had looked at one).\n\n\nShow code\npar(mfrow = c(1,2))\nboxplot(Test, ylim = c(5000, 8500), main = \"Test\")\nboxplot(Cannister, ylim = c(5000, 8500), main = \"Cannister\")\n\n\n\n\n\n\n\n\n\n\n\nShow code\nmean(Test)-mean(Cannister)\n\n\n[1] 1467.045\n\n\nShow code\nmean(Test)\n\n\n[1] 7354.545\n\n\nShow code\nmean(Cannister)\n\n\n[1] 5887.5\n\n\nShow code\nsd(Test)\n\n\n[1] 613.7811\n\n\nShow code\nsd(Cannister)\n\n\n[1] 317.9301"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230130-Original-Flashcards/index.html",
    "href": "blog/Technical-Blog/posts/20230130-Original-Flashcards/index.html",
    "title": "Flashcards",
    "section": "",
    "text": "This post uses interactive flashcards to cover terminology in the fields of data science, machine learning, mathematics, probability, and statistics.\n\n\nIntroduction\nI’ve recently been applying to and partaking in interviews for remote data science positions all over the country. The last couple of first round technical interviews I went on I was asked a range of questions related to data science, statistics, machine learning, probability, linear algebra, and mathematics. To test myself on answering these types of questions more confidently I created these definitions “Flashcards”, which are actually tabset panels, a component layout in quarto.\nEach definition can be viewed by clicking on the “Definition” tab for each word. All words are sorted alphabetically, definitions are generally casual, words will be continually added, and regularly updating.\n\n\nA\n\nA/B TestingDefinition\n\n\n\n\n\nTo compare two versions of something, usually a control (A) and a test variable (B).\n\n\n\n\n\nB\n\nBayes TheoremDefinition\n\n\n\n\n\nA method for calculating conditional probability, or the likelihood of one event occurring based on prior knowledge of conditions that might be related to the event.\n\\[P(A|B)=\\frac{P(A|B)P(A)}{P(B)}=\\frac{\\text{(likelihood)}\\times\\text{(prior)}}{\\text{(evidence)}}\\]\n\n\n\n\nBiasDefinition\n\n\n\n\n\nWhen a model or statistic doesn’t provide a true representation of the population.\n\\[bias=\\mathbb{E}[f'(x)]-f(x)\\]\nBias of the estimated function tells us the capacity of the underlying model to predict the values.\nHigh bias = overly-simplified model, under-fitting, high error on both testing and training data.\n\n\n\n\nBinomial PropertyDefinition\n\n\n\n\n\nThe probability of exactly x successes on n repeated trials in an experiment which has two possible outcomes. \\[P_x={n\\choose x}p^xq^{n-x}\\]\n\n\n\n\n\nC\n\nCategorical DataDefinition\n\n\n\n\n\nData that can be divided into groups or categories such as sex, race, and age.\n\n\n\n\nConditional ProbabilityDefinition\n\n\n\n\n\nThe probability of an event (A) given that another event (B) has already occurred.\n\\[P(A|B)=P(A\\cap B)P(B)\\]\n\n\n\n\nConfusion (Error) MatrixDefinition\n\n\n\n\n\nA technique for summarizing performance measurement for machine learning classification algorithms that makes it easy to see whether the system is confusing classes.\n\n\n\n\n\nD\n\nData ClassificationDefinition\n\n\n\n\n\nOrganizing data by relevant categories according to predefined criteria so that it may be used and protected more efficiently.\n\n\n\n\nData LeakageDefinition\n\n\n\n\n\nWhen information outside the training data is used to create the model.\n\n\n\n\nData ScienceDefinition\n\n\n\n\n\nStudying data to find insight using computer science, mathematics, and statistics.\n\n\n\n\nDatabaseDefinition\n\n\n\n\n\nOrganized collection of data.\n\n\n\n\nDecision TreeDefinition\n\n\n\n\n\nA flowchart that starts with one main idea or question and branches out with potential outcomes of each decision using classification and regression techniques.\n\n\n\n\nDerivativeDefinition\n\n\n\n\n\nRate of change.\n\n\n\n\nDescriptive StatisticsDefinition\n\n\n\n\n\nDescribes features and summaries of data such as mean, and variance.\n\n\n\n\nDeterminateDefinition\n\n\n\n\n\nA scalar function made up of the entries of a square matrix. It is used to find the inverse of a matrix, and has a lot of important properties related to systems of linear equations.\n\\[\\begin{bmatrix} a & b \\\\ c & d\\end{bmatrix}=ad-bc\\]\n\n\n\n\nDimensionality ReductionDefinition\n\n\n\n\n\nThe technique of reducing the amount of random variables (or features) while retaining as much information as possible. This is done to reduce complexity, improve performance, and make the data easier to visualize.\n\n\n\n\nDiscrete MathematicsDefinition\n\n\n\n\n\nMathematics that deals with distinct, separate values instead of continuous values.\n\n\n\n\n\nE\n\nEigenvalueDefinition\n\n\n\n\n\nA scalar that is used to transform an eigenvalue, and considered as a factor by which it is stretched. Often denoted by \\(\\lambda\\).\n\n\n\n\nEigenvectorDefinition\n\n\n\n\n\nAre non-zero vectors that do not change direction when any linear transformation is applied.\n\n\n\n\n\nG\n\nGamma FunctionDefinition\n\n\n\n\n\nA generalization of the factorial function, it is commonly used to estimate new data points based on known values.\n\\[\\Gamma(x)=(n-1)!\\]\n\\[\\Gamma(z)=\\int_0^\\infty t^{z-1}e^{-t}dt\\]\n\n\n\n\n\nH\n\nHomogenousDefinition\n\n\n\n\n\nAn equation that contains itself, or one of its derivatives.\n\\[f(zx,zy)=z^n f(x,y)\\]\n\n\n\n\nHypothesis TestingDefinition\n\n\n\n\n\nTesting a hypothesis and comparing it against the null.\n\n\n\n\n\nI\n\nInferential StatisticsDefinition\n\n\n\n\n\nUsed to make predictions about population or data.\n\n\n\n\nInterval DataDefinition\n\n\n\n\n\nData that is measured along a scale, where each point is placed at equal distance from one another. Examples would be temperature, or SAT scores.\n\n\n\n\n\nK\n\nK-Mean ClusteringDefinition\n\n\n\n\n\nAn unsupervised learning algorithm which groups an unlabeled dataset into clusters with similar properties such as mean. An example might be to group similar customers and then to target them using different types of marketing.\n\n\n\n\n\nL\n\nLaw of Large NumbersDefinition\n\n\n\n\n\nAs the sample size increases the mean gets closer to the average of the population.\n\n\n\n\nLinear RegressionDefinition\n\n\n\n\n\nUses a liner approach to modeling the relationship between regressor (predictor) variables \\(x\\) and a response variable \\(y\\).\n\\[y=\\beta_0+\\beta_1x+\\epsilon\\]\n\\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_nx_n+\\epsilon\\]\n\n\n\n\n\nM\n\nMachine LearningDefinition\n\n\n\n\n\nA method that uses algorithms to build models to make predictions or decisions.\n\n\n\n\nMatrixDefinition\n\n\n\n\n\nA rectangular array of numbers arranged in rows and columns which represent a mathematical expression.\n\n\n\n\n\nN\n\nNeural NetworkDefinition\n\n\n\n\n\nA type of artificial intelligence that uses connected nodes which loosely model the neurons in the brain. Each node, also known as a neuron, is connected by what is called an edge. Both neurons and edges have a weight that adjusts as the model learns, and can increase or decrease the strength of the signal which travels from the first layer (input) to the last layer (output).\n\n\n\nA neural network showing nodes connected by edges, an input layer, hidden layers, and an output layer.\n\n\n\n\n\n\nNominal DataDefinition\n\n\n\n\n\nIs categorical data that groups variables into labeled categories that do not overlap, and cannot be ranked. Nominal data needs to be grouped to be analyzed. Examples would be sex or race.\n\n\n\n\n\nO\n\nOrdinal DataDefinition\n\n\n\n\n\nIs categorical data that has an order or ranking system such as education level, economic status, or satisfaction rating.\n\n\n\n\nOverfittingDefinition\n\n\n\n\n\nWhen machine learning models fit exactly to the training model, and therefore may fail to predict future observations.\n\n\n\n\n\n\nR\n\nRandom Forest ModelDefinition\n\n\n\n\n\nA classification algorithm that consists of many decision trees, and can correct decision trees’ habit of overfitting to their training set.\n\n\n\n\nRatio DataDefinition\n\n\n\n\n\nIs quantitative data that has a true zero such as speed, age, and weight.\n\n\n\n\nRegularizationDefinition\n\n\n\n\n\nA technique to reduce the errors of overfitting by adding extra information.\n\n\n\n\n\nS\n\nSnowflake and Start SchemeDefinition\n\n\n\n\n\nBoth are logical arrangements of a multidimensional database, where the fact table is in the middle of the structure, and it is surrounded by dimension tables. A snowflake scheme has normalized dimension tables meaning there are sub-dimensional tables, whereas a star schema is denormalized and easier to query since there are fewer joins between tables.\n\n\nStar Schema\n\n\nSnowflake Schema\n\n\n\n\n\n\n\nStationary ProcessDefinition\n\n\n\n\n\nA type of stochastic (random) process whose joint probability distribution does not change over time. An example would be white noise.\n\n\n\n\nStatisticsDefinition\n\n\n\n\n\nApplied math used to study data to form a judgment in a case of real world applications.\n\n\n\n\nSystems of Linear EquationsDefinition\n\n\n\n\n\nTwo or more linear equations working together.\n\n\n\n\n\nT\n\nTraining and Test DataDefinition\n\n\n\n\n\nTraining data is a subset of the original data which is used to train machine learning models.\nTest data is another subset of the original data which is independent of the training data, and used to test the accuracy of the model.\n\n\n\n\nTransformationDefinition\n\n\n\n\n\nA linear mapping between two vector spaces that preserves the operations of vector addition and scalar multiplication.\n\n\n\n\nType 1 and Type 2 ErrorDefinition\n\n\n\n\n\nType 1 error is a false positive (rejects the null which is actually true), and a type 2 error is a false negative (fails to reject the null which is actually false).\n\n\n\nLeft image reads “Type 1 error (false positive)”, and shows a doctor telling a man he is pregnant. Right image reads “Type 2 error (false negative)”, and shows a doctor telling a pregnant patient “You’re not pregnant”.\n\n\n\n\n\n\n\nU\n\nUneven or Unbalanced DataDefinition\n\n\n\n\n\nWhen the target variable has more observations in a specific class than the others. It would not be a good idea to use accuracy as a performance measure for highly imbalanced data."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230530-TidyTuesday-CentenariansData/index.html",
    "href": "blog/Technical-Blog/posts/20230530-TidyTuesday-CentenariansData/index.html",
    "title": "Week 22 Tidy Tuesday: Centenarians Data",
    "section": "",
    "text": "For week 22 of #TidyTuesday , explore centenarian data, revealing a surprising trend: many centenarian deaths occur in January.\n\nVerified Oldest People\nThe data this week comes from the Wikipedia List of the verified oldest people via frankiethull on GitHub. Thank you for the submission, Frank!\n\nThese are lists of the 100 known verified oldest people sorted in descending order by age in years and days. The oldest person ever whose age has been independently verified is Jeanne Calment (1875–1997) of France, who lived to the age of 122 years and 164 days. The oldest verified man ever is Jiroemon Kimura (1897–2013) of Japan, who lived to the age of 116 years and 54 days. The oldest known living person is Maria Branyas of Spain, aged 116 years, 85 days. The oldest known living man is Juan Vicente Pérez of Venezuela, aged 114 years, 1 day. The 100 oldest women have, on average, lived several years longer than the 100 oldest men.\n\n\n\n\nCode\nThis week I wanted to do more to spice up a bar plot. So I gave the visual a dark black background, and highlighted the main findings in red. This project retains its core components of cleaning, visualizing, and styling, while the index.R file handles data loading, file execution, and the generation of a data visualization.\n\nCleaningVisualizeStyleindex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\n# magrittr: %&gt;% pipe function. \n# dplyr: data cleaning functions. \nbase::library(dplyr)\nbase::library(magrittr)\n\n#### Cleaning Function ####\nclean &lt;- function(df){\n  clean_df &lt;- df %&gt;%\n    dplyr::mutate(death_month = base::format(death_date, \"%m\")) %&gt;%\n    dplyr::group_by(death_month) %&gt;%\n    dplyr::summarise(count = dplyr::n()) %&gt;%\n    # highlight jan. \n    mutate(to_highlight = ifelse(death_month == \"01\", \"yes\", \"no\"))\n  clean_df$to_highlight[13] &lt;- \"no\"\n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(ggplot2)\n\n#### Visual Function ####\nvis &lt;- function(clean_df){\n  vis &lt;- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(x = death_month, y = count, fill = to_highlight)) +\n    geom_bar(stat = \"identity\")\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Install Packages\ninstall.packages(\"ggchicklet\", repos = \"https://cinc.rud.is\")\n\n#### Load Packages ####\nbase::library(ggplot2)\nbase::library(ggchicklet)\n\n#### Load Fonts ####\nsysfonts::font_add_google(\"EB Garamond\", \"font\")\nshowtext::showtext_auto()\n\n#### Load Colors ####\ncol1 &lt;- \"white\"\ncol2 &lt;- \"#0d0d0d\"\ncol3 &lt;- \"#b22b2e\"\ncol4 &lt;- \"#6d6a6e\"\n\n#### Style Function ####\nsty &lt;- function(vis){\n  sty &lt;- vis +\n    # labs\n    ggplot2::labs(\n      title = \"High Number of Centenarian Deaths in January\",\n      subtitle = \"Of the 200 oldest verified men and women,\\n the most common month for a centarian to die in is January.\",\n      caption = \"\\nRandi Bolt ~ #TidyTuesday - Verified Oldest People ~ May 2023\",\n      x = \"Death Month\",\n      y = \"Count\")  +\n    # ylim\n    ylim(0, 36) +\n    # add numbers above boxes\n    ggplot2::geom_text(\n      ggplot2::aes(\n        label = count,\n        vjust = -.5),\n      color = col1,\n      size = 20\n    ) + \n    # scales \n    ggplot2::scale_x_discrete(\n      labels = c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\", \"Alive\")\n    ) +\n    scale_fill_manual(values  = c(\"yes\" = col3, \"no\"= col4), guide = \"none\") + \n    # theme\n    ggplot2::theme(\n      plot.title = element_text(\n        size = 100,\n        family = \"font\",\n        face = \"bold\",\n        hjust = .5,\n        vjust = -5,\n        color = col1),\n      plot.subtitle = element_text(\n        size = 55,\n        family = \"font\",\n        hjust = .5,\n        vjust = -20,\n        color = col1),\n      plot.caption = element_text(\n        size = 30,\n        family = \"font\",\n        hjust = .5,\n        color = col1),\n      axis.title = element_text(\n        size = 50, \n        family = \"font\",\n        face = \"bold\",\n        color = col1),\n      axis.text = element_text(\n        size = 45, \n        family = \"font\",\n        color = col1),\n      axis.text.x = element_text(angle = 25), \n      plot.background = element_rect(fill = col2),\n      panel.background = element_rect(fill = col2),\n      panel.grid.major.x = element_blank(),\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank()) +               \n    # ggplot2 barplot with round corners\n    geom_chicklet(radius = grid::unit(3, \"mm\")) \n  return(sty)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\nbase::library(tidyverse)\n\n#### Load Data ####\n# centenarians: 100 verified oldest men and women.\ncentenarians &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-05-30/centenarians.csv')\n\n#### Clean Data ####\nclean_data &lt;- clean(centenarians)\n\n#### Create Data Visual ####\nvis_data &lt;- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis &lt;- sty(vis_data)\n\n#### Save Plot ####\nggsave(\n  \"plot.png\", \n  width = 30, \n  height = 20, \n  units = \"cm\")\n\n\n\n\nWeek 22 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nTo highlight the month of January in the cleaning file I used the dplyr::mutate() function to create a “to_highlight” column for any death month equal to January. Then in the visualize package I assigned the fill aesthetic to the “to_highlight” column.\nTo curve the edges of the boxes (so they look more like tombstones) in the styling file I used ggchicklet::geom_chicklet() to assign a curved radius of 3 milimeters.\nTo angle the labels on the x-axis I used ggplot2::theme(axis.text.x = element_text(angle = 25)).\nSpacing is a little funky, but I think it might work for this visual.\nFrequentist data visuals seem to be the low hanging fruit for these Tid Tuesday submissions, and my goal is to push onto more predictive and model based visual analysis by the end of the summer."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html",
    "href": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html",
    "title": "Midterm Prep. Modern College Geometry",
    "section": "",
    "text": "Notes consists of logic, Euclid’s 5th, Congruence and Length Theorem, Congruence and Angle Theorem, angles and parallel lines, and triangle congruence theorems."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html#example",
    "href": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html#example",
    "title": "Midterm Prep. Modern College Geometry",
    "section": "Example",
    "text": "Example\nConditional Statement: If it is cloudy then it is raining.\nNegation: It could be cloudy and not raining.\nInverse: If it is not cloudy then it is not raining.\nContrapositive: If it is not raining then it is not cloudy.\nConverse. If it is rainy, then it is cloudy."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html#example-1",
    "href": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html#example-1",
    "title": "Midterm Prep. Modern College Geometry",
    "section": "Example",
    "text": "Example\nWhen proving supplementary angles add to \\(180^\\circ\\) we were able to use Euclids 5th element to say that the supplementary interior angles added up to \\(\\geq 180^\\circ\\) because the lines are parallel (and don’t intersect)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html#proof-points",
    "href": "blog/Technical-Blog/posts/20220321-School-MidtermPrepModernCollegeGeometry/index.html#proof-points",
    "title": "Midterm Prep. Modern College Geometry",
    "section": "Proof Points",
    "text": "Proof Points\n\n“Because \\(\\triangle ABC\\cong \\triangle DEF\\) then there is an isometry f that superimposes angle ABC on angle DEF. Isometries preserve angle measure, so angles ABC and DEF must have had the same measure.”\n“Suppose angles ABC and DEF have the same measure.” Then explain isometry needed to move angle ABC to angle DEF. “Translation, rotation, and reflection are all isometries, so we’ve shown angles ABC and DEF are congruent.”"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210927-Original-GenerativeArtJasmines/index.html",
    "href": "blog/Technical-Blog/posts/20210927-Original-GenerativeArtJasmines/index.html",
    "title": "Generative Art with Jasmines",
    "section": "",
    "text": "For a fall 2021 PSU class, I created ‘self’ art using generative techniques in R. Most pieces are based on examples from djnavarro’s github.\n\n\n1. Set-Up\nI will be using jasmines to create art, and dplyr to pipe the code.\n\n\nShow code\n# remotes::install_github(\"djnavarro/jasmines\")\nlibrary(jasmines)\nlibrary(dplyr) \n\n\n\n\n2. Randi\nWhen playing around with this package, I initially had something less fluid and full of right angles, but wanted to show more movement in the design. I have a dance background and aside from fluid movement we also focused a lot on circles and rotation. Another reason I like this design is because it reminds me of a flower. I have seven tattoos, two of which are flowers. The two colors I chose are salmon and rosewood. I enjoy different shades of pink, and colors like salmon, and rosewood feel like a more sophisticated pink to me.\n\n\nShow code\nuse_seed(5) %&gt;%\n  entity_circle(grain = 1000, size = 10) %&gt;%\n  unfold_warp(iterations = 100) %&gt;%\n  style_ribbon(\n    color = \"#9E4244\",\n    background = \"#FDAB9F\")\n\n\n\n\n\n\n\n\n\n\n\n3. Unfolding Circle\n\n\nShow code\nuse_seed(1) %&gt;%\n  entity_circle(grain = 1000, size = 4) %&gt;%\n  unfold_warp(iterations = 100) %&gt;%\n  style_ribbon(\n    palette=\"base\",\n    colour = \"ind\",\n    background = \"mistyrose\")\n\n\n\n\n\n\n\n\n\n\n\n4. Typophobia\n\n\nShow code\nscene_discs(\n  rings = 13, \n  points = 500, \n  size = 5\n  ) %&gt;%\n  mutate(ind = 1:n()\n         ) %&gt;%\n  unfold_warp(\n    iterations = 10,\n    scale = .5, \n    output = \"layer\" \n  ) %&gt;%\n  unfold_tempest(\n    iterations = 10,\n    scale = .01\n  ) %&gt;%\n  style_ribbon(\n    color = \"#48AAAD\",\n    colour = \"ind\",\n    alpha = c(.4,.1),\n    background = \"#016064\" \n  ) \n\n\n\n\n\n\n\n\n\n\n\n5. Snake Charmer\n\n\nShow code\nuse_seed(4) %&gt;%\n  entity_circle(grain = 10000) %&gt;%\n  unfold_tempest(iterations = 13) %&gt;%\n  style_ribbon(background = \"oldlace\")"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20211129-Proof-Idempotent/index.html",
    "href": "blog/Technical-Blog/posts/20211129-Proof-Idempotent/index.html",
    "title": "Prove H and I-H are Idempotent",
    "section": "",
    "text": "Define the hat matrix \\(H=X(X^TX)^{-1}X^T\\). Prove H and I-H are Idempotent\n\n\nProof\nFor H to be Idempotent then \\(HH=H\\)\n\\[\\begin{equation}\\label{HH=H}\n\\begin{split}\nHH & =[X(X^TX)^{-1}X^T][X(X^TX)^{-1}X^T]\\\\\n& = X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T\\quad\\quad(X^TX)^{-1}X^TX=1\\\\\n& = X(X^TX)^{-1}X^T\\\\\n& = H\n\\end{split}\n\\end{equation}\\]\nTherefore by the series of equalities H is idempotent.\nFor I-H to be idempotent then \\((I-H)(I-H)=I-H\\)\n\\[\\begin{equation}\\label{I-H}\n\\begin{split}\n(I-H)(I-H) & =II-HI-IH+HH\\quad\\quad II=I, HI=IH=H, HH=H\\\\\n& = I-H-H+H\\\\\n& = I-H\n\\end{split}\n\\end{equation}\\]\nTherefor by the series of equalities I-H is idempotent.\nQED."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20211025-Original-Epsilon/index.html",
    "href": "blog/Technical-Blog/posts/20211025-Original-Epsilon/index.html",
    "title": "Let epsilon = min(x-a, b-x)",
    "section": "",
    "text": "This post explains what it means for \\(\\epsilon=\\text{min}(x-a,b-x)\\).\n\nExplanation\nConsider the real number line \\(\\mathbb{R}\\) and some value x which lies between (a,b). When \\(\\epsilon=\\text{min}(x-a,b-x)\\) then \\(\\epsilon\\) is equal to \\(2\\times\\) smaller distance to either a or b. For example, in the picture below x is closer to a, so x-a is smaller than b-x. Therefore \\(\\epsilon=2(x-a)\\).\n\nIf it helps to apply values consider \\(a=1\\), \\(b=4\\), and \\(x=2\\). Then\n\\(\\epsilon=\\text{min}(x-a,b-x)=\\text{min}(2-1,4-2)=\\text{min}(1,2)=2(1)=2\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240326-TidyTuesday-marchmadness/index.html",
    "href": "blog/Technical-Blog/posts/20240326-TidyTuesday-marchmadness/index.html",
    "title": "Week 13 Tidy Tuesday: NCAA Men’s March Madness",
    "section": "",
    "text": "For TidyTuesday, I analyzed March Madness, comparing actual round winners to fan forecasts. My chart highlights bracket busters: red for teams that exceeded expectations, blue for those that fell short.\n\nNCAA’s Men’s March Madness\n\nThis week’s data is NCAA Men’s March Madness data from Nishaan Amin’s Kaggle dataset and analysis Bracketology: predicting March Madness.\n\n\nCode\nThis code follows the established #TidyTuesday tradition of cleaning, visualizing, and styling data. However, it introduces a streamlined structure organized into four distinct phases: Set-Up, Clean, Graph, and Save.\n\nSet-UpCleanGraphSaveLinks\n\n\n\n\nShow Code\n#### Packages ####\n# tidyverse: A collection of data-related packages.\n# googlesheets4: Read data from a google sheet URL. \n# forcats: Working with Categorical Variables (Factors)\n# showtext: Use various fonts. \n# ggtext: Use markdown in subtitle. \nbase::library(tidyverse)\nbase::library(googlesheets4)\nbase::library(forcats)\nbase::library(showtext)\nbase::library(ggtext)\n\n#### Data ####\n# public_picks: TidyTuesday Data of public picks. \n# team_results: My data set of team results for 2024.\npublic_picks &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-03-26/public-picks.csv')\nteam_results &lt;- googlesheets4::read_sheet(\n  \"https://docs.google.com/spreadsheets/d/1rxHKeP0RVZZ2ujjqRPq3AhK6R8a4aI7LMRyli543gTs/edit#gid=0\",\n  sheet = \"2024\",\n  na=\"TBD\"\n)\n\n#### Fonts ####\n# font_add_google(): Search Google Fonts. \n# showtext_auto(): Turn showtext on for graphics. \nsysfonts::font_add_google(\"Merriweather\", \"font\")\nshowtext::showtext_auto()\n\n#### Colors ####\n# col1: Text \n# col2: Background\ncol1 &lt;- \"#002A5C\"\ncol2 &lt;- \"#e0a761\"\n\n### Text ####\n# title_text\n# subtitle_text\n# caption_text\n# xlab_text\n# ylab_text\n# fill_text\ntitle_text &lt;- \"Team Performance vs. Public Expectation\"\nsubtitle_text &lt;- \"March Madness is a knockout college basketball tournament with 68 teams &lt;br&gt;battling in six rounds for the title. This chart, covering men's teams for &lt;br&gt; 2024, shows when teams didn't meet or beat fan expectations. &lt;span style='color:red;'&gt;Red &lt;/span&gt; means &lt;br&gt;a team did &lt;span style='color:red;'&gt;better than expected&lt;/span&gt;, &lt;span style='color:blue;'&gt;blue means worse&lt;/span&gt;, and gray indicates &lt;br&gt;that the data isn't currently available. The values represent the percentage &lt;br&gt;point difference between actual wins and public forecasts.\"\ncaption_text &lt;- \"Randi Bolt \\nApril 2024 \\n#TidyTuesday \\nNCAA March Madness\"\nxlab_text &lt;- \"NCAA Tournament Rounds\"\nylab_text &lt;- \"Team (Seed)\"\nfill_text &lt;- \"Difference\"\n\n\nSet-Up\n\nPackages\nData\nFonts\nColors\nText\n\n\n\n\n\nShow Code\n### Clean Data\n# 1. Create win columns for each round. \n# 2. Convert char percentage columns into doubles.\n# 3. Join team_results and public picks by TEAM.\n# 4. Calculate the difference b/t team results and public picks.\n# 5. Pivot heatmap data longer so that rounds are in one column.\n# 6. Rename rounds to extend names for labels.\n# 7. Re-assign factor levels for rounds column.\n\n# 1. Creates new columns that will have a 1 value if the team made it to that round at least once, otherwise it will be 0.\nteam_results &lt;- team_results |&gt;\n  mutate(R64WIN = ifelse(R64 &gt; 0, 1, 0),\n         R32WIN = ifelse(R32 &gt; 0, 1, 0),\n         S16WIN = ifelse(S16 &gt; 0, 1, 0),\n         E8WIN = ifelse(E8 &gt; 0, 1, 0),\n         F4WIN = ifelse(F4 &gt; 0, 1, 0),\n         F2WIN = ifelse(F2 &gt; 0, 1, 0),\n         CHAMPWIN = ifelse(CHAMP &gt; 0, 1, 0))\n\n# 2. Convert character percentage columns into doubles.\npublic_picks &lt;- public_picks |&gt;\n  filter(YEAR == 2024) |&gt;\n  mutate(\n    across(\n      R64:FINALS, \n      ~readr::parse_number(as.character(.))\n    )\n  )\n\n# 3. Join team_results and public picks by TEAM\ndata_for_heatmap &lt;- team_results |&gt;\n  inner_join(public_picks, by = \"TEAM\") |&gt;\n  # 4. Calculate the difference b/t team results and public picks.\n  mutate(TEAM_SEED = paste(TEAM, \" (\", SEED, \")\", sep = \"\"),\n         DIFF_R64 = R64WIN * 100 - R64.y,\n         DIFF_R32 = R32WIN * 100 - R32.y,\n         DIFF_S16 = S16WIN * 100 - S16.y,\n         DIFF_E8 = E8WIN * 100 - E8.y,\n         DIFF_F4 = F4WIN * 100 - F4.y,\n         DIFF_FINALS = F2WIN * 100 - FINALS)\n\n# 5. Pivot heatmap data longer so that rounds are in one column.\ndata_for_heatmap_long &lt;- data_for_heatmap |&gt;\n  select(TEAM_SEED, REGION, DIFF_R64, DIFF_R32, \n         DIFF_S16, DIFF_E8, DIFF_F4, DIFF_FINALS) |&gt;\n  pivot_longer(cols = starts_with(\"DIFF\"), \n               names_to = \"ROUND\", \n               values_to = \"DIFFERENCE\") |&gt;\n  mutate(ROUND = sub(\"DIFF_\", \"\", ROUND)) \n\n# 6. Rename rounds to extend names for labels.\ndata_for_heatmap_long$ROUND &lt;- \n  factor(\n    data_for_heatmap_long$ROUND,\n    levels = c(\"R64\", \"R32\", \"S16\", \n               \"E8\", \"F4\", \"FINALS\"),\n    labels = c(\"Round of 64\", \"Round of 32\", \n               \"Sweet 16\", \"Elite 8\", \n               \"Final Four\", \"Finals\")\n  )\n\n# 7. Re-assign factor levels for rounds column.\ndata_for_heatmap_long$ROUND &lt;- fct_relevel(\n  data_for_heatmap_long$ROUND, \n  \"Round of 64\", \"Round of 32\",\n  \"Sweet 16\", \"Elite 8\",\n  \"Final Four\", \"Finals\")\n\n\nClean\n\nCreate win columns for each round.\nConvert character percentage columns into doubles.\nJoin team_results and public_picks by TEAM.\nCalculate the difference between team results and public picks.\nPivot heatmap data longer so that rounds are in one column.\nRename rounds to extend names for labels.\nRe-assign factor levels for rounds column.\n\n\n\n\n\nShow Code\n#### Heatmap ####\n# Data: data_for_heatmap_long\n# Aesthetics: x = Round, y = Team (Seed), fill = difference\n# Graph Colors: low = blue, high = red, midpoint = 0\n# Labels: defined under Set-Up &gt; Text. \n# Add Values: If the difference is NA define as TBD, else\n# round the difference to 2 decimals and add a % sign. \n# Facet Wrap: by region. \n# Apply minimal theme. \n# Theme: title, subtitle, caption, axis titles, axis text,\n# background colors, and legend position. \n\n# heatmap\nheatmap &lt;- ggplot2::ggplot(\n  # Data\n  data_for_heatmap_long, \n  # Aesthetics\n  aes(x = ROUND, \n      y = TEAM_SEED, \n      fill = DIFFERENCE)\n  ) +\n  geom_tile() + \n  # Graph Colors\n  ggplot2::scale_fill_gradient2(\n    low = \"blue\", \n    high = \"red\", \n    midpoint = 0\n  ) +\n  # Labels\n  ggplot2::labs(\n    title = title_text,\n    subtitle = subtitle_text,\n    caption = caption_text,\n    x = xlab_text,\n    y = ylab_text,\n    fill = fill_text\n  ) +\n  # Percentage Point Difference Values\n  ggplot2::geom_text(\n    ggplot2::aes(\n      label = ifelse(\n        is.na(DIFFERENCE), \"TBD\",\n        paste0(round(DIFFERENCE, \n                     digits = 2),\"%\")\n        ),\n      hjust = 0\n    )\n  ) +\n  # Facet Wrap\n  facet_wrap(~ REGION, ncol = 1, scales = \"free_y\") +\n  # Apply minimal theme\n  ggplot2::theme_minimal() +\n  # Theme\n  ggplot2::theme(\n      # Title\n      plot.title = element_text(\n        size = 30,\n        family = \"font\",\n        face = \"bold\",\n        hjust = 0,\n        color = col1),\n      # Subtitle\n      plot.subtitle = element_markdown(\n        size = 16,\n        family = \"font\",\n        hjust = 0,\n        color = col1),\n      # Caption\n      plot.caption = element_text(\n        size = 12,\n        family = \"font\",\n        hjust = 1,\n        color = col1),\n      # Axis Titles\n      axis.title = element_text(\n        size = 24,\n        family = \"font\",\n        color = col1),\n      # X Axis Title\n      axis.title.x = element_text(vjust = -1.5),\n      # Axis Text\n      axis.text = element_text(\n        size = 12,\n        family = \"font\",\n        color = col1),\n      # Background colors\n      plot.background = element_rect(fill = col2),\n      # Adjust Axis text\n      axis.text.x = element_text(vjust = -1),\n      # Position Legend\n      legend.position = \"bottom\") \n\n\nGraph\n\nData: data_for_heatmap_long\nAesthetics: x = Round, y = Team (Seed), fill = difference\nGraph Colors: low = blue, high = red, midpoint = 0\nLabels: defined under Set-Up &gt; Text.\nAdd Values: If the difference is NA define as TBD, else round the difference to 2 decimals and add a % sign.\nFacet Wrap: by region.\nApply minimal theme.\nTheme: title, subtitle, caption, axis titles, axis text, background colors, and legend position.\n\n\n\n\n\nShow Code\nggsave(\n  \"plot.png\", \n  width = 35, \n  height = 45, \n  units = \"cm\",\n  dpi = 85)\n\n\nSave plot as plot.png!\n\n\n\nrfordatascience/tidytuesday Repo\nrbolt13/tidytuesday Repo\n\n\n\n\n\nQuick Notes\n\nI created a custom dataset for this year’s team results in a Google Sheet, which allowed me to include each team’s region and seed. This addition helps organize the data more effectively and gives readers additional context regarding each team’s perceived strength.\nFor incorporating blue and red text into the subtitle, I utilized the ggtext package. To apply this within the ggplot2 environment, I used the element_markdown() function in place of the element_text() function within the ggplot2::theme() settings.\nI employed scales = \"free_y\" in the facet_wrap() function to prevent y-axis labels from overlapping. This ensures that each facet has its own set of y-axis elements, maintaining clear separation and readability.\nData for the public picks in the championship round was not available, which could have been a compelling addition to this analysis."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html",
    "href": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html",
    "title": "NBA Salaries - Part 1: Web-Scrapping",
    "section": "",
    "text": "Scraping and cleaning NBA Salary data from ESPN - NBA Players Salaries, with some simple statistic and data visuals."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#change-salaray-from-character-to-numeric",
    "href": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#change-salaray-from-character-to-numeric",
    "title": "NBA Salaries - Part 1: Web-Scrapping",
    "section": "4.1 Change Salaray from Character to Numeric",
    "text": "4.1 Change Salaray from Character to Numeric\nNotice that the SALARY column is a character value. This will not be helpful when trying to do math, or make graphs with this numerical data. To change this 3 things must be addressed:\n\nRemoving the dollar sign.\nRemoving the commas.\nChange character type to numeric.\n\n\n\nShow code\nnba_salaries_2023$SALARY &lt;- str_remove_all(nba_salaries_2023$SALARY,\n                    \"\\\\$\")\nnba_salaries_2023$SALARY &lt;- str_remove_all(nba_salaries_2023$SALARY,\n                    \",\")\nnba_salaries_2023$SALARY &lt;- as.numeric(nba_salaries_2023$SALARY)\n\n\nNow we are able to do math, make graphs, and arrange the data by salary."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#basic-statistics",
    "href": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#basic-statistics",
    "title": "NBA Salaries - Part 1: Web-Scrapping",
    "section": "4.2 Basic Statistics",
    "text": "4.2 Basic Statistics\n\nHighest paid value : 51,915,615\nLowest paid value : 289,542\nMedian : 5e+06\nMean : 9,925,143\nStandard Deviation : 11,295,000"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#box-plots",
    "href": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#box-plots",
    "title": "NBA Salaries - Part 1: Web-Scrapping",
    "section": "4.3 Box Plots",
    "text": "4.3 Box Plots\n\n4.3.1 2022 - 2023 Yearly Salary by Postion\n\n\nShow code\nggplot2::ggplot(data = nba_salaries_2023,\n                mapping = ggplot2::aes(x = SALARY,\n                                       y = POSITION)) + \n  ggplot2::geom_boxplot()\n\n\n\n\n\n\n\n\n\nFrom this visual we can see that Point Guards (PG) appear to be paid the most, while Guards (G) and Forwards (F) on average are paid the least.\n\n\n4.4 2022-2023 Yearly Salary by Team\n\n\nShow code\nggplot2::ggplot(data = nba_salaries_2023,\n                mapping = ggplot2::aes(x = SALARY,\n                                       y = TEAM)) + \n  ggplot2::geom_boxplot()\n\n\n\n\n\n\n\n\n\nThis plot is not the easiest to read, and might be worth sub-setting the information further. However eye-balling this visual we can see most teams pay between $2,000,000 and $15,000,000 per player with a few outliers. These outliers of course being superstar players."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#footnotes",
    "href": "blog/Technical-Blog/posts/20221219-Original-NBASalaries/index.html#footnotes",
    "title": "NBA Salaries - Part 1: Web-Scrapping",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nForbes - The Worlds 10 Highest-Paid Athletes of 2022↩︎"
  },
  {
    "objectID": "projects/Educational-Insights.html",
    "href": "projects/Educational-Insights.html",
    "title": "Educational Insights",
    "section": "",
    "text": "Notes from Ge Shao’s Applied Regression Analysis Fall 2021 class. Topics include simple linear regression, least-square estimates, hypothesis testing, t-stat, p-stat, sum of squares estimates, mean square error, confidence interval, R squared, multiple linear models, hat matrix, multilinearity, model adequacy checking, residual analysis, PRESS statistic, detecting outliers, correcting model inadequacies, stabilizing variance, DIFFTS, DFBETAS, COVRATIO,… Continue reading.\n\n\n\nNotes from Nadee Jayasena’s Introduction ot Mathematical Statistics II Winter 2022 class. Topics include marginal PDF’s, joint PDF’s and CDF’s, independence of random variables, bivariate probability, conditional distribution, conditional covariance, correlation, univariate transformations, bivariate transformations, Jacobians, method of moment-generating functions, sampling distributions, central limit theorem, … Continue reading\n\n\n\nNotes from Dorcas Ofori-Boaten’s Introduction to Mathematical Statistics I Fall 2021 class. Topics include mean, median, mode, variance, standard deviation, Empirical Rule, set theory, DeMorgans Law, Distributive Law, counting rules, statistical independence, mutual independence, total law of probability, Bayes Rule, discrete and continuous random variables, probability distribution, expected value, CDF, PMF, PDF, … Continue reading\n\n\n\nNotes from Subash Kochar’s Applied Statistics for Engineers and Scientists Summer 2021 class. Topics include discrete and continous random various, bernoulli random variables, pmf, pdf, expeced value, variance, mean, standard deviation, probablity distributions: Poisson, binomial, uniform, normal, exponential, gamma, chi-squared, Weibull, lognormal, …Continue reading"
  },
  {
    "objectID": "projects/Educational-Insights.html#stat-464-applied-regression-analysis",
    "href": "projects/Educational-Insights.html#stat-464-applied-regression-analysis",
    "title": "Educational Insights",
    "section": "",
    "text": "Notes from Ge Shao’s Applied Regression Analysis Fall 2021 class. Topics include simple linear regression, least-square estimates, hypothesis testing, t-stat, p-stat, sum of squares estimates, mean square error, confidence interval, R squared, multiple linear models, hat matrix, multilinearity, model adequacy checking, residual analysis, PRESS statistic, detecting outliers, correcting model inadequacies, stabilizing variance, DIFFTS, DFBETAS, COVRATIO,… Continue reading."
  },
  {
    "objectID": "projects/Educational-Insights.html#stat-462-introduction-to-mathematical-statistics-ii",
    "href": "projects/Educational-Insights.html#stat-462-introduction-to-mathematical-statistics-ii",
    "title": "Educational Insights",
    "section": "",
    "text": "Notes from Nadee Jayasena’s Introduction ot Mathematical Statistics II Winter 2022 class. Topics include marginal PDF’s, joint PDF’s and CDF’s, independence of random variables, bivariate probability, conditional distribution, conditional covariance, correlation, univariate transformations, bivariate transformations, Jacobians, method of moment-generating functions, sampling distributions, central limit theorem, … Continue reading"
  },
  {
    "objectID": "projects/Educational-Insights.html#stat-461-introduction-to-mathematical-statistics-i",
    "href": "projects/Educational-Insights.html#stat-461-introduction-to-mathematical-statistics-i",
    "title": "Educational Insights",
    "section": "",
    "text": "Notes from Dorcas Ofori-Boaten’s Introduction to Mathematical Statistics I Fall 2021 class. Topics include mean, median, mode, variance, standard deviation, Empirical Rule, set theory, DeMorgans Law, Distributive Law, counting rules, statistical independence, mutual independence, total law of probability, Bayes Rule, discrete and continuous random variables, probability distribution, expected value, CDF, PMF, PDF, … Continue reading"
  },
  {
    "objectID": "projects/Educational-Insights.html#stat-451-applied-statistics-for-engineers-and-scientists",
    "href": "projects/Educational-Insights.html#stat-451-applied-statistics-for-engineers-and-scientists",
    "title": "Educational Insights",
    "section": "",
    "text": "Notes from Subash Kochar’s Applied Statistics for Engineers and Scientists Summer 2021 class. Topics include discrete and continous random various, bernoulli random variables, pmf, pdf, expeced value, variance, mean, standard deviation, probablity distributions: Poisson, binomial, uniform, normal, exponential, gamma, chi-squared, Weibull, lognormal, …Continue reading"
  },
  {
    "objectID": "projects/Educational-Insights.html#mth-388-modern-college-geometry",
    "href": "projects/Educational-Insights.html#mth-388-modern-college-geometry",
    "title": "Educational Insights",
    "section": "MTH 388: Modern College Geometry",
    "text": "MTH 388: Modern College Geometry\nNotes from Rebecca Tramel’s Modern College Geometry Winter 2022 class. Topics include axioms, proof writing, Euclid’s Elements, congruence, isometries, triangles, area proofs, circles, group theory for symmetries, Euclidean distance, taxi-cab geometry, spherical geometry, hyperbolic geometry, … Continue reading"
  },
  {
    "objectID": "projects/Educational-Insights.html#mth-344-introduction-to-group-theory",
    "href": "projects/Educational-Insights.html#mth-344-introduction-to-group-theory",
    "title": "Educational Insights",
    "section": "MTH 344: Introduction to Group Theory",
    "text": "MTH 344: Introduction to Group Theory\nNotes from Julie Bracken’s Introduction to Group Theory Winter 2022 class. Topics include set theory, cancellation law, subgroups, cyclic subgroups, injecteive, surjective, bijective, function composition, permutation groups, symmetric groups, dihedral group, cycle decomposition, parity of transposistions, alternating groups, isomorphism, partitions, Lagrange’s Theorem, homomorphism, normal subgroups, kernal, … Continue reading"
  },
  {
    "objectID": "projects/Technical-Blogs.html",
    "href": "projects/Technical-Blogs.html",
    "title": "Archived Blogs",
    "section": "",
    "text": "Current iteration of my blog. Made using Quarto and deployed with Netlify. Technical posts include a lot of data analysis and visualization in R, and tutorials."
  },
  {
    "objectID": "projects/Technical-Blogs.html#rbolt.me-2024",
    "href": "projects/Technical-Blogs.html#rbolt.me-2024",
    "title": "Archived Blogs",
    "section": "",
    "text": "Current iteration of my blog. Made using Quarto and deployed with Netlify. Technical posts include a lot of data analysis and visualization in R, and tutorials."
  },
  {
    "objectID": "projects/Technical-Blogs.html#rbolt22-2023",
    "href": "projects/Technical-Blogs.html#rbolt22-2023",
    "title": "Archived Blogs",
    "section": "rbolt22 (2023)",
    "text": "rbolt22 (2023)\nMade using Quarto and deployed with Netlify. Topics include collaborating with Posit Cloud and Google Drive, using R, SQL, and Python, creating a package in R, web scraping, data visualization, machine learning, statistics and probability, and more."
  },
  {
    "objectID": "projects/Technical-Blogs.html#rbolt2-2022",
    "href": "projects/Technical-Blogs.html#rbolt2-2022",
    "title": "Archived Blogs",
    "section": "rbolt2 (2022)",
    "text": "rbolt2 (2022)\nMade with R using the blogdown package and deployed with Netlify. Topics include reproducible research for data science, docker for reproducible research, project organization for data science, Bayes statistics, predicting stock prices with R packages, statistics and mathematics notes, and more."
  },
  {
    "objectID": "projects/Technical-Blogs.html#rbolt-2021",
    "href": "projects/Technical-Blogs.html#rbolt-2021",
    "title": "Archived Blogs",
    "section": "rbolt (2021)",
    "text": "rbolt (2021)\nMade with R using the blogdown package and deployed with Netlify. Topics include using API’s, analyzing and modifying linear regression models, and more."
  },
  {
    "objectID": "projects/Data-Visuals-and-Analysis.html",
    "href": "projects/Data-Visuals-and-Analysis.html",
    "title": "Data Visuals and Analysis",
    "section": "",
    "text": "TidyTuesday is a weekly data analysis and visualization challenge that provides a structured format to practice data wrangling and visualization skills using R programming language. Each week, a new dataset is posted to the TidyTuesday Github repository and participants are encouraged to explore and visualize the data using the principles of tidy data. Continue reading …"
  },
  {
    "objectID": "projects/Data-Visuals-and-Analysis.html#tidy-tuesday",
    "href": "projects/Data-Visuals-and-Analysis.html#tidy-tuesday",
    "title": "Data Visuals and Analysis",
    "section": "",
    "text": "TidyTuesday is a weekly data analysis and visualization challenge that provides a structured format to practice data wrangling and visualization skills using R programming language. Each week, a new dataset is posted to the TidyTuesday Github repository and participants are encouraged to explore and visualize the data using the principles of tidy data. Continue reading …"
  },
  {
    "objectID": "projects/Data-Visuals-and-Analysis.html#rsite",
    "href": "projects/Data-Visuals-and-Analysis.html#rsite",
    "title": "Data Visuals and Analysis",
    "section": "#Rsite",
    "text": "#Rsite\nFour different projects from Statistics 363: Introduction to R, which includes analysis of NYC Flight Data, investigating if Kobe Bryant had “hot hands”, comparing Gas and Hydro power using base R and ggplot2, and exploring states with the highest and lowest numbers of Covid cases. Continue reading …"
  },
  {
    "objectID": "projects/Data-Visuals-and-Analysis.html#oregon-grown",
    "href": "projects/Data-Visuals-and-Analysis.html#oregon-grown",
    "title": "Data Visuals and Analysis",
    "section": "Oregon Grown",
    "text": "Oregon Grown\nA state known for growing all varieties of trees and greenery, it’s no wonder that Oregon was one of the first states to legalize recreational cannabis in 2014. Measure 91 changed the lives of both those affiliated with cannabis and those who benefit from the tax revenue it brings in. While many businesses and industries struggled in 2020 due to COVID-19 the cannabis industry kept growing. Continue reading …"
  },
  {
    "objectID": "projects/Talks.html",
    "href": "projects/Talks.html",
    "title": "Talks",
    "section": "",
    "text": "Code to Content (2024)\n\nUsed 4 years of hands-on experience creating technical blogs to compile a short 6 minute presentation to guide and encourage R users to create and share their expertise and journeys online.\nCreated accessible slides with Quarto, and hosted on Github.\nPresented as a lightning talk at PDX R User Group, February 2024.\nAccepted as a presenter for a lightning talk at Cascadia R Conference, June 2024.\n\n\n\nOregon Grown (2021)\n\nUsed spatial references from Oregon Spatial Library, ARLIS, and PCC Geography Department to apply mapping techniques such as select by attribute, spatial join, clip, and XY Table to Point to investigate dispensary diversity in Oregon November 2020.\nFormatted, organized, added classifications, and geocoded a data table of approximately 700 rows of data from the Oregon Liquor Control Commission’s active marijuanna retail license.\nDiscovered more than half of dispensaries in Oregon are owned by chains, and about 10% of dispensaries in Portland’s Urban Growth Boundary are owned by one chain, Groundworks.\nCreated an abstract that was accepted for GIS in Action 2021, and presented virtually April 2021."
  },
  {
    "objectID": "projects/Dashboards.html",
    "href": "projects/Dashboards.html",
    "title": "Dashboards",
    "section": "",
    "text": "2024 Oscar Prediction Challenge Dashboard is an interactive shiny app that presents the results of a survey where participants predicted the winners of various Oscars categories."
  },
  {
    "objectID": "projects/Dashboards.html#oscar-prediction-challenge",
    "href": "projects/Dashboards.html#oscar-prediction-challenge",
    "title": "Dashboards",
    "section": "",
    "text": "2024 Oscar Prediction Challenge Dashboard is an interactive shiny app that presents the results of a survey where participants predicted the winners of various Oscars categories."
  },
  {
    "objectID": "projects/Dashboards.html#covid-19",
    "href": "projects/Dashboards.html#covid-19",
    "title": "Dashboards",
    "section": "Covid-19",
    "text": "Covid-19\nCovid-19 Dashboard is an interactive analysis tool that specifically examines Covid-19 cases and fatalities in both the United States and my home state of Oregon. What initially began as a class project at the onset of the pandemic, has undergone multiple iterations and refinements to evolve into a fully functional and informative dashboard."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome 🌹",
    "section": "",
    "text": "I am a statistical data analyst from the city of roses who leverages data science techniques to guide strategic decision-making. I have a relentless passion for exploring new topics and continuously improving upon my past and current work.\nBelow are the focal points of my virtual rosarium:\n\nCentral Display: Technical Blog\nFeature Roses: Book Reviews & Personal Blog\nShowpiece Structures (Projects): Dashboards , Data Visuals & Analysis , Educational Insigts , Talks and Archive Blogs\nThematic Area (Links): Email , Github, Linkedin , Letterbox, and Spotify\nEnjoy!"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210920-Original-TidycensusAPI/index.html",
    "href": "blog/Technical-Blog/posts/20210920-Original-TidycensusAPI/index.html",
    "title": "API’s and tidycensus",
    "section": "",
    "text": "Learn how to use an API with the tidycensus R package to access U.S. Census data. I’ll cover API setup basics and practical examples to help with your analytical projects.\n\n\n1. Set-up\nTo start Request a Key to get an API key.\nThen create an .Renviron file to your projects main directory with “CENSUS_API_KEY=XXXXXXXXXXX”, where all the X’s represent you key.\nNote:\n\nThis key will not work with spaces on either side of the equal sign.\ntidycensus already has this utility worked into it (read ?census_api_key). They call their api key CENSUS_API_KEY (it is common for this key to be in all caps), so that is what I also called mine. This will be especially helpful in not mixing up API keys if I use other API keys in the future.\n\nNow load the tidycensus package and use readRenviron() to access the API key.\n\n\nShow code\nbase::library(tidycensus)\nbase::readRenviron(\"../../../../.Renviron\")\n\n\nNote:\n\nThe first time you access your API key you may want to reload your environment so you don’t have to restart R.\n../ tells your machine to go one folder outside the folder it is in.\nUse Sys.getenv(\"CENSUS_API_KEY\") to check your key is accesible and correct.\n\n\n\n2. Using tidycensus\nUse load_variables(year, dataset, chache=T/F) for various data sets. Read ?load_variables() for more information.\nNote:\n\nlabel shows the estimates by total, and then sex and age range.\nconcept is by sex, then race, origins, and ancestry.\n\n\n\nShow code\na &lt;- tidycensus::load_variables(2019, \"acs1\")\nutils::head(a, 10)\n\n\n# A tibble: 10 × 3\n   name        label                                    concept                 \n   &lt;chr&gt;       &lt;chr&gt;                                    &lt;chr&gt;                   \n 1 B01001A_001 Estimate!!Total:                         SEX BY AGE (WHITE ALONE)\n 2 B01001A_002 Estimate!!Total:!!Male:                  SEX BY AGE (WHITE ALONE)\n 3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years   SEX BY AGE (WHITE ALONE)\n 4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years    SEX BY AGE (WHITE ALONE)\n 5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years  SEX BY AGE (WHITE ALONE)\n 6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years  SEX BY AGE (WHITE ALONE)\n 7 B01001A_007 Estimate!!Total:!!Male:!!18 and 19 years SEX BY AGE (WHITE ALONE)\n 8 B01001A_008 Estimate!!Total:!!Male:!!20 to 24 years  SEX BY AGE (WHITE ALONE)\n 9 B01001A_009 Estimate!!Total:!!Male:!!25 to 29 years  SEX BY AGE (WHITE ALONE)\n10 B01001A_010 Estimate!!Total:!!Male:!!30 to 34 years  SEX BY AGE (WHITE ALONE)\n\n\nLet’s only focus on the first line for now, “B01001_001” which should be the total estimates. Then we can use get_acs() to get data population data by state from the American Community Survey.\n\n\nShow code\nb &lt;- tidycensus::get_acs(geography = \"state\", year = 2019, variable = \"B01001_001\")\nutils::head(b, 10)\n\n\n# A tibble: 10 × 5\n   GEOID NAME                 variable   estimate   moe\n   &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n 1 01    Alabama              B01001_001  4876250    NA\n 2 02    Alaska               B01001_001   737068    NA\n 3 04    Arizona              B01001_001  7050299    NA\n 4 05    Arkansas             B01001_001  2999370    NA\n 5 06    California           B01001_001 39283497    NA\n 6 08    Colorado             B01001_001  5610349    NA\n 7 09    Connecticut          B01001_001  3575074    NA\n 8 10    Delaware             B01001_001   957248    NA\n 9 11    District of Columbia B01001_001   692683    NA\n10 12    Florida              B01001_001 20901636    NA\n\n\nWe can get similar population estimates setting the variable = c(“POP), with get_estimates. As well as”DENSITY”; for housing unit estimates, c(“HUEST”); and for components of change estimates, c(“BIRTHS”, “DEATHS”, “DOMESTICMIG”, “INTERNATIONALMIG”, “NATURALINC”, “NETMIG”, “RBIRTH”, “RDEATH”, “RDOMESTICMIG”, “RINTERNATIONALMIG”, “RNATURALINC”, “RNETMIG”).\n\n\nShow code\nc &lt;- tidycensus::get_estimates(geography = \"state\", year = 2019, variable = c(\"POP\"))\nutils::head(c, 10)\n\n\n# A tibble: 10 × 4\n   NAME                 GEOID variable    value\n   &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 Alabama              01    POP       4903185\n 2 Alaska               02    POP        731545\n 3 Arizona              04    POP       7278717\n 4 Arkansas             05    POP       3017804\n 5 California           06    POP      39512223\n 6 Colorado             08    POP       5758736\n 7 Delaware             10    POP        973764\n 8 District of Columbia 11    POP        705749\n 9 Connecticut          09    POP       3565287\n10 Florida              12    POP      21477737\n\n\n\n\nShow code\nd &lt;- tidycensus::get_estimates(geography = \"county\", state = \"OR\", year = 2019, variable = c(\"POP\"))\nutils::head(d, 10)\n\n\n# A tibble: 10 × 4\n   NAME                      GEOID variable  value\n   &lt;chr&gt;                     &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 Lane County, Oregon       41039 POP      382067\n 2 Washington County, Oregon 41067 POP      601592\n 3 Clatsop County, Oregon    41007 POP       40224\n 4 Jackson County, Oregon    41029 POP      220944\n 5 Grant County, Oregon      41023 POP        7199\n 6 Clackamas County, Oregon  41005 POP      418187\n 7 Tillamook County, Oregon  41057 POP       27036\n 8 Josephine County, Oregon  41033 POP       87487\n 9 Umatilla County, Oregon   41059 POP       77950\n10 Columbia County, Oregon   41009 POP       52354"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230102-Original-SportsObserveR/index.html",
    "href": "blog/Technical-Blog/posts/20230102-Original-SportsObserveR/index.html",
    "title": "SportsObserveR - Part 2: Creating a Package in R",
    "section": "",
    "text": "In this tutorial I create my own package, SportsObserveR, with functions from my previous post, SportsObserveR - Part 1: Scraping Functions."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230102-Original-SportsObserveR/index.html#technologies",
    "href": "blog/Technical-Blog/posts/20230102-Original-SportsObserveR/index.html#technologies",
    "title": "SportsObserveR - Part 2: Creating a Package in R",
    "section": "0.1 Technologies",
    "text": "0.1 Technologies\nTo build a package in R you need three things:\n\nR installed on your computer.\nA coding editor such as R Studio, or Sublime.\nA bash terminal."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230102-Original-SportsObserveR/index.html#creating-the-function",
    "href": "blog/Technical-Blog/posts/20230102-Original-SportsObserveR/index.html#creating-the-function",
    "title": "SportsObserveR - Part 2: Creating a Package in R",
    "section": "8.1 Creating the Function",
    "text": "8.1 Creating the Function\nFrom the R Studio Console, create the function file, and load it into your enviroment.\n\n\nShow code\nuse_r(\"scrape_nba_player_stats\")\n\n\n\n\nShow code\nload_all()\n\n\nIn the R file named “scrape_nba_player_stats.R” copy the code below:\n\n\nShow code\n#' Scrapes NBA player stats tables off basketball-reference.com.\n#'\n#'@import rvest \n#'@import magrittr\n#'\n#'@param name is a char string that corresponds to the players name.\n#'@param stats_tb is a char string that corresponds to the statistics table such as #per_game, #totals, #per_36_minutes, and #advanced.\n#'\n#'@return a data frame of statistics for a specific NBA player. \n#'@export\n#'\n#'@examples\n#'scrape_nba_player_stats(\"Allen Iverson\", \"#per_game\")\nscrape_nba_player_stats &lt;- function(name, stats_tb){\n  # make name lower case\n  lower_case_name &lt;- base::tolower(name)\n\n  # split name \n  split_name &lt;- base::strsplit(lower_case_name, \" +\")[[1]]\n\n  # define first and last name\n  first_name &lt;- split_name[[1]]\n  last_name &lt;- split_name[[2]]\n  \n  # first letter of last name\n  letter &lt;- base::substr(last_name, 1,1)\n  \n  # first five letters of last name \n  last_5 &lt;- base::substr(last_name, 1, 5)\n  \n  # first two letters of first name\n  first_2 &lt;- base::substr(first_name, 1,2)\n  \n  # define team page URL\n  url &lt;- base::paste0(\"https://www.basketball-reference.com/players/\",letter ,\"/\",last_5,first_2,\"01.html\")\n  \n  # Read stats table\n  stats_tb &lt;- url %&gt;%\n  read_html %&gt;%\n  html_node(stats_tb) %&gt;% \n  html_table()\n  \n  # Rename Column 2 to Name \n  names(stats_tb)[2] &lt;- \"Name\"\n  \n  # Replace NA values with 0 (for stat functions)\n  stats_tb[base::is.na(stats_tb)] &lt;- 0\n  \n  # make list a dataframe\n  df &lt;- base::data.frame(stats_tb)\n  \n  base::return(df)\n  }\n\n\nNow save this file, and in the R Studio Console type document() to create documentation for this function.\n\n\nShow code\ndocument()\n\n\nCheck that the documentation works:\n\n\nShow code\n?scrape_nba_player_stats()\n\n\nUse the check() function to look at any potential errors.\n\n\nShow code\ncheck()\n\n\nIf everything looks good, from the Bash Terminal push this code to github.\n\n\nShow code\ngit status\n\n\n\n\nShow code\ngit add --all\n\n\n\n\nShow code\ngit status\n\n\n\n\nShow code\ngit commit -m\"Created second function\"\n\n\n\n\nShow code\ngit push origin main"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230102-Original-SportsObserveR/index.html#creating-tests-1",
    "href": "blog/Technical-Blog/posts/20230102-Original-SportsObserveR/index.html#creating-tests-1",
    "title": "SportsObserveR - Part 2: Creating a Package in R",
    "section": "8.2 Creating Tests",
    "text": "8.2 Creating Tests\nFrom the R Studio Console use use_test() again to create a test file for scrape_nba_player_stats().\n\n\nShow code\nuse_test(\"scrape_nba_player_stats\")\n\n\nNext create a similar code to what was previously done for scrape_nba_team_stats().\n\n\nShow code\ntest_that(\"Returns that typeof is list.\", {\n  expect_equal(typeof(scrape_nba_player_stats(\"Kareem Abdul-Jabbar\", \"#totals\")),\n               \"list\")\n})\n\n\nCheck for errors.\n\n\nShow code\ncheck()\n\n\nIf everything looks green, from the Bash Terminal push to Github.\n\n\nShow code\ngit status\n\n\n\n\nShow code\ngit add --all\n\n\n\n\nShow code\ngit status\n\n\n\n\nShow code\ngit commit -m\"Created test for scrape_nba_player_stats\"\n\n\n\n\nShow code\ngit push origin main"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230102-Original-SportsObserveR/index.html#update-readme",
    "href": "blog/Technical-Blog/posts/20230102-Original-SportsObserveR/index.html#update-readme",
    "title": "SportsObserveR - Part 2: Creating a Package in R",
    "section": "8.3 Update README",
    "text": "8.3 Update README\nAdd another example to the readme.rmd file using the scrape_nba_player_stats() function, and then update the .md file.\n\n\nShow code\nbuild_readme()\n\n\nFrom the Bash Terminal push this update to Github.\n\n\nShow code\ngit status\n\n\n\n\nShow code\ngit add --all\n\n\n\n\nShow code\ngit status\n\n\n\n\nShow code\ngit commit -m\"added scrape_nba_player_stats example to readme\"\n\n\n\n\nShow code\ngit push origin main"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221212-Original-DataScienceInterviewQuestions/index.html",
    "href": "blog/Technical-Blog/posts/20221212-Original-DataScienceInterviewQuestions/index.html",
    "title": "Data Science Interview Questions - Part 1",
    "section": "",
    "text": "This post covers 12 questions that might be asked in a technical interview for a data science position.\n\n\n1. What is the difference between supervised and unsupervised learning?\nSupervised Learning: Uses labeled data for prediction. (Logistical Regression, Linear Regression, Decision Tree)\nUnsupervised Learning: Uses unlabeled data for analysis such as identifying hidden patterns in clustering, association, and anomalies or errors. (K-means clustering, hierarchical clustering)\n\n\n2. What is the difference between univariate, bivariate, and multivariate analysis?\nUnivariate: Looks at only one variable at a time.\nBivariate: Compares two variables.\nMultivariate: Compares more than two variables.\n\n\n3. What is the difference between wide format data and long format data?\nWide Format Data: Has a column for each variable.\nLong Format Data: Has a column for possible variable types, and a value for each of those variables.\n\n\n4. What is the difference between normalization and stadardization?\nNormalization: Rescales the values into a range of [0,1].\nStandardization: Rescales data to have a mean of 0 and a standard deviation of 1.\n\n\n5. What is variance?\nVariance: \\(s^2=\\frac{\\sum(x_i-\\bar{x})}{n-1}\\) is a measure of spread within the data.\n\n\n6. What is a normal distribution?\nNormal distribution: (Gaussian distribution or bell curve) is a probability distribution that is symmetrical about the mean.\n\n\n7. What is the law of large numbers?\nLaw of Large Numbers: if an experiment is repeated independently a large number of times then the average results of the obtained results should be close to the expected value.\n\n\n8. What is the goal of A/B testing?\nTo determine which experiment A or B preformed better.\n\n\n9. You are given a dataset consisting of variables with more than 30 percent missing values. How do you deal with them?\n\nDelete rows or columns with missing values. This can be problematic if it means loosing valuable data.\nFill in the missing values with an approximation of the average of the other values in the column.\n\n\n\n10. For the given points how do you calculate the Euclidean distance in Python?\npoint 1 = (2,3) and point 2 = (1,1)\n\nimport numpy as np\n \n# initializing points in numpy arrays\npoint1 = np.array((1, 2, 3))\npoint2 = np.array((1, 1, 1))\n \n# calculating Euclidean distance using linalg.norm()\ndist = np.linalg.norm(point1 - point2)\n \n# printing Euclidean distance\nprint(dist)\n\n2.23606797749979\n\n\nNote: Check out the distance package I created to solve this same problem using R.\n\n\n11. How do you find RMSE and MSE in a linear regression model?\nRoot Mean Square Error (RMSE): \\(\\text{RMSE}=\\sqrt{\\text{MSE}}=\\sqrt\\frac{\\sum{(y_i-\\hat{y_i})^2}}{n}\\)\nMean Square Error (MSE): \\(\\text{MSE}=\\frac{\\sum{(\\text{observed}-\\text{predicted})^2}}{n}=\\frac{\\sum{(y_i-\\hat{y_i})^2}}{n}\\)\nNote: The values are squared is to prevent negative values, and to increase the impact of of larger errors.\n\n\n12. What is the significance of p-value?\nThe p value is the probability that the null hypothesis is true. Meaning there is no statistical significance that exists between variables.\nWhen p is \\(\\leq\\) 0.05 then we can reject the null hypothesis.\nWhen p is &gt; 0.05 then we fail to reject the null hypothesis aka accept it."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230221-TidyTuesday-BobRoss/index.html",
    "href": "blog/Technical-Blog/posts/20230221-TidyTuesday-BobRoss/index.html",
    "title": "Week 8 Tidy Tuesday: Bob Ross Paintings",
    "section": "",
    "text": "Step into the world of happy little data with my Week 8 #TidyTuesday submission which reveals the number of colors Bob Ross used each season of “The Joy of Painting”.\n\nBob Ross Paintings\nThe data this week comes from Jared Wilber’s data on Bob Ross Paintings via @frankiethull Bob Ross Colors data package.\n\n\n\nCode\nThis week I wanted to keep the visual fairly simple so that I could focus my efforts on designing the project in a way that felt intuitive, was easy to read, and had a manageable workflow. To achieve these objectives I broke the project into three key functions: cleaning, visualizing, and styling - organized within the ‘Functions’ folder for that weeks submission. The index.R file integrates data loading, function execution, and the generation of of a data visualization.\n\nCleaning FunctionVisual FunctionStyle FunctionIndex.RLinks\n\n\n\n\nShow Code\n#### Load Packages ####\n# dplyr: data cleaning functions.\nbase::library(dplyr)\n#### Cleaning Function ####\nclean &lt;- function(df){\n  # clean function\n  clean_df &lt;- df %&gt;%\n    dplyr::group_by(season) %&gt;%\n    dplyr::summarise(\n      total_num_colors = base::sum(num_colors))\n  return(clean_df)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# ggplot2: graphing functions. \nbase::library(ggplot2)\n#### Visual Function ####\nvis &lt;- function(clean_df){\n  vis &lt;- ggplot2::ggplot(\n    clean_df,\n    ggplot2::aes(\n      x = season,\n      y = total_num_colors)) +\n    ggplot2::geom_point() + \n    ggplot2::geom_segment(\n      ggplot2::aes(\n        x = season,\n        xend = season,\n        y = 0,\n        yend = total_num_colors)) + \n    ggplot2::geom_hline(\n      yintercept=137.871, \n      linetype=\"dashed\", \n      color = \"red\") +\n    ggplot2::geom_text(\n      ggplot2::aes(label = total_num_colors,\n                   vjust = -1)\n    )\n  return(vis)\n}\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# ggplot2: graphing functions. \nbase::library(ggplot2)\n#### Style Function ####\nsty &lt;- function(vis){\n  sty &lt;- vis +\n    # labels \n    ggplot2::labs(\n      title = \"Colors Used Each Season\",\n      subtitle = \"The dashed red line shows the average number of colors used each season, 137.871.\",\n      caption = \"Graph by Randi Bolt \\n Data from #TidyTuesday\",\n      x = \"Season\",\n      y = \"Colors Used\"\n    ) + \n    # themes\n    ggplot2::theme_classic() + \n    ggplot2::theme(\n      plot.title = element_text(\n        face = \"bold\",\n        hjust = .5),\n      plot.subtitle = element_text(\n        hjust = .5)\n    ) \n  return(sty)\n  }\n\n\n\n\n\n\nShow Code\n#### Load Packages ####\n# tidyverse: A collection of data-related packages.\nbase::library(tidyverse)\n\n#### Load Data ####\n# bob_ross: data about and from \"The Joy of Painting\".\ntt_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-21/bob_ross.csv')\n\n#### Clean Data ####\nclean_data &lt;- clean(tt_data)\n\n#### Create Data Visual ####\nvis_data &lt;- vis(clean_data)\n\n#### Style Data Visual ####\nsty_vis &lt;- sty(vis_data)\n\n#### Save Plot ####\nggplot2::ggsave(\n  base::paste0(\"plot.png\"), \n  width = 30, \n  height = 20, \n  units = \"cm\")\n\n\n\n\nWeek 8 Submission\nTidyTuesday Repository.\n\n\n\n\n\nQuick Notes\n\nTo make a lollipop graph using ggplot2 you need to assign geom_point() and geom_segment().\ngoem_hline() was used to created the dashed horizontal red line.\ngeom_text() is used to add the values above the lollipops.\nUsing the TidyTuesdayR package can be problematic.\nIt looks nicer to keep all labels in the labs() function."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220131-Original-LearnGeom/index.html",
    "href": "blog/Technical-Blog/posts/20220131-Original-LearnGeom/index.html",
    "title": "LearnGeom",
    "section": "",
    "text": "In this post I create some polygons and line segments with LearnGeom.\n\n\n1. Set Up\nTo create coordinate planes, trianges, and line segments I will be using the LearnGeom package.\n\n\nShow code\nlibrary(LearnGeom)\n\n\n\n\n2. Coordinate Plane\nTo create a coordinate plane I will first need to define x and y minimums and maximums, and then plot the planes with the CoordinatePlane() function.\n\n\nShow code\nx_min &lt;- 0\nx_max &lt;- 10\ny_min &lt;- 0\ny_max &lt;- 10\nLearnGeom::CoordinatePlane(x_min, x_max, y_min, y_max)\n\n\n\n\n\n\n\n\n\nNULL\n\n\n\n\n3. Polygons\nTo create a triangle with labels:\n\nPrint the coordinate plane I just created.\nDefine three points of a triangle.\nUse CreatePolygon() function to create the polygon.\nUse the Draw() function to draw the polygon.\nDefine label = TRUE to show the points of a triangle.\n\n\n\nShow code\nLearnGeom::CoordinatePlane(x_min, x_max, y_min, y_max)\n\n\nNULL\n\n\nShow code\nP1 &lt;- c(1,4)\nP2 &lt;- c(3,7)\nP3 &lt;- c(4, 1)\nPoly &lt;- LearnGeom::CreatePolygon(P1, P2, P3)\n\n\n[1] \"Some of the inserted points are collinear. This could lead to a defective polygon.\"\n\n\nShow code\nLearnGeom::Draw(Poly, c(\"pink\"), label = TRUE)\n\n\n\n\n\nTriangle\n\n\n\n\nNULL\n\n\nTo create a trapezoid:\n\nPrint the coordinate plane I just created.\nDefine four points of a trapezoid.\nUse CreatePolygon() function to create the polygon.\n\nNote: The order of points will matter.\n\nUse the Draw() function to draw the polygon.\n\n\n\nShow code\nLearnGeom::CoordinatePlane(x_min, x_max, y_min, y_max)\n\n\nNULL\n\n\nShow code\nP4 &lt;- c(6, 3)\nP5 &lt;- c(8, 3)\nP6 &lt;- c(9, 8)\nP7 &lt;- c(7, 8)\nPoly2 &lt;- LearnGeom::CreatePolygon(P4, P5, P6, P7)\n\n\n[1] \"Some of the inserted points are collinear. This could lead to a defective polygon.\"\n\n\nShow code\nLearnGeom::Draw(Poly2, c(\"light blue\"))\n\n\n\n\n\nTrapezoid\n\n\n\n\nNULL\n\n\nWe can also print both polygons on the same graph, shown below.\n\n\nShow code\nLearnGeom::CoordinatePlane(x_min, x_max, y_min, y_max)\n\n\nNULL\n\n\nShow code\nLearnGeom::Draw(Poly, c(\"pink\"), label = TRUE)\n\n\nNULL\n\n\nShow code\nLearnGeom::Draw(Poly2, c(\"light blue\"))\n\n\n\n\n\nTriangle and Trapezoid\n\n\n\n\nNULL\n\n\n\n\n4. Angle and Point Line Segments\nTo create a Segment Angle:\n\nPrint the coordinate plane I just created.\nDefine a points where the line originates from.\nDefine the angle of the line.\nDefine the length of the line.\nUse CreateSegmentAngle() function to create the line segment.\nUse the Draw() function to draw the line.\n\n\n\nShow code\nLearnGeom::CoordinatePlane(x_min, x_max, y_min, y_max)\n\n\nNULL\n\n\nShow code\nP &lt;- c(0,0)\nangle &lt;- 30\nlen &lt;- 10\nSegment &lt;- LearnGeom::CreateSegmentAngle(P, angle, len)\nLearnGeom::Draw(Segment, \"blue\")\n\n\n\n\n\n\n\n\n\nNULL\n\n\nSegment Point\nTo create a Segment (with) Point(s):\n\nPrint the coordinate plane.\nDefine two endpoint.\nUse CreateSegmentPoint() function to create the line segment.\nUse the Draw() function to draw the line.\n\n\n\nShow code\nLearnGeom::CoordinatePlane(x_min, x_max, y_min, y_max)\n\n\nNULL\n\n\nShow code\nP1 &lt;- c(2,8)\nP2 &lt;- c(8,6)\nSegment &lt;- LearnGeom::CreateSegmentPoints(P1, P2)\nLearnGeom::Draw(Segment, \"purple\")\n\n\n\n\n\n\n\n\n\nNULL"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221121-Original-QuartroLinks/index.html",
    "href": "blog/Technical-Blog/posts/20221121-Original-QuartroLinks/index.html",
    "title": "Quarto Links",
    "section": "",
    "text": "Here is a list of links I used to help create this Quarto blog. As well as links for web accessibility, examples of other quarto blogs, and examples of non-quarto blogs.\n\n\nQuarto LinksAccessibilityQuarto BlogsOther Blogs\n\n\nA Quarto tip a day\nCreating a blog with Quarto in 10 steps - Bea Milz\nCreate your Data Science portfolio with Quarto: Slides\nFrom R Markdown to Quarto\nHow to style your Quarto blog without knowing a lot of HTML/CSS\nInstalling and Configuring Python with RStudio\nManaging Execution\nQuarto: Creating a Blog (Documentation)\nQuarto/RMarkdown - What’s Different: Slides Repo\nquarto-trio (Using Python, R, and Apache Arrow)\n\n\na11Y - digital accessibility\nWAVE - Web Accessibility Evaluation Tool\n\n\nAbhirup Moitra\nAlbert Rapp\nBea Milz: Repo\nEmy Tamika\nDaniel Tran: Repo\nDanielle Navarro - Notes from a data witch: Repo\nDeesha menghani: Repo\nJesse Adler\nLydia Gibson: Repo\nMeghan Harris: Repo\nRober Mitchell: Repo\nSam Csik: Repo\nTed Landeras, PhD: repo\n\n\nAlicia Johnson (Made with blogdown))\nAllison Hill, PhD\nCrystal Lewis\nGreg Wilson\nDavid Neuzerling: Repo\nJenny Bryan (Made with Blogdown and Hugo): Repo\nJon Harmon\nJulia Silge (Made with Blogdown and Hugo): Repo\nMachine Learning Mastery!\nNan Xiao: Repo\nNicola Rennie: Repo\nTanner Heffner (Made with Svelte): Repo\nYanina Bellini Saibene\nYihui Xie"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html",
    "href": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "",
    "text": "This is a “presentation” style proof for Math-338: Modern College Geometry, and looks at the symmetries of a non-square rectangle."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#suppose-r-is-a-non-square-rhombus.",
    "href": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#suppose-r-is-a-non-square-rhombus.",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "Suppose R is a non-square rhombus.",
    "text": "Suppose R is a non-square rhombus.\n\nList the symmetries of R. Write as a composition of translations, reflections, and rotations.\nProve that the set of symmetries of R is a group under composition."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#parallelogram",
    "href": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#parallelogram",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "1.1 Parallelogram",
    "text": "1.1 Parallelogram\nBoth pairs of opposite sides are parallel.\n\n\n\n\n\nFig. 1:  Parallelogram\n\n\n\n\n\n\\(AB\\parallel DC\\)\n\\(CB\\parallel DA\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#rhombus",
    "href": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#rhombus",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "1.2 Rhombus",
    "text": "1.2 Rhombus\nParallelogram whose sides are all the same length.\nNotation: R\n\n\n\n\n\nFig. 2:  Rhombus\n\n\n\n\n\n\\(\\overline{AB}=\\overline{BC}=\\overline{CD}=\\overline{DA}\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#isometry",
    "href": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#isometry",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "1.3 Isometry",
    "text": "1.3 Isometry\nPreserves distance, length, and angle measure through “rigid motion” of rotations, reflections, translations, and compositions.\n\n1.3.1 Translations\nMove all the points across a vector \\(\\vec{v}\\).\nNotation: \\(\\tau_{\\text{start point,end point}}\\)\n\n\n\n\n\nFig. 3:  Translation of line AB along vector v.\n\n\n\n\n\n\n1.3.2 Rotations\nPick a center O (origin), \\(\\theta\\) (angle), takes P to P’ on a circle on a circle with center O and radius \\(\\overline{OP}\\) with \\(\\angle POP'=\\theta\\).\nNotation: \\(R_{O,\\theta}\\)\n\n\n\n\n\nFig. 4:  Rotation of line OP.\n\n\n\n\n\n\n1.3.3 Reflections\nMirror of a shape across a line.\nPick line b, shown in red in Fig. 5.\n\nPoints on B don’t move.\nPoints not on b, such as P, go to P’ where b is perpendicular to bisector of \\(\\overline{PP'}\\).\nMidpoint m of PP’ on b make right angle \\(\\overline{PP'}\\)\n\nNotation: \\(r_{b}\\)\n\n\n\n\n\nFig. 5:  Triangle QPR reflected across line b.\n\n\n\n\n\n\n1.3.4 Compositions\nCombinations of rotations, reflections, and translations."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#symmetry",
    "href": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#symmetry",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "1.4 Symmetry",
    "text": "1.4 Symmetry\nAn isometry that sends a geometric figure to itself."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#group",
    "href": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#group",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "1.5 Group",
    "text": "1.5 Group\n\n1.5.0 Closure\n\nOrder doesn’t matter.\n\\(ab=ba\\)\n\n\n\n1.5.1 Associativity\n\nParentheses don’t matter.\n\\((ab)c=a(bc)\\)\n\n\n\n1.5.2 Identity\n\nAnything combined with identity equals itself.\n\\(ea=ae=a\\)\n\n\n\n1.5.3 Inverses\n\nUndoes isometry.\n\\(f^{-1}(f(a))=a\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#symmetries-of-r-1",
    "href": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#symmetries-of-r-1",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "2.1 4 Symmetries of R",
    "text": "2.1 4 Symmetries of R\n(Using Boyce’s Notation)\n\n2.1.1 \\(e\\)\n\n\n\n\n\nIdentity\n\n\n\n\n\n\n2.1.2 \\(R_{O,180^\\circ}\\)\n\n\n\n\n\nRotation about center (O) by \\(180^o\\).\n\n\n\n\n\n\n2.1.3 \\(r_{m_1}\\)\n\n\n\n\n\nReflection over \\(m_1\\)\n\n\n\n\n\n\n2.1.4 \\(r_{m_2}\\)\n\n\n\n\n\nReflection over \\(m_2\\)\n\n\n\n\n\n\n2.1.5 Verify\nTo verify this we can imagine that one point only has two places to choose from, then the a different point only has two places to choose from, and then the remaining two points only have one place to choose i.e. \\(2\\cdot 2\\cdot 1\\cdot 1=4\\). Therefore there are only four possible symmetries of R."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#claim-mathscrs-is-a-group-under-composition.",
    "href": "blog/Technical-Blog/posts/20210726-School-Quiz2/index.html#claim-mathscrs-is-a-group-under-composition.",
    "title": "Proving that the Set of Symmetries of Non-Square Rhombus is a Group Under Composition",
    "section": "3.1 Claim: \\(\\mathscr{S}\\) is a group under composition.",
    "text": "3.1 Claim: \\(\\mathscr{S}\\) is a group under composition.\n\n3.1.0 Closure\nWe want to show that composing two symmetries equals a symmetry.\nLet \\(\\square 1234\\) be a non-square rhombus, and suppose F and G are in \\(\\mathscr{S}\\).\n\\[F\\circ G(\\square 1234)=F(G(\\square 1234))=F(\\square 1234)=\\square 1234\\]\nFor example:\nLet \\(F=r_{m_1}\\) and \\(G=r_{m_2}\\). Then \\(F\\circ G(\\square 1234)=R_{O,180^\\circ}\\).\n\n\n3.1.1 Associative\n\\(F\\circ(G\\circ H)=(F\\circ G)\\circ H\\)\n\\[\\begin{equation}\\label{D14,1}\n\\begin{split}\nF\\circ(G\\circ H) &= F\\circ (G\\circ H)(\\square 1234)\\\\\n&= F(G\\circ H (\\square 1234))\\\\\n&= F(G(H(\\square 1234)))\\\\\n&= (F\\circ G)\\circ H(\\square 1234)\n\\end{split}\n\\end{equation}\\]\n\n\n3.1.2 Identity\ne is one of the symmetries.\nExample: \\(R_{O,180^\\circ}\\circ e=R_{O,180^\\circ}\\)\n\n\n3.1.3 Inverses\nEvery symmetry of a rhombus undoes itself.\n\\(e\\circ e=e\\)\n\\(R_{O,180^\\circ}\\circ R_{O,180^\\circ}=e\\)\n\\(r_{m_1}\\circ r_{m_1}=e\\)\n\\(r_{m_2}\\circ r_{m_2}=e\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20211227-School-DiagnosticForLeverageandInfluence/index.html",
    "href": "blog/Technical-Blog/posts/20211227-School-DiagnosticForLeverageandInfluence/index.html",
    "title": "Diagnostic For Leverage And Influence",
    "section": "",
    "text": "This post covers examples from Chapter 6 of Introduction to Linear Regression Analysis.\n\n\n1. Set Up\nFor this analysis I will be using four packages:\n\nmagrittr: for piping (%&gt;%)\ndplyr: to arrange the data\nMASS: to use the boxcox function\nlatex2exp: to put latex on graphs\n\n\n\nShow code\nlibrary(latex2exp) \nlibrary(magrittr) \nlibrary(dplyr) \nlibrary(MASS) \n\n\n\n\n2. Example 6.1: The Delivery Time Data\n(Introduced in Chapter 3 Example 3.1) A soft drink bottler is analyzing the vending machine service routes in his distribution system. He is interested in predicting the amount of time required by the route driver to service the vending machines in an outlet. The service activity includes stocking the machine with beverage products and minor maintenance or housekeeping. The industrial engineer responsible for the study suggests that the two most important variables affecting the delivery time (y) are the number of cases of product stocked (\\(x_1\\)) and the distance walked by the route driver (\\(x_2\\)).\n\n\nShow code\nex31 &lt;- utils::read.csv(\"data/ex6-1.csv\")\nex31\n\n\n    X Observation Delivery_Time_y Number_of_Cases_x1 Distance_x2_.ft.\n1   1           1           16.68                  7              560\n2   2           2           11.50                  3              220\n3   3           3           12.03                  3              340\n4   4           4           14.88                  4               80\n5   5           5           13.75                  6              150\n6   6           6           18.11                  7              330\n7   7           7            8.00                  2              110\n8   8           8           17.83                  7              210\n9   9           9           79.24                 30             1460\n10 10          10           21.50                  5              605\n11 11          11           40.33                 16              688\n12 12          12           21.00                 10              215\n13 13          13           13.50                  4              255\n14 14          14           19.75                  6              462\n15 15          15           24.00                  9              448\n16 16          16           29.00                 10              776\n17 17          17           15.35                  6              200\n18 18          18           19.00                  7              132\n19 19          19            9.50                  3               36\n20 20          20           35.10                 17              770\n21 21          21           17.90                 10              140\n22 22          22           52.32                 26              810\n23 23          23           18.75                  9              450\n24 24          24           19.83                  8              635\n25 25          25           10.75                  4              150\n\n\n\n\nShow code\nlm1 &lt;- stats::lm(ex31$Delivery_Time_y ~ ex31$Number_of_Cases_x1 + ex31$Distance_x2_.ft., data = ex31)\n\n\n\n\nShow code\nex31$hii &lt;- stats::hatvalues(lm1)\nex31\n\n\n    X Observation Delivery_Time_y Number_of_Cases_x1 Distance_x2_.ft.\n1   1           1           16.68                  7              560\n2   2           2           11.50                  3              220\n3   3           3           12.03                  3              340\n4   4           4           14.88                  4               80\n5   5           5           13.75                  6              150\n6   6           6           18.11                  7              330\n7   7           7            8.00                  2              110\n8   8           8           17.83                  7              210\n9   9           9           79.24                 30             1460\n10 10          10           21.50                  5              605\n11 11          11           40.33                 16              688\n12 12          12           21.00                 10              215\n13 13          13           13.50                  4              255\n14 14          14           19.75                  6              462\n15 15          15           24.00                  9              448\n16 16          16           29.00                 10              776\n17 17          17           15.35                  6              200\n18 18          18           19.00                  7              132\n19 19          19            9.50                  3               36\n20 20          20           35.10                 17              770\n21 21          21           17.90                 10              140\n22 22          22           52.32                 26              810\n23 23          23           18.75                  9              450\n24 24          24           19.83                  8              635\n25 25          25           10.75                  4              150\n          hii\n1  0.10180178\n2  0.07070164\n3  0.09873476\n4  0.08537479\n5  0.07501050\n6  0.04286693\n7  0.08179867\n8  0.06372559\n9  0.49829216\n10 0.19629595\n11 0.08613260\n12 0.11365570\n13 0.06112463\n14 0.07824332\n15 0.04111077\n16 0.16594043\n17 0.05943202\n18 0.09626046\n19 0.09644857\n20 0.10168486\n21 0.16527689\n22 0.39157522\n23 0.04126005\n24 0.12060826\n25 0.06664345\n\n\n\n\nShow code\nprint(influence.measures(lm1))\n\n\nInfluence measures of\n     stats::lm(formula = ex31$Delivery_Time_y ~ ex31$Number_of_Cases_x1 +      ex31$Distance_x2_.ft., data = ex31) :\n\n     dfb.1_ dfb.e31.N dfb.e31.D   dffit cov.r   cook.d    hat inf\n1  -0.18727   0.41131  -0.43486 -0.5709 0.871 1.00e-01 0.1018    \n2   0.08979  -0.04776   0.01441  0.0986 1.215 3.38e-03 0.0707    \n3  -0.00352   0.00395  -0.00285 -0.0052 1.276 9.46e-06 0.0987    \n4   0.45196   0.08828  -0.27337  0.5008 0.876 7.76e-02 0.0854    \n5  -0.03167  -0.01330   0.02424 -0.0395 1.240 5.43e-04 0.0750    \n6  -0.01468   0.00179   0.00108 -0.0188 1.200 1.23e-04 0.0429    \n7   0.07807  -0.02228  -0.01102  0.0790 1.240 2.17e-03 0.0818    \n8   0.07120   0.03338  -0.05382  0.0938 1.206 3.05e-03 0.0637    \n9  -2.57574   0.92874   1.50755  4.2961 0.342 3.42e+00 0.4983   *\n10  0.10792  -0.33816   0.34133  0.3987 1.305 5.38e-02 0.1963    \n11 -0.03427   0.09253  -0.00269  0.2180 1.172 1.62e-02 0.0861    \n12 -0.03027  -0.04867   0.05397 -0.0677 1.291 1.60e-03 0.1137    \n13  0.07237  -0.03562   0.01134  0.0813 1.207 2.29e-03 0.0611    \n14  0.04952  -0.06709   0.06182  0.0974 1.228 3.29e-03 0.0782    \n15  0.02228  -0.00479   0.00684  0.0426 1.192 6.32e-04 0.0411    \n16 -0.00269   0.06442  -0.08419 -0.0972 1.369 3.29e-03 0.1659    \n17  0.02886   0.00649  -0.01570  0.0339 1.219 4.01e-04 0.0594    \n18  0.24856   0.18973  -0.27243  0.3653 1.069 4.40e-02 0.0963    \n19  0.17256   0.02357  -0.09897  0.1862 1.215 1.19e-02 0.0964    \n20  0.16804  -0.21500  -0.09292 -0.6718 0.760 1.32e-01 0.1017    \n21 -0.16193  -0.29718   0.33641 -0.3885 1.238 5.09e-02 0.1653    \n22  0.39857  -1.02541   0.57314 -1.1950 1.398 4.51e-01 0.3916   *\n23 -0.15985   0.03729  -0.05265 -0.3075 0.890 2.99e-02 0.0413    \n24 -0.11972   0.40462  -0.46545 -0.5711 0.948 1.02e-01 0.1206    \n25 -0.01682   0.00085   0.00559 -0.0176 1.231 1.08e-04 0.0666"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20211122-Original-Skewness/index.html",
    "href": "blog/Technical-Blog/posts/20211122-Original-Skewness/index.html",
    "title": "Skewness",
    "section": "",
    "text": "This post will give a physical interpretation of skewness that will help you always know if a graph is left or right skewed.\n\nPhysical Interpretation\nImagine your body is a symmetrical bell curve where your neck is your mode, waist is your median, hips are your mean, and are all stacked on top of each other to create a symmetrical bell.\nWhen the hips are to the the right of your neck then you are creating a right skew. When you hips are to the left of your neck then your body bell curve is left skewed as shown in the diagram below.\n\nShakira famously said her hips don’t lie, but my hips dictate skewness."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220530-School-FinalPrepGeometry/index.html",
    "href": "blog/Technical-Blog/posts/20220530-School-FinalPrepGeometry/index.html",
    "title": "Final Prep. Modern College Geometry",
    "section": "",
    "text": "Notes consist of Euclidean geometry, similarity, circles, isometries and symmetries, taxicab geometry, spherical geometry, and hyperbolic geometry.\n\n\n1. Euclidean Geometry\n\nEuclid’s 5th Axiom\nTriangle Congruence: SAS, ASA, SSS, AAS\nAngles and Parallel Lines\n\nvertical angels are congruent\ncorresponding angles are congruent\nalternate interior angles are congruent\nsupplementary angles add to \\(180^\\circ\\)\n\n\nQuadrilaterals: 4 sided figure in the plane, where the edges are straight lines.\nParallelogram: Both pairs of opposite sides are parallel.\nTrapezoid: At least 1 pair of opposite sides are parallel.\nRhombus: Parallelogram, all sides are same length.\nRectangle: Parallelogram whose internal angles are all right angles.\nSquare: Rectangle whose sides are all equal length.\nParallelogram Theorem: Let ABCD be a parallelogram. Then the following are equivalent:\n\nABCD is a parallelogram. (opposite sides are parallel)\n\\(\\angle DAB \\cong \\angle BCD\\) and \\(\\angle ABC\\cong \\angle CDA\\) (angles that are across from each other are congruent)\n\\(AB=CD\\) and \\(BC=DA\\) (opposite sides have equal measure)\n\\(\\overline{AC}\\) and \\(\\overline{BD}\\) bisect each other. (diagnals bisect each other)\n\nAxioms of Area:\n\nTo every polygonal region (space enclosed by straight lines in the plane) there corresponds a unique positive number called \\(\\underline{area}\\).\nIf 2 tirangles are congruent their areas are equal.\nIf \\(R=R_1\\cup R_2\\) and \\(R_1\\cap R_2\\) is a finite number of segments or points, then the area of \\(R\\) is the sum of the areas of \\(R_1+R_2\\).\nThe area of a rectangle is its base times its height.\n\n\n\n2. Similarity\nDilation: shrink or expand by a scaling factor, k, from a center point, P.\nSimilarity: 2 figures are similar if one can be superimposed on the other by a dilation and a sequence of isometries.\n\nAA, SAS\n\n\n\n3. Circles\nCircle: center, radius\nPoints on circle have distance r from the center point O.\nUnit Circle: radius length 1.\nArc: a connected subset of the points on circle.\nChord: line segment connecting 2 points on circle.\nCentral Angle: vertex is center of circle, rays intersect in 2 different points. (pie slice)\nInscribed Angles vertex is on circle, rays intersect circle in 2 points.\nInscribed: verities on circle.\nFor an inscribed square, all four points of a square are on the circle.\nTangent Line: Line that intersects circle at only 1 point.\nRadian: measure of the central angle in a unit circle with arc length of 1.\nInscribed Angle Theorem: An inscribed angle is half of a centeral angle that subtends the same arc.\nCorollary: Any two inscribed angles have the same arc on the circle are congruent.\nPower of the Point Theorem 1: If \\(\\overline{AB}\\) and \\(\\overline{CD}\\) are chords of circle intersecting in x inside a circle. Then \\(Ax\\cdot xB=Cx\\cdot xD\\)\nPower of the Point Theorem 2: LEt P be a point outside a given circle. Suppose we draw two rays from the point P: one ray intersects the circle at points A and B (in that order), and the other intersects the circle at the points C and D (in that order). Then \\(PA\\cdot PB=PC\\cdot PD\\)\n\n\n4. Isometries and Symmetries\nThe set of isometries with composition is a group:\n\nClosure\nAssociativity\nIdentity\nInverses\n\nSymmetry A symmetry is an isometry that sends a geometric figure to itself.\n\n6 symmetries of an equilateral triangle\n8 symmetries of a square\n2n symmetries of a regular polygon\n\n\n\n5. Taxicab Geometry\nEuclidean Distance: \\(d_E(A,B)=\\sqrt{(x_B-x_A)^2+(y_B-y_A)^2}\\)\n\n\\(\\pi\\approx 3.14\\)\n\nTaxicab Distance: \\(d_T=|x_B-x_A|+|y_B-y_A|\\)\n\n\\(\\pi = 4\\)\n\nIsometries for taxicab (traingles):\n\ntranslations\nrotations by \\(90^\\circ k\\) where k is an integer\ncombinations\n\n\n\n6. Spherical Geometry\nNo parallel may be drawn through a point not on a given line.\nEquation of a Sphere: \\(S^2=\\{(x,y,z)\\in\\mathbb{R}^3|x^2+y^2+z^2=\\rho^2\\}\\)\n\ngreat circles are straight lines (equator and longitudes)\n\nDistance of a Sphere: \\(d_s(A,B)=\\rho\\cdot\\text{arc cos}(\\frac{A\\cdot B}{\\rho ^2})\\)\nTriangle Angle Measurements\n\nCan have three right angles\nAll three angles added together will be greater than \\(180^\\circ\\)\n\nArea of a Sphere: \\(\\rho^2\\cdot E\\) (where the excess \\(E=\\alpha+\\beta+\\gamma-180^\\circ\\))\nConsider the surface area of a sphere to be \\(4\\pi\\rho^2\\), then the area of a triangle on a sphere will be a proportion of that.\n\n\n7. Hyperbolic Geometry\nMore than one parallel may be drawn through a point not on a given line.\nInversions about a circle\n\npreserve angles (conformal)\n\nCross Ratio: Given four distinct points (A,B,C,D) in the plane, the cross ratio is define \\((A,B;C,D)=\\frac{AC\\cdot BD}{BC\\cdot AD}\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "",
    "text": "In this post, I’ll be sharing my review of the data visuals and analysis presented in the article Gun Law Scorecard."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#introduction",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#introduction",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "Introduction",
    "text": "Introduction\nGun Law Scorecard grades each U.S. state on gun safety, highlighting the correlation between gun regulation and lower gun-related deaths. The article features 9 interactive data visuals that tell the story of gun laws and deaths in the country. In this review, I will describe each data visual’s features and provide three bullet points summarizing my thoughts."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#grading-the-states-map",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#grading-the-states-map",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "1. Grading the States (Map)",
    "text": "1. Grading the States (Map)\n\n\n\n\n\nThe data visualization is a choropleth map that displays the relationship between a state’s gun law strength, gun death rank, gun death rate, and grade for gun safety. Each state is represented with a color-coded gradient that corresponds to its value for each of these factors.\n\n\n\n\n\n\nThe title of each section in this article corresponds to the content depicted in the visual presented within in.\nThe visual explanation consists of four sentences. The first two describe the grading system used in the article.The third presents the thesis that “strong gun laws save lives”. The last sentence explains how the visual works.\nThe interactive features of the visual include: (1) a drop-down menu located beside the “GRADE” title that allows users to switch between “GUN LAW STRENGTH,” “GUN DEATH RANK,” and “GUN DEATH RATE”; (2) the ability to hover over each state with a mouse, revealing its grade, strength, or rank; and (3) pop-up windows that provide a more detailed analysis of each state."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#grading-the-states-table",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#grading-the-states-table",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "2. Grading the States (Table)",
    "text": "2. Grading the States (Table)\n\n\n\n\n\nA table showing Grade, State, Gun Law Strength, Gun Death Rank, and Fun Death Rate.\n\n\n\n\n\n\nGun Law Strength (Ranked) is the left most column. This follows our intuition to have a ranked row as the leftmost column of the table. It is also the more optimistic choice between Law Strength and Death Rate.\nPop-out darkens the rest of the screen, and gives state specific information of what has changed this year and how each state can improve.\n\n\n\n\nThe table shows all 50 states (no option to show less), so it is helpful to be able to switch back to the map to keep scrolling."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#gun-laws-vs.-gun-deaths",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#gun-laws-vs.-gun-deaths",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "3. Gun Laws Vs. Gun Deaths",
    "text": "3. Gun Laws Vs. Gun Deaths\n\n\n\n\n\nA linear regression dot plot comparing Gun Deaths Per 100,000 to Gun Law Strength.\n\n\n\n\n\n\nTitle of the plot tells the viewer the conclusion of the the visual which is, As Gun Laws Weaken, Gun Deaths Rise.\nI’m curious about some of the outlier states such as New Mexico, and New Hampshire.\nIt would be interesting to include more data and run a more in depth linear analysis."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#compare-states",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#compare-states",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "4. Compare States",
    "text": "4. Compare States\n\n\n\n\n\nComparison Table to compare two states on Grade, Gun Law Strength Rank, Gun Death Rate Rank, Fun Deaths Per 100k, and % Difference from National Average.\n\n\n\n\n\n\nStarting comparison is the top ranking state California, and the bottom ranking state Arkansas.\nThe interactive table above was helpful in finding other comparisons such as Massachusetts and Mississippi.\nThis visual and the table are the only two with the option to share to twitter or facebook on the footer of the visual."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#federal-process",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#federal-process",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "5. Federal Process",
    "text": "5. Federal Process\n\n\n\n\n\nTimeline of Federal Progress from 1934 to 2022.\n\n\n\n\n\n\nBold text in left tells what the visual is about which is that in the past 100 years there has only been 6 national gun laws passed. The idea of more national regulation is a theme throughout this article.\nBoth the word “Progress” and the years are written in red. Showing that progress happened these years.\nSimple, effective, and informative. By clicking on the different hears the reader can learn a little bit about the law. There is also more information about each law when clicking on the “Learn More” button under the text on the left."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#best-worst",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#best-worst",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "6. Best & Worst",
    "text": "6. Best & Worst\n\n\n\n\n\nBest and Worst for Strongest Laws, Most Improved, Safest State, Weakest Laws, Biggest Drop, and Deadliest State.\n\n\n\n\n\n\nGood visual to compare the good and the bad. Each has two interactive buttons. One to show a states entire score card, and the other to input that state into the comparison table.\nMetrics for plaques were Strongest/Weakest Laws, Most Improved/ Biggest Drop, and Safest/Deadliest State.\nEach plaque includes an image of a state, but the image (unlike the other buttons below the line) does not pop the user up to the interactive map. Meaning it is purely decorative. Something to consider."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#state-progress-heat-map-chart",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#state-progress-heat-map-chart",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "7. State Progress (Heat Map Chart)",
    "text": "7. State Progress (Heat Map Chart)\n\n\n\n\n\nHeat Map Chart of New Gun Safety Laws in 2022.\n\n\n\n\n\n\nInteresting choice of visual, but helpful in giving a broad understanding of what the state laws related to guns look like.\nWhen hovering over the category name the rectangle changes from blue to red, and a dark blue box pops up to tell the reader the exact number of laws.\nAnother visual I may have considered using for this is a horizontal bar chart. This visual style would make it easier for the reader to notice that both “Access to Guns” and “Other Gun Safety Laws” have 14 new laws in 2022. Similarly the “Domestic Violence” and “Gun Dealer Regulations” both have 6 new gun laws in 2022."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#state-progress-dot-plot",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#state-progress-dot-plot",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "8. State Progress (Dot Plot)",
    "text": "8. State Progress (Dot Plot)\n\n\n\n\n\nDot Plot of New Gun Safety Laws from 2013 to 2022.\n\n\n\n\n\n\nThis visual works well with the one above it, because after reading about the new laws in 2022 one would be curious what the progression over time has been.\nInteractivity is fun, but I would have preferred the amount actively visible above the plot since there are only 10 points.\nI am curious if there exists data that shows which state gun laws may have been created because of a mass shooting event."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#hate-crimes-guns-bar-graph",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#hate-crimes-guns-bar-graph",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "9. Hate Crimes & Guns (Bar Graph)",
    "text": "9. Hate Crimes & Guns (Bar Graph)\n\n\n\n\n\nBar Graph of Hate Crimes Reported to the FBI from 2012 to 2021.\n\n\n\n\n\n\nImportant visual to add that is related to the main thesis of the article.\nSince the hate crimes seem to be on the rise, it may be interesting to view the number of hate crimes by state, or see if there is any relevence to included that data into the scorecard.\nIt may be interesting to see this same visual, but broken down by hate crime, or comparing other categories of gun violence such as suicide, domestic violence, and police violence."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#final-thoughts",
    "href": "blog/Technical-Blog/posts/20230504-Review-GunLawScoreCard/index.html#final-thoughts",
    "title": "Gun Law Scorecard (Article Review)",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nHere are some of my main take away from this analysis:\n\nA consistent color scheme of Red, White, and Blue was present across the article, and data visuals. Red is more prominently used with guns, gun deaths, worst states, and as a highlight, accent, or call to action color. The blue seems more neutral or positive. Something to consider when deciding on color palletes for visuals.\nA “new” metric was created (the scorecard) to tell a consistent story through multiple data visuals.\nInteractive visuals are a great way to engage the reader to investigate their own state, and familiarize themselves with other states.\nOther data that might be interesting to view, gun sales, suicide by gun, mass shootings, state regulations, and gun related hate crimes."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "",
    "text": "Alicia Johnson led this R User Connect Philadelphia to discus the book she co-authored, Bayes Rules!."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#alicia-johnson",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#alicia-johnson",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "1.1. Alicia Johnson",
    "text": "1.1. Alicia Johnson\n\nStatistics Professor.\nAuthor of Bayes Rules!."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#materials",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#materials",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "1.2. Materials",
    "text": "1.2. Materials\nSlides\nGithub Repository"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#what-does-pheads0.5-mean",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#what-does-pheads0.5-mean",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "2.1. What does P(heads)=0.5 mean?",
    "text": "2.1. What does P(heads)=0.5 mean?\n\nIf I flip this coin over and over, roughly 50% will be heads.\nHeads and Tails are equally plausible.\nBoth a and b make sense.\n\n\nScores: a = 1, b = 3, c = 2\nMajority Responses: C\n\n\nFrequentist Philosophy\n\nlong run outcome\n\nBayesian Philosophy\n\nrelative probability of events\n\nPragmatic Philosophy\n\nboth interpretations make sense"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#what-does-pcandidate-a-wins-0.8-mean",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#what-does-pcandidate-a-wins-0.8-mean",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "2.2 What does P(candidate A wins) = 0.8 mean?",
    "text": "2.2 What does P(candidate A wins) = 0.8 mean?\n\nIf we observe this election over and over, candidate A will win roughly 80% of the time.\nCandidate A is much more likely to win than to lose (4 times more likely).\nThe pollster’s calculation is wrong.\n\n\nMajority Response: B\nScores: a = 1, b = 3, c = 1\n\n\nFreq.\nBayes\nFreq.\n\nthe event cannot be repeated over and over again"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#alicia-claims-she-can-predict-the-outcome-of-a-coin-flip.-mine-claims-she-can-distinguish-between-a-crown-burger-and-a-vegan-alternative.-both-succeed-in-10-out-of-10-trials.-what-do-you-conclude",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#alicia-claims-she-can-predict-the-outcome-of-a-coin-flip.-mine-claims-she-can-distinguish-between-a-crown-burger-and-a-vegan-alternative.-both-succeed-in-10-out-of-10-trials.-what-do-you-conclude",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "2.3 Alicia claims she can predict the outcome of a coin flip. Mine claims she can distinguish between a Crown Burger and a Vegan Alternative. Both succeed in 10 out of 10 trials. What do you conclude?",
    "text": "2.3 Alicia claims she can predict the outcome of a coin flip. Mine claims she can distinguish between a Crown Burger and a Vegan Alternative. Both succeed in 10 out of 10 trials. What do you conclude?\n\nYou’re still more confident in Mine’s claim than Alicia’s claim.\nThe evidence supporting Mine’s claim.\n\n\nScore: a = 3, b = 1\n\n\nBayes\nFreq."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#youve-tested-positive-for-a-very-rare-genetic-trait.-if-you-only-get-to-ask-the-doctor-one-question-which-would-it-be",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#youve-tested-positive-for-a-very-rare-genetic-trait.-if-you-only-get-to-ask-the-doctor-one-question-which-would-it-be",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "2.4 You’ve tested positive for a very rare genetic trait. If you only get to ask the doctor one question, which would it be?",
    "text": "2.4 You’ve tested positive for a very rare genetic trait. If you only get to ask the doctor one question, which would it be?\n\nP(rare trait|+)\nP(+| no rare trait)\n\n\nScore: a = 3, b = 1\n\n\nBayes\n\nasking about uncertainty of hypothesis given certainty of the data\nmore natural question to ask\n\nFreq. = p-value\n\nhard to wrap minds around\nasking about uncertainty in data\nless natural question to ask (since data is certain)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#goals",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#goals",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.1 Goals",
    "text": "3.1 Goals\n\nLearn to think like Bayesians.\nApply Bayesian thinking in a regression setting."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#set-up",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#set-up",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.2 Set Up",
    "text": "3.2 Set Up\n\n\nShow code\n# Load packages\nlibrary(tidyverse)\n#library(tidybayes)\nlibrary(bayesrules)\nlibrary(bayesplot)\n# library(rstanarm)\nlibrary(broom.mixed)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#background",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#background",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.3 Background",
    "text": "3.3 Background\nLet \\(\\pi\\) (“pi”) be the proportion of U.S. adults that believe that climate change is real and caused by people. Thus \\(\\pi\\) is some value between 0 and 1."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#exercise-1-specify-a-prior-model",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#exercise-1-specify-a-prior-model",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.4 Exercise 1: Specify a Prior Model",
    "text": "3.4 Exercise 1: Specify a Prior Model\nThe first step in learning about \\(\\pi\\) is to specify a prior model for \\(\\pi\\) (i.e. prior to collecting any data). Suppose your friend specifies their understanding of \\(\\pi\\) through the “Beta(2, 20)” model. Plot this Beta model and discuss what it tells you about your friend’s prior understanding. For example:\n\nWhat do they think is the most likely value of \\(\\pi\\)?\n\nWhat range of \\(\\pi\\) values do they think are plausible?\n\n\n\nShow code\nplot_beta(alpha = 2, beta = 20)\n\n\n\n\n\n\n\n\n\nNotes:\n\nproportion between 0 and 1 (not \\(-\\infty\\) to \\(+\\infty\\))\nthis model, beta-2-20, is right skewed\n\n\n3.4.1 What is your friend saying is the most likely value of pi?\nAbout .12 or 12 % or people believe in climate change.\n\nspike of model is best estimate\nlooking at range the prior model drops off above .25, so you friend believes under 25% of people believe in climate change."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#check-out-some-data",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#check-out-some-data",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.5 Check Out Some Data",
    "text": "3.5 Check Out Some Data\nThe second step in learning about \\(\\pi\\), the proportion of U.S. adults that believe that climate change is real and caused by people, is to collect data. Your friend surveys 10 people and 6 believe that climate change is real and caused by people. The likelihood function of \\(\\pi\\) plots the chance of getting this 6-out-of-10 survey result under different possible \\(\\pi\\) values. Based on this plot:\n\n\nShow code\nplot_binomial_likelihood(y = 6, n = 10)\n\n\n\n\n\n\n\n\n\nNotes:\n\nThe next step after creating a model (beta-2-20) we collect data.\nThis plot is showing us what the chance is that we got these survey results under different possible pie values.\n\n\n3.5.1 With what values of \\(\\pi\\) are the 6-out-of-10 results most consistent?\nApprox. 60% of people believe in climate change. Shown by our graph spiking at that value.\n\n\n3.5.2 For what values of \\(\\pi\\) would these 6-out-of-10 results be unlikely?\nOur data would not be very likely to happen for values below .25 and above .9."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#exercist-3-build-the-posterior-model",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#exercist-3-build-the-posterior-model",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.6 Exercist 3: Build the Posterior Model",
    "text": "3.6 Exercist 3: Build the Posterior Model\nIn a Bayesian analysis of \\(\\pi\\), we build a posterior model of \\(\\pi\\) by combining the prior model of \\(\\pi\\) with the data (represented through the likelihood function). Plot all 3 components below. Summarize your observations:\n\n\nShow code\nplot_beta_binomial(alpha = 2, beta = 20, y = 6, n = 10)\n\n\n\n\n\n\n\n\n\nNotes:\n\nDepends on a lot of factors\n\n\n3.6.1 What’s your friend’s posterior understanding of \\(\\pi\\)?\nMy friend’s prior understanding of \\(\\pi\\) is not as low as what it was before, but also not as high as what is suggested in the data.\n\n\n3.6.2 How does their posterior understanding compare to their prior and likelihood? Thus how does their posterior balance the prior and data?\nTheir posterior understanding is higher than their prior knowledge and likelihood. The density of their posterior knowledge is lower than their previous density."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#exercise-4-another-friend",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#exercise-4-another-friend",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.7 Exercise 4: Another Friend",
    "text": "3.7 Exercise 4: Another Friend\nConsider another friend that saw the same 6-out-of-10 polling data but started with a Beta(1, 1) prior model for \\(\\pi\\):\n\n\nShow code\nplot_beta(alpha = 1, beta = 1)\n\n\n\n\n\n\n\n\n\n\n3.7.1 Describe the new friend’s understanding of \\(\\pi\\). Compared to the first friend, are they more or less sure about \\(\\pi\\)?\nThis is a uniform distribution which maybe indicates the friend thinks everyone believes in Climate Change.\n\n\n3.7.2 Do you think the new friend will have a different posterior model than the first friend? If so, how do you think it will compare?\nYes, I think their posterior model will be higher than the first friend.\nTest your intuition. Use plot_beta_binomial() to explore your new friend’s posterior model of \\(\\pi\\).\n\n\nShow code\nplot_beta_binomial(alpha = 1, beta = 1, y = 6, n = 10)\n\n\n\n\n\n\n\n\n\nNotes:\n\nThis is a shoulder shrug, uncertain prior model. It could really be anything."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#exercise-5-more-data",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#exercise-5-more-data",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.8 Exercise 5: More Data",
    "text": "3.8 Exercise 5: More Data\nTest your intuition. Use plot_beta_binomial() to explore your new friend’s posterior model of \\(\\pi\\).\n\n\nShow code\ndata(\"pulse_of_the_nation\")\npulse_of_the_nation %&gt;% \n  count(climate_change)\n\n\n# A tibble: 3 × 2\n  climate_change                    n\n  &lt;fct&gt;                         &lt;int&gt;\n1 Not Real At All                 150\n2 Real and Caused by People       655\n3 Real but not Caused by People   195\n\n\n\n3.8.1 How do you think the additional data will impact your first friend’s posterior understanding of \\(\\pi\\)? What about the second friend’s?\nI think the first friends understanding of \\(\\pi\\) would increase will increase, while the second friends understanding will decrease.\n\n\n3.8.2 Upon seeing the 1000-person survey results, do you think your two friends’ posterior understandings of \\(\\pi\\) will disagree a lot or a little?\nI think the two friends’ posterior understanding will disagree a little.\nTest your intuition. Use plot_beta_binomial() to explore both friends’ posterior models of \\(\\pi\\).\n\n\nShow code\n# first friend \nplot_beta_binomial(alpha = 2, beta = 20, y = 655, n = 1000)\n\n\n\n\n\n\n\n\n\nShow code\n# second friend \nplot_beta_binomial(alpha = 1, beta = 1, y = 655, n = 1000)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#exercise-6-your-turn",
    "href": "blog/Technical-Blog/posts/20220822-Workshop-BayesRules/index.html#exercise-6-your-turn",
    "title": "Bayes Rules! - Salt Lake City R User Group",
    "section": "3.9 Exercise 6: Your Turn",
    "text": "3.9 Exercise 6: Your Turn\nLet \\(\\pi\\) be the proportion of U.S. adults that believe in ghosts.\n\nUse plot_beta() to tune your prior model of \\(\\pi\\). To this end, think about what values of \\(\\pi\\) you think are most likely and how sure you are.\n\nNote:\n\nalpha and beta must be positive.\nThe prior means falls at alpha/(alpha + beta). Thus when alpha is smaller than beta, the prior mode falls below 0.5.\nIn general, the smaller the alpha and beta, the more variable / less certain the prior.\n\n\n\nShow code\nbayesrules::plot_beta(alpha = 20, beta = 10)\n\n\n\n\n\n\n\n\n\n\nCollect some data. How many of the 1000 pulse_of_the_nation respondents believe in ghosts?\n\n\n\nShow code\npulse_of_the_nation %&gt;% \n  count(ghosts)\n\n\n# A tibble: 2 × 2\n  ghosts     n\n  &lt;fct&gt;  &lt;int&gt;\n1 No       621\n2 Yes      379\n\n\n\nUse plot_beta_binomial() to visualize your prior, data, and posterior.\n\n\n\nShow code\nbayesrules::plot_beta_binomial(\n  alpha = 20, \n  beta = 10, \n  y = 378, \n  n = 1000)\n\n\n\n\n\n\n\n\n\nCheck out the Github Repository for Part 2: Apply Bayesian thinking to a regression model."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240202-Ready4R-skimr/index.html",
    "href": "blog/Technical-Blog/posts/20240202-Ready4R-skimr/index.html",
    "title": "Ready4R: skimr Package",
    "section": "",
    "text": "The inaugural installment of Ready4R focuses on the skimr package, maintained by rOpenSci.\n\n\n\nSummary table of numeric values of cereals.\n\n\n\nIntroduction\nReady4R is a mailing list offering a free online course initiated by local Oregonian Ted Laderas to impart foundational knowledge in rstats and the tidyverse. Subscribers receive a weekly email delving into various methods for data exploration and analysis. On a weekly basis, I will look into these examples, providing additional insights based on my own experiences.\n\n\nSkim You Data\nThe inaugural installment focuses on the skimr package, maintained by rOpenSci.Ted emphasizes its usefulness with new datasets to grasp the broader picture. In the following code snippet, I install the skimr package and load all necessary data manipulation packages for this tutorial.\n\n\nShow code\n# Install Package\n# install.packages(\"skimr\")\n# Load Packages\nlibrary(skimr)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(janitor)\nlibrary(ggplot2)\n\n\nNext, we proceed to obtain the dataset, which is the Kaggle 80 Cerals. I am using a slightly modified version compared to Ted’s, necessitating adjustments to the code below.\n\n\nShow code\n# Load Data\ncereals &lt;- readr::read_csv(\"../../../../data/cereal.csv\") |&gt;\n  # clean names by converting to lowercase, replace spaces \n  # with underscore and removes special characters\n  janitor::clean_names() |&gt;\n  # make shelf an ordered factor\n  dplyr::mutate(shelf = factor(shelf, ordered = TRUE)) |&gt;\n  # convert mfr and type columns to categorical data\n  dplyr::mutate(across(c(\"mfr\", \"type\"), as.factor))\n\n\n\nOverall Summary\nBefore getting into the skimr package, let’s start with a traditional summary of the dataset:\n\n\nShow code\nsummary(cereals)\n\n\n     name           mfr    type      calories        protein     \n Length:77          A: 1   C:74   Min.   : 50.0   Min.   :1.000  \n Class :character   G:22   H: 3   1st Qu.:100.0   1st Qu.:2.000  \n Mode  :character   K:23          Median :110.0   Median :3.000  \n                    N: 6          Mean   :106.9   Mean   :2.545  \n                    P: 9          3rd Qu.:110.0   3rd Qu.:3.000  \n                    Q: 8          Max.   :160.0   Max.   :6.000  \n                    R: 8                                         \n      fat            sodium          fiber            carbo     \n Min.   :0.000   Min.   :  0.0   Min.   : 0.000   Min.   :-1.0  \n 1st Qu.:0.000   1st Qu.:130.0   1st Qu.: 1.000   1st Qu.:12.0  \n Median :1.000   Median :180.0   Median : 2.000   Median :14.0  \n Mean   :1.013   Mean   :159.7   Mean   : 2.152   Mean   :14.6  \n 3rd Qu.:2.000   3rd Qu.:210.0   3rd Qu.: 3.000   3rd Qu.:17.0  \n Max.   :5.000   Max.   :320.0   Max.   :14.000   Max.   :23.0  \n                                                                \n     sugars           potass          vitamins      shelf      weight    \n Min.   :-1.000   Min.   : -1.00   Min.   :  0.00   1:20   Min.   :0.50  \n 1st Qu.: 3.000   1st Qu.: 40.00   1st Qu.: 25.00   2:21   1st Qu.:1.00  \n Median : 7.000   Median : 90.00   Median : 25.00   3:36   Median :1.00  \n Mean   : 6.922   Mean   : 96.08   Mean   : 28.25          Mean   :1.03  \n 3rd Qu.:11.000   3rd Qu.:120.00   3rd Qu.: 25.00          3rd Qu.:1.00  \n Max.   :15.000   Max.   :330.00   Max.   :100.00          Max.   :1.50  \n                                                                         \n      cups           rating     \n Min.   :0.250   Min.   :18.04  \n 1st Qu.:0.670   1st Qu.:33.17  \n Median :0.750   Median :40.40  \n Mean   :0.821   Mean   :42.67  \n 3rd Qu.:1.000   3rd Qu.:50.83  \n Max.   :1.500   Max.   :93.70  \n                                \n\n\nThe above summary provides a wealth of information. For columns designated as factors (mfr, type, and shelf), we observe counts for each category. Other column types display quantiles.\nNow, let’s utilize skimr::skim to generate a condensed summary of the dataset:\n\n\nShow code\nskim_output &lt;- skimr::skim(cereals)\nsummary(skim_output)\n\n\n\nData summary\n\n\nName\ncereals\n\n\nNumber of rows\n77\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n3\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nThis condensed summary, as Ted aptly notes, offers a more succinct overview. Discrepancies in variable counts by type often signal the need for variable transformation. Personally, I find this approach invaluable, especially when dealing with datasets containing numerous columns. Quickly verifying high-level assumptions can significantly streamline the analysis process. Let’s explore different types of summaries.\n\n\nCharacter Summary\nBelow, we validate our assumptions:\n\nThere is only one character column, “name”.\nThere are 77 unique rows, aligning with our assumption that each row represents a different cereal.\n\n\n\nShow code\nskimr::yank(skim_output, \"character\")\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1\n3\n38\n0\n77\n0\n\n\n\n\n\n\n\nFactor Summary\nBelow, we confirm our assumptions:\n\nThere are three factor columns: mfr, type, and shelf.\nNo missing values are present.\nShelf is the only ordered factor.\nThe dataset comprises seven manufacturers, two types of cereal (cold and hot), and three shelf heights (1: floor, 2: middle, 3: top).\n\n\n\nShow code\nskimr::yank(skim_output, \"factor\")\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nmfr\n0\n1\nFALSE\n7\nK: 23, G: 22, P: 9, Q: 8\n\n\ntype\n0\n1\nFALSE\n2\nC: 74, H: 3\n\n\nshelf\n0\n1\nTRUE\n3\n3: 36, 2: 21, 1: 20\n\n\n\n\n\n\n\nNumeric Summary\nLastly, we examine the numeric summary, which provides information similar to the traditional summary. However, it also includes histograms, offering additional insights.\n\n\nShow code\nskimr::yank(skim_output, \"numeric\")\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncalories\n0\n1\n106.88\n19.48\n50.00\n100.00\n110.00\n110.00\n160.0\n▁▂▇▂▁\n\n\nprotein\n0\n1\n2.55\n1.09\n1.00\n2.00\n3.00\n3.00\n6.0\n▇▆▂▁▁\n\n\nfat\n0\n1\n1.01\n1.01\n0.00\n0.00\n1.00\n2.00\n5.0\n▇▂▁▁▁\n\n\nsodium\n0\n1\n159.68\n83.83\n0.00\n130.00\n180.00\n210.00\n320.0\n▃▂▇▇▂\n\n\nfiber\n0\n1\n2.15\n2.38\n0.00\n1.00\n2.00\n3.00\n14.0\n▇▃▁▁▁\n\n\ncarbo\n0\n1\n14.60\n4.28\n-1.00\n12.00\n14.00\n17.00\n23.0\n▁▁▆▇▃\n\n\nsugars\n0\n1\n6.92\n4.44\n-1.00\n3.00\n7.00\n11.00\n15.0\n▅▇▇▆▇\n\n\npotass\n0\n1\n96.08\n71.29\n-1.00\n40.00\n90.00\n120.00\n330.0\n▇▇▂▁▁\n\n\nvitamins\n0\n1\n28.25\n22.34\n0.00\n25.00\n25.00\n25.00\n100.0\n▁▇▁▁▁\n\n\nweight\n0\n1\n1.03\n0.15\n0.50\n1.00\n1.00\n1.00\n1.5\n▁▁▇▁▁\n\n\ncups\n0\n1\n0.82\n0.23\n0.25\n0.67\n0.75\n1.00\n1.5\n▂▇▇▁▁\n\n\nrating\n0\n1\n42.67\n14.05\n18.04\n33.17\n40.40\n50.83\n93.7\n▅▇▅▁▁\n\n\n\n\n\n\n\nOverall\nExploring this has been enlightening, and I anticipate revisiting it. It’s surprising how frequently I find myself explaining to non-data professionals that the mean (or average) isn’t always the most reliable indicator of sample behavior. Data can be heavily skewed, making visualizations essential for accurate interpretation. While tools like ggplot offer sophisticated visualization options, the initial data review provided by skimr is invaluable."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220228-Workshop-RStudioBuildingaBlogwithR/index.html",
    "href": "blog/Technical-Blog/posts/20220228-Workshop-RStudioBuildingaBlogwithR/index.html",
    "title": "RStudio: Building a Blog with R",
    "section": "",
    "text": "These are my notes from Posit’s (formerly R Studio) “Building a Blog with R” tutorial, which was presented by Isabella Velasquez. Materials available on Github.\n\n\n1. About Isabella Velesquez\nEmail: isabella.velasquez@rstudio.com\n\nWorks at Posit.\nSeattle Lady Co-Organizer.\nFirst R-Ladies talk in 2018.\n\n\n\n2. Agenda\n\nWhy create a blog?\nDeciding on a topic.\nTools for building a blog.\n\n\n\n3. Why create a blog?\n\nWhen you’re given the same advice 3 times, write a blog post.\nShare what you’ve learned.\nWrite your opinions.\nShare updates, and news.\nExternal blogs (for business)\n\nPosit has 4 different external blogs:\n\nThe Posit Blog\nPosit AI Blog\nTidyverse\nR Views\n\n\nInternal blogs (for business)\n\nShare information more easily and effectively.\nImprove collaboration.\nServing as a bulletin board for projects.\n\n\n\n\n4. Types of Posts\n\nStandard lists\nHow To’s / tutorials\nNew posts\nProblem - and - solution\nFAQ\nCheat sheets\nChecklists\nInfo graphics\nPresentations\nDebates\nInspiration\nInterviews\n\n\n\n5. Seperating Posts\n\nTutorials (learning oriented)\nHow To’s (task oriented)\nExplanation (understanding oriented)\nReference (information oriented)\n\n\n\n6. Building a Blog with R\n\nKnowledge of R and R Markdown.\nVersion Control (Github)\nNetlify\n\n\n\n7. Overall Thoughts\nWhile this was called “Building a blog”, there wasn’t a lot of blog building. It was very business and product information heavy.\n\n\n8. Recommended Blog (from chat)\nMachine Learning Mastery\n\nCheck out Posit meet-up for more from Posit."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220418-Workshop-TidySpice/index.html",
    "href": "blog/Technical-Blog/posts/20220418-Workshop-TidySpice/index.html",
    "title": "Tidy Spice",
    "section": "",
    "text": "Inspired by Julia Silge’s Topic Modeling for #TidyTuesday Spice Girls Lyrics, and Ariane Aumaitre’s, Tutorial: Text analysis and data visualization with Taylor Swift songs.\nShow code\nknitr::opts_chunk$set(error=FALSE, \n                      message= FALSE,\n                      warning=FALSE)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220418-Workshop-TidySpice/index.html#gamma-matrix",
    "href": "blog/Technical-Blog/posts/20220418-Workshop-TidySpice/index.html#gamma-matrix",
    "title": "Tidy Spice",
    "section": "Gamma Matrix",
    "text": "Gamma Matrix\nGamma Matricies\n\n\nShow code\nsong_topics &lt;- tidytext::tidy(topic_model,\n                              matrix = \"gamma\",\n                              document_names = base::rownames(lyrics_sparse)\n)\nsong_topics\n\n\n# A tibble: 124 × 3\n   document                   topic    gamma\n   &lt;chr&gt;                      &lt;int&gt;    &lt;dbl&gt;\n 1 2 Become 1                     1 0.932   \n 2 Denying                        1 0.00154 \n 3 Do It                          1 0.996   \n 4 Get Down With Me               1 0.300   \n 5 Goodbye                        1 0.000971\n 6 Holler                         1 0.00155 \n 7 If U Can't Dance               1 0.000896\n 8 If You Wanna Have Some Fun     1 0.0171  \n 9 Last Time Lover                1 0.140   \n10 Let Love Lead the Way          1 0.00178 \n# ℹ 114 more rows\n\n\n\n\nShow code\nsong_topics %&gt;%\n  dplyr::mutate(\n    song_name = fct_reorder(document, gamma),\n    topic = base::factor(topic)\n  ) %&gt;%\n  ggplot2::ggplot(ggplot2::aes(gamma, topic, fill = topic)) +\n  ggplot2::geom_col(show.legend = FALSE) +\n  ggplot2::facet_wrap(vars(song_name), ncol = 4) +\n  ggplot2::scale_x_continuous(expand = c(0, 0)) +\n  ggplot2::labs(x = base::expression(gamma), y = \"Topic\")"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html",
    "href": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html",
    "title": "Code to Content Lighting Talk",
    "section": "",
    "text": "In this post are the materials for my PDX R lightning talk called: Code to Content."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html#slide-1-code-to-content",
    "href": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html#slide-1-code-to-content",
    "title": "Code to Content Lighting Talk",
    "section": "Slide 1: Code to Content",
    "text": "Slide 1: Code to Content\nHello, good afternoon everyone!\nI’m Randi, and since rendering my first R markdown file to HTML back in 2020, I’ve been complimenting my growing technical skills with web pages and blogs. Today, I’m excited to share the transformative power of turning code into content."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html#slide-2-agenda",
    "href": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html#slide-2-agenda",
    "title": "Code to Content Lighting Talk",
    "section": "Slide 2: Agenda",
    "text": "Slide 2: Agenda\n[image: Purple Scroll]\nBefore we set sail, let me briefly outline our agenda.\n[bullet: Adventure Awaits /n Reasons to Embark on Your Expedition]\nWe’ll kick things off by diving into the compelling reasons why you should start blogging.\n[bullet: Blogging Blueprint /n Building Your Narrative With Quarto]\nNext, we’ll sail through the high-level steps on blog origination, ensuring you’re equipped to navigate your blogging journey smoothly.\n[bullet: Curated Counsel /n Advice for Smooth Sailing]\nAfter that, I’ll share some friendly and hopefully time saving advice.\n[bullet: Dive Deeper /n Resourceful Links]\nFinally, I’ll drop the anchor and share some resources that have been instrumental in my own blogging odyssey.\n[transition]"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html#slide-3-adventure-awaits",
    "href": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html#slide-3-adventure-awaits",
    "title": "Code to Content Lighting Talk",
    "section": "Slide 3: Adventure Awaits",
    "text": "Slide 3: Adventure Awaits\nNow, let’s talk about the adventure that awaits us.\n[image: shadowy ghost pirate ship]\nWith the Reasons to Embark on Your Expedition\n[bullet: Joy in Expression and Self Discovery]\nBlogging in the tech realm isn’t just about information; it’s a canvas for personal expression. Your blog is your playground, and being your own critic is a rewarding journey.\n[bullet: Own Your Narrative]\nIt’s yours - no meddling billionaires, no unwelcome changes. Every tweak, every evolution - it’s all in your hands. Your blog is your ship, and you’re the captain determining its course.\n[bullet: Continuous Growth and Learning]\nInitially, I blogged to enhance my R skills, but the unexpected bonus was the development of soft skills and a cross-disciplinary dialect. Even without direct work experience, different topics I’ve explored in blog posts made me more adept at communicating complex concepts and helped me learn about new tools and industries much faster.\n[bullet: Reveal Fresh Perspectives]\nUse your blog to broaden your view, and see beyond what you see immediately.\n[bullet: Data Advocacy and Empowerment]\nYour blog has the power to empower others with knowledge. Even a simple concept not only benefits your fellow data enthusiast but also those outside our technical bubble.\n[bullet: Embedded in R Culture]\nContribute to the vibrant culture of R by paying it forward. We’ve all found insights, solutions, and even community in blog posts. Being a part of this exchange enriches the collective knowledge.\n[transition]\nNow, let;s chart our course with the …"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html#slide-4-blogging-blueprint",
    "href": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html#slide-4-blogging-blueprint",
    "title": "Code to Content Lighting Talk",
    "section": "Slide 4: Blogging Blueprint",
    "text": "Slide 4: Blogging Blueprint\nBlogging Blueprint,\n[image: ship blueprint]\nbuilding your narrative with Quarto.\n[bullet: knowledge acquisition, Ideation, and Planning]\nBegin your voyage by delving into the Quaro documentation. Explore blogs within and beyond your field for inspiration. Then, draft an outline that guides both the structure and content of your blog.\nNext,\n[bullet: Setup, Version Control, and Deployment]\nUse RStudio to swiftly create a Quarto blog. Connect that to a Github repository for version control. Finally, anchor it to your Netlify account to make your blog visible on the digital sea.\nFrom there,\n[bullet: Configure, Personalize, and Post]\nConfigure your blog using the .yaml file. Infuse it with your personality, and launch your inaugural post.\nBut we’re not done yet.\n[bullet: Aesthetics, Ownership, and Analysis]\nNavigate the currents of aesthetics with a .scss file, claim your digital territory by securing a web URL, and activate Google Analytics to analyze waves of traffic hitting your shores to offer you insights to further optimizations.\nNow let’s set sail smoothly with some,\n[transition]"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html#slide-5-curated-council",
    "href": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html#slide-5-curated-council",
    "title": "Code to Content Lighting Talk",
    "section": "Slide 5: Curated Council",
    "text": "Slide 5: Curated Council\nCurated Council\n[image: three dreamy pirates in the clouds]\nAdvice for smooth sailing.\nLet’s navigate through the essentials:\n[bullet: Prioritize Accessibility]\nJust like setting sail with the wind, prioritize accessibility. Make it the norm, and not an afterthought. Include alt text for images, mind the colors for contrast, ensure a consistent design, and be mindful of different screen sizes.\n[bullet: Select the Right Tool for the Job]\nIn the vast sea of tools, Quarto stands tall with its versatility in rendering outputs and proficiency in various languages. But beware, for extensive data or bulky long term updates, consider other tools more suitable for that size and scale.\n[bullet: Avoid Duplicating Other Profiles]\nWhile it’s tempting to copy and paste from existing platforms, redundancy will send your audience to different waters. Be distinctive, or risk seeing your viewers sail away to more familiar ports.\n[bullet: Consider Your Audience]\nTailor your blog to your audience, and craft your landing page with care. Guide your viewers; don’t overload them. Give them clear paths to set their course with ease.\nNow it’s time to,"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html#slide-6-dive-deeper",
    "href": "blog/Technical-Blog/posts/20240508-Original-CodetoContentSlides/index.html#slide-6-dive-deeper",
    "title": "Code to Content Lighting Talk",
    "section": "Slide 6: Dive Deeper",
    "text": "Slide 6: Dive Deeper\n[image: pirate boat from ocean perspective]\nInto the vast ocean of resources that will aid you in your own journey.\nTo start,\n[bullet: Quarto &gt; Websites &gt; Creating a Blog]\nExplore the Quarto documentation, a scrollable one-page guide.\nThen, check out\n[bullet: Creating a blog with Quarto in 10 steps]\nThis comprehensive guide provides a detailed, image-rich walkthrough.\nNext, explore\n[bullet: Create Your Data Science Portfolio with Quarto]\nA hands-on tutorial where you create a Data Science Portfolio for Wednesday Adams.\nOnce you’ve got the hang of creating a blog with Quarto, move on to\n[bullet: Style Your Quarto Blog without Knowing a lot of HTML/CSS]\nThis game-changer saves time finding a template and imparts high-level skills in web development.\nLast, but not least\n[bullet: Quarto/ RMarkdown - What’s Different?]\nPresented by the organizers of this group, Ted Laderas. This presentation helps bridge the gap between RMarkdown and Quarto, and also inspired more interactivity in my blog posts.\n[Conclusion]\nAnd there you have it! We’ve uncovered some of the adventure that awaits us, charted course through the blogging blueprint, navigated smoother seas with curated advice and dove deeper into some resources to aid you on your journey. Set sail with code, and let your journey of content begin!"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220627-Original-QuantmodStockPrices/index.html",
    "href": "blog/Technical-Blog/posts/20220627-Original-QuantmodStockPrices/index.html",
    "title": "Predicting Stock Prices",
    "section": "",
    "text": "This post uses the R packages quantmod and prophet to predict stock prices for MicroVision.\n\n1. Background\nDuring the summer of 2021 I took a business fiance class, and learned a lot about stocks and the stock market. Most of my projects centered around MicroVision which is a research and development company creating the newest Lidar technology for autonomous vehicles.\n\n\n2. Set Up\nThis analysis will use three packages:\n\ntidyverse: to clean the data\nquantmod: to get data from yahoo finace\nprophet: to make predictions\n\n\n\nShow code\n# Load Libraries \nlibrary(quantmod)\nlibrary(tidyverse)  \nlibrary(prophet) \n\n\nThere are a few places that quantmod can pull data from, but the default which I will be using is Yahoo Finace. You can specify this with src=\"yahoo\". Use ?getSymbols for more information on this functions parameters.\nNote: If you are having issues with quantmod try re-installing it.\n\n\nShow code\nquantmod::getSymbols(\"MVIS\", src=\"yahoo\")\n\n\n[1] \"MVIS\"\n\n\n\n\n3. quantmod Functions\nRight away we quantmod functions like chartSeries() can be used to look at various plots of MicroVision stocks. Below are a few examples showing various subsets of data from 2019 - 2022. As well as different types such as bar, line, candlesticks, and auto. The last graph also includes addBBands(), addMomentum() and addROC(). Use ? with any of these functions to find out more details on their parameters.\n\n\nShow code\nchartSeries(MVIS, type = \"candlesticks\", subset = '2019-09-01::2019-12-31', theme= chartTheme('white'))\n\n\n\n\n\n\n\n\n\nShow code\nchartSeries(MVIS, type = \"bar\", subset = '2020', theme= chartTheme('white'))\n\n\n\n\n\n\n\n\n\nShow code\nchartSeries(MVIS, type = \"line\", subset = '2021', theme= chartTheme('white'))\n\n\n\n\n\n\n\n\n\nShow code\nchartSeries(MVIS, type = \"auto\", subset = '2022', theme= chartTheme('white'))\n\n\n\n\n\n\n\n\n\nShow code\nchartSeries(MVIS, type = \"auto\", subset = '2022-06', theme= chartTheme('white'))\n\n\n\n\n\n\n\n\n\nShow code\naddBBands(n=20,sd=2)\n\n\n\n\n\n\n\n\n\nShow code\naddMomentum(n=1)\n\n\n\n\n\n\n\n\n\nShow code\naddROC(n=7)\n\n\n\n\n\n\n\n\n\n\n\n4. Tidy Data\n\n\nShow code\nhead(MVIS)\n\n\n           MVIS.Open MVIS.High MVIS.Low MVIS.Close MVIS.Volume MVIS.Adjusted\n2007-01-03     25.68     26.40    24.16      24.56      106750         24.56\n2007-01-04     25.76     30.48    25.44      29.92      703538         29.92\n2007-01-05     30.00     31.44    28.16      29.60      333425         29.60\n2007-01-08     31.92     32.64    30.24      31.44      341050         31.44\n2007-01-09     30.96     31.20    29.20      29.36      242725         29.36\n2007-01-10     29.68     30.80    27.76      29.68      169038         29.68\n\n\nYou might notice that the date column doesn’t have a column name, and that is because it is being read as the names of the rows. To change this I will first need to change the data into a data frame, and then change the row names into a column with the function rownames_to_column(). It is then important to assign the date series column the name ds. That is how the prophet package will recognize it, so this will save us the step of renaming this later on.\n\n\nShow code\nMVIS &lt;- base::data.frame(MVIS)\nMVIS &lt;- tibble::rownames_to_column(MVIS, \"ds\")\nutils::head(MVIS)\n\n\n          ds MVIS.Open MVIS.High MVIS.Low MVIS.Close MVIS.Volume MVIS.Adjusted\n1 2007-01-03     25.68     26.40    24.16      24.56      106750         24.56\n2 2007-01-04     25.76     30.48    25.44      29.92      703538         29.92\n3 2007-01-05     30.00     31.44    28.16      29.60      333425         29.60\n4 2007-01-08     31.92     32.64    30.24      31.44      341050         31.44\n5 2007-01-09     30.96     31.20    29.20      29.36      242725         29.36\n6 2007-01-10     29.68     30.80    27.76      29.68      169038         29.68\n\n\nIt will also be important that the ds column is read as date values instead of character value.\n\n\nShow code\nMVIS &lt;- MVIS %&gt;% \n  dplyr::mutate(ds = as.Date(ds))\nutils::head(MVIS)\n\n\n          ds MVIS.Open MVIS.High MVIS.Low MVIS.Close MVIS.Volume MVIS.Adjusted\n1 2007-01-03     25.68     26.40    24.16      24.56      106750         24.56\n2 2007-01-04     25.76     30.48    25.44      29.92      703538         29.92\n3 2007-01-05     30.00     31.44    28.16      29.60      333425         29.60\n4 2007-01-08     31.92     32.64    30.24      31.44      341050         31.44\n5 2007-01-09     30.96     31.20    29.20      29.36      242725         29.36\n6 2007-01-10     29.68     30.80    27.76      29.68      169038         29.68\n\n\nLastly to create a data frame for the prophet package with just the date and closing costs. It is important to rename the closing cost column to y.\n\n\nShow code\n# CLosing Data \nMVIS_close &lt;- base::data.frame(ds = MVIS$ds, y = MVIS$MVIS.Close)\nutils::head(MVIS_close)\n\n\n          ds     y\n1 2007-01-03 24.56\n2 2007-01-04 29.92\n3 2007-01-05 29.60\n4 2007-01-08 31.44\n5 2007-01-09 29.36\n6 2007-01-10 29.68\n\n\n\n\n5. Prophet Functions\nUsing the prophet() function I can create a model of the data. Then I can use make_future_dataframe() to make a predicted model of the next three years.\n\n\nShow code\n# call prophet function to fit the model \nModel1 &lt;- prophet::prophet(MVIS_close,\n                           daily.seasonality=TRUE)\nFuture1 &lt;- prophet::make_future_dataframe(Model1,\n                                          periods = 365*3)\nutils::tail(Future1)\n\n\n             ds\n5473 2027-05-26\n5474 2027-05-27\n5475 2027-05-28\n5476 2027-05-29\n5477 2027-05-30\n5478 2027-05-31\n\n\n\n\n6. Predict Function\nThe predict() function is a stats function that uses various model fitting functions to predict future results.\n\n\nShow code\n# Forecast Proper \nForecast1 &lt;- stats::predict(Model1, Future1)\n# Forecast Values \npredict_date &lt;- Forecast1$ds[length(Forecast1$ds)]\npredict_value &lt;- Forecast1$yhat[length(Forecast1$yhat)]\npredict_lower &lt;- Forecast1$yhat_lower[length(Forecast1$yhat_lower)]\npredict_upper &lt;- Forecast1$yhat_upper[length(Forecast1$yhat_upper)]\n\n\nThis model predicts that on 2027-05-31, the value of MicroVision stock will be about -5.228349. This value is in a range between -38.7229441 and 33.7563832. Note that this range is so large because of the long time period on which it is making the prediction.\n\n\n7. Plot Model\nBelow is an interactive plot that shows the actual values in black, and the predicted values in blue. The grey graph underneath can be adjusted to look at a specific window of time.\n\n\nShow code\nprophet::dyplot.prophet(Model1, Forecast1)\n\n\n\n\n\n\n\n\n8. Plot Componets\nLastly using the prophet_plot_components() function can be used to see yearly, weekly, seasonally, and daily trends.\n\n\nShow code\nprophet::prophet_plot_components(Model1, Forecast1)\n\n\n\n\n\n\n\n\n\nLooking at daily trends it appears that MicroVision was on the decline after 2010 for some time, but has been trading up since about 2019.\nLooking at the weekly trends, it obvious MicroVision is popular during the Monday - Friday trading week, however looking at the time trends the stock is most popular at the beginning and end of the day.\nThe third graph shows us that MicroVision does not seem to preform well in the first quarter of the year, but picks up around May until it starts to drop off again around November.\n\n\n9. Sources\nEasily Import Financial Data to R with Quantmod\nTechnical Analysis with R - Ch.7 Quantmod\nForecasting Bitcoin Prices Using Prophet in R"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210621-Original-balletdata/index.html",
    "href": "blog/Technical-Blog/posts/20210621-Original-balletdata/index.html",
    "title": "Ballet Data",
    "section": "",
    "text": "This post composes a short data set of 13 ballets, and then creates a table with clickable links, and an interactive graph with plotly."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210621-Original-balletdata/index.html#footnotes",
    "href": "blog/Technical-Blog/posts/20210621-Original-balletdata/index.html#footnotes",
    "title": "Ballet Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA Breif History of Ballet↩︎"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20221031-Proof-LogicalEquivalence/index.html",
    "href": "blog/Technical-Blog/posts/20221031-Proof-LogicalEquivalence/index.html",
    "title": "Prove P is Logically Equivalent to the Negation of the Negation of P",
    "section": "",
    "text": "Let P be a statement form. Prove that \\(P \\equiv\\neg (\\neg P)\\).\n\n\nSolution 1\nConsider the truth table for P, \\(\\neg P\\), and \\(\\neg (\\neg P)\\), as shown below in Figure 1:\n\nFigure 1: Truth Table\n\n\nP\n\\(\\neg P\\)\n\\(\\neg (\\neg P)\\)\n\n\n\n\nT\nF\nT\n\n\nF\nT\nF\n\n\n\nSince the truth values for P and \\(\\neg (\\neg P)\\) are the same then P and \\(\\neg (\\neg P)\\) are logically equivalent.\n\n\nSolution 2\nSuppose by way of contradiction (BWOC) that P and \\(\\neg (\\neg P)\\) are not logically equivalent.\nLet P be true and then \\(\\neg (\\neg P)\\) would be false.\nIf P is true then \\(\\neg P\\) would be false, but \\(\\neg P\\) and \\(\\neg (\\neg P)\\) cannot both be false. Therefore BWOC \\(\\neg (\\neg P)\\equiv P\\).\n\\(\\square\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220221-Workshop-UOggplot2/index.html",
    "href": "blog/Technical-Blog/posts/20220221-Workshop-UOggplot2/index.html",
    "title": "UO: ggplot2 Part 2",
    "section": "",
    "text": "Here are my notes from the University of Oregon’s Data Service Consultants workshop on ggplot2, part 2, led by Cameron Mulder.\n\n1. Set Up\nFor this post we used the following packages:\n\nggplot2: to create nice looking plots.\nmagrittr: to pipe %&gt;%.\ndplyr: to use filter().\nflametree: to make art.\nozmaps: to make Australian Maps.\nrmapshaper: to use ms_simplyfy to simplify polygons.\nplotly: to create interactive plots.\n\n\n\nShow code\nbase::library(ggplot2)\nbase::library(magrittr)\nbase::library(dplyr)\nbase::library(flametree)\nbase::library(ozmaps)\nbase::library(rmapshaper)\nbase::library(plotly)\n\n\nAnd the following data sets:\n\ncars\nBOD(Biochemical Oxygen Demand)\n\n\n\nShow code\nutils::data(\"cars\")\nutils::data (\"BOD\")\n\n\n\n\n2. ggplot2 Review\nTo start we made a simple point plot using the cars data set.\nNote: ggplot(data= &lt;DATA&gt;, mapping = aes(&lt;MAPPING&gt;))+ &lt;GEOM FUNCTION&gt;()\n\n\nShow code\nggplot2::ggplot(\n  data = mpg, \n  mapping = ggplot2::aes(\n    x = displ, \n    y = hwy)\n  ) + \n  ggplot2::geom_point()\n\n\n\n\n\n\n\n\n\nWe can compare this to a simple plot in base R.\nNote : the $ is how we id the specific variable we are wanting to work with.\n\n\nShow code\nbase::plot(mpg$displ, mpg$hwy)\n\n\n\n\n\n\n\n\n\n\n\n3. Line Graph\nStarting with base R:\nNote: help(pressure) is the same as ?pressure\n\n\nShow code\nbase::plot(pressure$temperature, pressure$pressure, type = \"l\")\n# add points\ngraphics::points(pressure$temperature, pressure$pressure)\n# add lines (and points)\ngraphics::lines(pressure$temperature, pressure$pressure/2, col = \"red\")\ngraphics::points(pressure$temperature, pressure$pressure/2, col = \"red\")\n\n\n\n\n\n\n\n\n\nggplot:\n\n\nShow code\nggplot2::ggplot(pressure, ggplot2::aes(x = temperature, y = pressure)) + \n  ggplot2::geom_line() + \n  ggplot2::geom_point() + \n  ggplot2::geom_line(ggplot2::aes(x = temperature, y = pressure/2), color = \"red\") + \n  ggplot2::geom_point(ggplot2::aes(x = temperature, y = pressure/2), color = \"red\") \n\n\n\n\n\n\n\n\n\n\n\n4. Bar Graphs\nBase R:\n\n\nShow code\ngraphics::barplot(BOD$demand, names.arg = BOD$Time)\n\n\n\n\n\n\n\n\n\n\n\nShow code\ngraphics::barplot(base::table(mtcars$cyl))\n\n\n\n\n\n\n\n\n\nggplot2:\n\n\nShow code\nggplot2::ggplot(BOD, ggplot2::aes(x = base::factor(Time), y = demand)) + \n  ggplot2::geom_col()\n\n\n\n\n\n\n\n\n\nNotice that the 6 isn’t there because of factor().\nNote : geom_bar does counts, but column has the height of the bar based on the data.\n\n\nShow code\nggplot2::ggplot(mtcars, aes(x=cyl)) +\n  ggplot2::geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n5. Histogram\nBase R:\n\n\nShow code\ngraphics::hist(mtcars$mpg, breaks = 4)\n\n\n\n\n\n\n\n\n\nggplot2:\n\n\nShow code\nggplot2::ggplot(mtcars, ggplot2::aes(x=mpg)) +\n  ggplot2::geom_histogram(binwidth = 4)\n\n\n\n\n\n\n\n\n\n\n\n6. Boxplot\nBase R:\n\n\nShow code\nbase::plot(ToothGrowth$supp, ToothGrowth$len)\n\n\n\n\n\n\n\n\n\nBase R: Formula Syntax\n\n\nShow code\nbase::plot(len ~ supp, data = ToothGrowth)\nbase::plot(len ~ supp + dose, data = ToothGrowth)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2:\n\n\nShow code\nggplot2::ggplot(ToothGrowth, ggplot2::aes(x= supp, y = len)) +\n  ggplot2::geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n7. Time Series\nggplot2 will automatically recognize the variable as a date as long as the variable is imported as a date.\nTo start create some dummy data:\n\n\nShow code\ndata &lt;- base::data.frame(\n  day = base::as.Date(\"2017-06-14\")-0:364,\n  value = stats::runif(365)\n)\nutils::head(data)\n\n\n         day      value\n1 2017-06-14 0.33532259\n2 2017-06-13 0.57800034\n3 2017-06-12 0.06721417\n4 2017-06-11 0.40197493\n5 2017-06-10 0.10295153\n6 2017-06-09 0.67759936\n\n\nThen plot it with ggplot2:\n\n\nShow code\nggplot2::ggplot(data, ggplot2::aes(x = day, y = value)) +\n  ggplot2::geom_line()\n\n\n\n\n\n\n\n\n\nNow to make a plot with the economics data set which is included in ggplot2.\n\n\nShow code\nggplot2::ggplot(data = economics, ggplot2::aes(x = date, y = pop)) +\n  ggplot2::geom_line()\n\n\n\n\n\n\n\n\n\nNext create a subset of the data from 2006 and beyond:\n\n\nShow code\nsubset &lt;- ggplot2::economics %&gt;%\n  dplyr::filter(date&gt;base::as.Date(\"2006-1-1\"))\n\n\nNow to create a different line graph of the subset data over time where the size of the line is based on the value of unemployment (which is the number of unemployment in thousands).\n\n\nShow code\nggplot2::ggplot(economics, ggplot2::aes(x = date, y = pop)) +\n  ggplot2::geom_line(ggplot2::aes(size = unemploy), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n8. Maps\nUsing map_data() get lat and long data for counties in Oregon:\n\n\nShow code\nor_counties &lt;- ggplot2::map_data(\"county\", \"oregon\") %&gt;%\n  dplyr::select(lon = long, lat, group, id = subregion)\nutils::head(or_counties)\n\n\n        lon      lat group    id\n1 -117.2042 44.30683     1 baker\n2 -117.4907 44.30683     1 baker\n3 -117.4907 44.38704     1 baker\n4 -117.5366 44.42142     1 baker\n5 -117.5709 44.42142     1 baker\n6 -117.5996 44.43861     1 baker\n\n\nUsing or_counties data create a ggplot2 map:\n\n\nShow code\nggplot2::ggplot(or_counties, ggplot2::aes(lon, lat, group = group))+\n  ggplot2::geom_polygon(fill = \"white\", color = \"grey\") +\n  ggplot2::coord_quickmap()\n\n\n\n\n\n\n\n\n\nUsing ozmap_states get the names of the different states in Australia.\n\n\nShow code\noz_stats &lt;- ozmaps::ozmap_states\noz_stats\n\n\nSimple feature collection with 9 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 105.5507 ymin: -43.63203 xmax: 167.9969 ymax: -9.229287\nGeodetic CRS:  GDA94\n# A tibble: 9 × 2\n  NAME                                                                  geometry\n* &lt;chr&gt;                                                       &lt;MULTIPOLYGON [°]&gt;\n1 New South Wales              (((150.7016 -35.12286, 150.6611 -35.11782, 150.6…\n2 Victoria                     (((146.6196 -38.70196, 146.6721 -38.70259, 146.6…\n3 Queensland                   (((148.8473 -20.3457, 148.8722 -20.37575, 148.85…\n4 South Australia              (((137.3481 -34.48242, 137.3749 -34.46885, 137.3…\n5 Western Australia            (((126.3868 -14.01168, 126.3625 -13.98264, 126.3…\n6 Tasmania                     (((147.8397 -40.29844, 147.8902 -40.30258, 147.8…\n7 Northern Territory           (((136.3669 -13.84237, 136.3339 -13.83922, 136.3…\n8 Australian Capital Territory (((149.2317 -35.222, 149.2346 -35.24047, 149.271…\n9 Other Territories            (((167.9333 -29.05421, 167.9188 -29.0344, 167.93…\n\n\nThen create a ggplot2 map of Australia:\n\n\nShow code\nggplot2::ggplot(oz_stats)+\n  ggplot2::geom_sf() +\n  ggplot2::coord_sf()\n\n\n\n\n\n\n\n\n\nNext we remove the “Other territories”, and create a multi-polygon data set of Australian Bureau of Statistics.\n\n\nShow code\noz_stats &lt;- ozmaps::ozmap_states %&gt;% \n  dplyr::filter(NAME != \"Other Territories\")\noz_votes &lt;- rmapshaper::ms_simplify(ozmaps::abs_ced)\n\n\nThen create another map of Australian territories:\n\n\nShow code\nggplot2::ggplot()+\n  ggplot2::geom_sf(data = oz_stats, mapping = ggplot2::aes(fill = NAME)) +\n  ggplot2::geom_sf(data = oz_votes, fill = NA) +\n  ggplot2::coord_sf()\n\n\n\n\n\n\n\n\n\n\n\n9. Plotly\nAn interactive graph of the iris data:\n\n\nShow code\nfig &lt;- plotly::plot_ly(\n  data = iris, \n  x = ~Sepal.Length, \n  y = ~Petal.Length)\nfig\n\n\n\n\n\n\nAn interactive plot of the cars data set:\n\n\nShow code\nmpg %&gt;% plotly::plot_ly(x = ~displ, y = ~mpg, color = ~class)\n\n\n\n\n\n\nNote: you can double click on the legend to see a subset of the data.\nAnother plot with ggplot2:\n\n\nShow code\nplot &lt;- ggplot2::ggplot(mpg, ggplot2::aes(x = displ, y = hwy)) + \n  ggplot2::geom_point(mapping = aes(color = class)) +\n  ggplot2::geom_smooth()\nplot\n\n\n\n\n\n\n\n\n\nUse ggplotly to make it interactive:\n\n\nShow code\n#plotly::ggplotly(plot)\n\n\n\n\n10. Art\n\n\nShow code\nshades &lt;- c(\"blue\", \"green\", \"red\", \"orange\")\ndata &lt;- flametree::flametree_grow(time = 10, trees = 10)\ndata %&gt;% flametree::flametree_plot(\n  background = \"white\",\n  palette = shades,\n  style = \"nativeflora\"\n)\n\n\n\n\n\n\n\n\n\nPackage by Danielle Navarro. Check out her art."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240522-Original-TestDataGrantClaims/index.html",
    "href": "blog/Technical-Blog/posts/20240522-Original-TestDataGrantClaims/index.html",
    "title": "Creating Test Data for Grants Claim Submissions",
    "section": "",
    "text": "In this blog post I outline the steps I took to create a test data set to verify the functionality of a tracking system for the Oregon Department of Education (ODE)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240522-Original-TestDataGrantClaims/index.html#reactable",
    "href": "blog/Technical-Blog/posts/20240522-Original-TestDataGrantClaims/index.html#reactable",
    "title": "Creating Test Data for Grants Claim Submissions",
    "section": "Reactable",
    "text": "Reactable\nLastly lets look at this data in a reactable table.\n\nContentsConditional Formatting FuncitonsFormat ReactablePrint Reactable\n\n\n\nConditional Formatting Functions\nFormat Reactable\nPrint Reactable\n\n\n\n\n\nHide code\n# status colors\nstatus_color &lt;- function(status) {\n  if (status == \"Approved\") {\n    return(\"green\")\n  } else if (status == \"Pending Review\") {\n    return(\"yellow\")\n  } else if (status == \"Needs Revision\") {\n    return(\"red\")\n  } else {\n    return(\"white\")\n  }\n}\n# issue colors\nissue_color &lt;- function(issue) {\n  if (issue == \"No issues\") {\n    return(\"green\")\n  } else if (issue == \"Minor discrepancies\") {\n    return(\"yellow\")\n  } else if (issue == \"Major discrepancies\") {\n    return(\"red\")\n  } else {\n    return(\"white\")\n  }\n}\n\n\n\n\n\n\nHide code\ntest_reactable &lt;- reactable::reactable(\n  test,\n  # Define Column Formatting\n  columns = list(\n    GranteeName = colDef(name = \"Grantee Name\"),\n    LeadContact = colDef(show = F),\n    GranteeType = colDef(name = \"Grantee Type\"),\n    SubmissionDate = colDef(name = \"Submission Date\"),\n    Cycle = colDef(name = \"Submission Cycle\"),\n    Amount = colDef(\n      name = \"Total Claim Amount\",\n      format = colFormat(\n        currency = \"USD\",\n        digits = 2,\n        separators = TRUE)\n      ),\n    ClaimStatus = colDef(\n      name = \"Claim Status\",\n      style = function(value) {\n        list(background = status_color(value))}),\n    ClaimIssues = colDef(\n      name = \"Claim Issues\",\n      style = function(value) {\n        list(background = issue_color(value))}),\n    Contact = colDef(show = F)\n    ),\n  # Define Table Formatting\n  bordered = TRUE,\n  highlight = TRUE,\n  showPageSizeOptions = TRUE,\n  pageSizeOptions = c(5, 10, 20, 50),\n  defaultPageSize = 5)\n\n\n\n\n\n\nHide code\ntest_reactable"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html",
    "title": "Midterm Prep. Group Theory",
    "section": "",
    "text": "Notes consist of Sets, Subsets, Operations, Group, Abelian Group, Subgroups, and Homework 1 and 2 questions."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#examples",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#examples",
    "title": "Midterm Prep. Group Theory",
    "section": "Examples",
    "text": "Examples\n\n\\(\\mathbb{N}=\\{1,2,3,...\\}\\) : they exist naturally\n\\(\\mathbb{Z}=\\{...,-3,-2,-1,0,1,2,3,...\\}\\) : includes zero and negatives\n\\(\\mathbb{Q}=\\{\\frac{m}{n}|m,n\\in \\mathbb{Z}\\text{ and }n\\ne 0\\}\\) : integer fractions\n\\(\\mathbb{R}\\) : includes square roots and pie, real analysis starts with \\(\\sqrt{2}\\)\n\\(\\mathbb{C}\\) : includes imaginary numbers\nAsterisk in the superscript means delete zero\nPlus sign in the superscript means only positive values (&gt;0)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#properties",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#properties",
    "title": "Midterm Prep. Group Theory",
    "section": "Properties",
    "text": "Properties\n\n* is commutative if \\(a\\ne b\\), \\(a*b=b*a\\) \\(\\forall\\) a,b \\(\\in A\\).\n\n+ and \\(\\cdot\\) are commutative\n- , \\(\\div\\) , funtion composition and matrix multiplication are not commutative\n\n* is associative if \\((a*b)*c=a*(b*c)\\) \\(\\forall\\) a,b,c, \\(\\in A\\).\n\naddition is associative, subtraction is not associative\n\nIf \\(\\exists\\) \\(e\\in A\\) \\(\\Rightarrow\\) \\(e*a=a*e=a\\) \\(\\forall\\) \\(a\\in A\\), the we call e the identity element in A w.r.t. *.\n\n0=e w.r.t. addition\n1=e w.r.t. multiplication\n\nIf \\(e\\in A\\) is the identity w.r.t. * and \\(a,b\\in A\\Rightarrow\\) \\(a*b=b*a=e\\) we call a and b inverses of one another.\n\nthe inverse of \\(a\\in \\mathbb{R}\\) w.r.t. addition is -a since \\(a+(-a)=(-a)+a=0\\)\nthe inverse of \\(a\\in\\mathbb{R}^*\\) w.r.t. multiplication is \\(\\frac{1}{a}\\) since \\(a(\\frac{1}{a})=(\\frac{1}{a})a=1\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#proof-outlines",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#proof-outlines",
    "title": "Midterm Prep. Group Theory",
    "section": "Proof Outlines",
    "text": "Proof Outlines\n\nCommutative:\n\\(\\underline{\\text{No}}\\): Give an example, “Let a = 1, and b =2”, and then show \\(a*b\\ne b*a\\).\n\\(\\underline{\\text{Yes}}\\): “For any a,b in the set” and then show \\(a*b=b*a\\).\nAssociative\n\\(\\underline{\\text{No}}\\): Give an example, “Let a=1, b=2, c=3” and then show \\(a*(b*c)\\ne (a*b)*c\\).\n\\(\\underline{\\text{Yes}}\\): “For any a,b,c in the set” and then show \\(a*(b*c)=(a*b)*c\\).\nIdentity\n\\(\\underline{\\text{No}}\\): Suppose that \\(e\\in\\) the given set \\(\\Rightarrow\\) \\(a*e=a\\) \\(\\forall a\\in\\) the given set. Then show \\(a*e=a\\) by plugging e in for b and solving for e. “Since the identity element must be a constant then there is no identiy w.r.t.” the given set. (can’t involve variables)\n\\(\\underline{\\text{Yes}}\\): State what the identity element is w.r.t. the orperation and show that \\(a*e=a\\) and \\(e*a=a\\).\nInverses\n\\(\\underline{\\text{No}}\\): Given an example of an element who doesn’t have an inverse.\nNote: If there is not identity element then there is no inverse.\n\\(\\underline{\\text{Yes}}\\): “Suppose b=\\(a^{-1}\\). Then \\(a*b=e\\)” (where e is the identity found in 3), and then try to solve the eqation for b. Then check \\(b*a=e\\) as well.\nNote: Do not need to check \\(b*a=e\\) if we know * is commutative."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#proposition-1",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#proposition-1",
    "title": "Midterm Prep. Group Theory",
    "section": "Proposition 1",
    "text": "Proposition 1\nLet G be a group, then G has exactly one identity element."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#proposition-2",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#proposition-2",
    "title": "Midterm Prep. Group Theory",
    "section": "Proposition 2",
    "text": "Proposition 2\nEvery element of G has exactly one inverse."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#theorem-1-cancellation-law",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#theorem-1-cancellation-law",
    "title": "Midterm Prep. Group Theory",
    "section": "Theorem 1 (Cancellation Law)",
    "text": "Theorem 1 (Cancellation Law)\nLet G be a group and let \\(a,b,c\\in G\\), then \\(ab=ac\\Rightarrow b=c\\) and \\(ba=ca\\Rightarrow b=c\\)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#theorem-2",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#theorem-2",
    "title": "Midterm Prep. Group Theory",
    "section": "Theorem 2",
    "text": "Theorem 2\nLet G be a group and let \\(a,b\\in G\\). If \\(ab=e\\), then a and b are inverses, i.e. \\(a=b^{-1}\\) and \\(b=a^{-1}\\)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#theorem-3",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#theorem-3",
    "title": "Midterm Prep. Group Theory",
    "section": "Theorem 3",
    "text": "Theorem 3\nLet G be a group and let \\(a,b\\in G\\) then \\((ab)^{-1}=b^{-1}a^{-1}\\) and \\((a^{-1})^{-1}=a\\).\n\nto show a and b are inverses, show their product is e."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#klein-4-group",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#klein-4-group",
    "title": "Midterm Prep. Group Theory",
    "section": "Klein 4 Group",
    "text": "Klein 4 Group\n(for fintie groups) : \\(a^2=b^2=c^2=e\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#example",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#example",
    "title": "Midterm Prep. Group Theory",
    "section": "Example",
    "text": "Example\nAddition:\n\n\\(0\\in H\\).\n\\(\\forall\\) a,b \\(\\in H\\) \\(a+b=H\\).\n\\(\\forall\\) \\(a\\in H\\) , \\(-a\\in H\\)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#proof-outlines-two-step-subgroup-test",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#proof-outlines-two-step-subgroup-test",
    "title": "Midterm Prep. Group Theory",
    "section": "Proof Outlines (Two-Step Subgroup Test)",
    "text": "Proof Outlines (Two-Step Subgroup Test)\n\n\\(e\\in H\\)\nfor all a,b \\(\\in H\\), \\(ab^{-1}\\in H\\).\n\nProve something is a subgroup of G.\n\n“Suppose” then show e is 0 or 1 for the subgroup in a short series of equalities, “the additive or multiplicative element 0 or 1 is in” the subgroup.\n“Now take any a,b \\(\\in\\) the subgroup.” Then define a and b potentially for some other integers. Then show \\(ab^{-1}=\\) something identifiable in the subgroup.\n\n“Therefore we’ve shown that” our subgroup “contains the identity element and for all a,b \\(\\in H\\) , \\(ab^{-1}\\in\\)” our subgroup. Thus our subgroup is a subgroup of G."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#example-1",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#example-1",
    "title": "Midterm Prep. Group Theory",
    "section": "Example",
    "text": "Example\nAddition:\n\n\\(0\\in H\\).\nfor all \\(a,b\\in H\\), \\(a+(-b)=a-b\\in H\\)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#added-notes",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#added-notes",
    "title": "Midterm Prep. Group Theory",
    "section": "Added Notes",
    "text": "Added Notes\n\nIf G is abelian, then \\((ab)^n=a^nb^n\\) for all \\(n\\in \\mathbb{N}\\).\nIn any group \\((a^{-1})^n=(a^n)^{-1}\\) for all \\(n\\in \\mathbb{N}\\).\nIn any group \\(e^{-1}=e\\)."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#two-step-subgroup-test",
    "href": "blog/Technical-Blog/posts/20220328-School-MidtermPrepGroupTheory/index.html#two-step-subgroup-test",
    "title": "Midterm Prep. Group Theory",
    "section": "Two-step subgroup test",
    "text": "Two-step subgroup test\nLet G be a group. A subset H \\(\\subseteq G\\) is a subgroup of G if\n\n\\(e\\in H\\)\n\n\naddition: show 0 \\(\\in H\\)\n\n\n\\(\\forall a,b,\\in H\\), \\(ab^{-1}\\in H\\).\n\n\naddition: \\(\\forall\\) a,b \\(\\in H\\), \\(a+(-b)=a-b\\in H\\)"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240212-Ready4R-crosstables/index.html",
    "href": "blog/Technical-Blog/posts/20240212-Ready4R-crosstables/index.html",
    "title": "Ready4R: Crosstables",
    "section": "",
    "text": "This week the Ready4R mailing list focus on crosstables, also known as two-way tables.\n\n\n\nA crosstable of manufacturers cereals with mascots.\n\n\n\nIntroduction\nReady4R is a mailing list offering a free online course initiated by local Oregonian Ted Laderas to impart foundational knowledge in rstats and the tidyverse. Subscribers receive a weekly email delving into various methods for data exploration and analysis. On a weekly basis, I will look into these examples, providing additional insights based on my own experiences.\n\n\nCrosstables\nCrosstables are a helpful way to compare the results of two or more variables in a table, so that we can start asking questions about how they relate. We start this week using the same cereals dataset, but slightly modifying it.\n\n\nShow code\n# Install Package\n# Load Packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(janitor)\n# Load Data\ncereals &lt;- readr::read_csv(\"../../../../data/cereal.csv\") |&gt;\n  # clean names by converting to lowercase, replace spaces \n  # with underscore and removes special characters\n  janitor::clean_names() |&gt;\n  # make shelf an ordered factor\n  dplyr::mutate(shelf = factor(shelf, ordered = TRUE)) |&gt;\n  # convert mfr and type columns to categorical data\n  dplyr::mutate(across(c(\"mfr\", \"type\"), as.factor))\n# Rename Labels\nmanu_labels &lt;- c(\"American Home\"=\"A\",\n                   \"General Mills\"=\"G\",\n                   \"Kelloggs\"=\"K\",\n                   \"Nabisco\" = \"N\",\n                   \"Post\" = \"P\",\n                   \"Quaker Oats\" = \"Q\", \n                   \"Ralston Purina\" = \"R\")\ncereals &lt;- cereals |&gt;\n  dplyr::mutate(mfr = forcats::fct_recode(mfr, !!!manu_labels))\n\n\n\n\nCrosstabs with janitor::tabyl()\nFor a single variable we can use janitor::tabyl() to view counts and percentages, as shown below:\n\n\nShow code\ncereals |&gt;\n  janitor::tabyl(shelf) \n\n\n shelf  n   percent\n     1 20 0.2597403\n     2 21 0.2727273\n     3 36 0.4675325\n\n\nA slick way to get counts and percentage per shelf (AKA our ordered factor variable). Next we may want to know whether the manufacturers (factor variable) are evenly distributed in terms of cereal type (factor variable). Note: Use adorn_percentages() and adorn_n() interchangeably to get percentages or counts.\n\n\nShow code\ncereals |&gt;\n  # Create frequency table of manufacturers by type\n  janitor::tabyl(mfr, type) |&gt;\n  # Calculate percentages based on hot and cold cereal totals\n  janitor::adorn_percentages(denominator = \"row\") |&gt;\n  # Format percentages\n  janitor::adorn_pct_formatting() |&gt;\n  # Add totals\n  janitor::adorn_ns() |&gt;\n  # Nicely format table\n  knitr::kable()\n\n\n\n\n\nmfr\nC\nH\n\n\n\n\nAmerican Home\n0.0% (0)\n100.0% (1)\n\n\nGeneral Mills\n100.0% (22)\n0.0% (0)\n\n\nKelloggs\n100.0% (23)\n0.0% (0)\n\n\nNabisco\n83.3% (5)\n16.7% (1)\n\n\nPost\n100.0% (9)\n0.0% (0)\n\n\nQuaker Oats\n87.5% (7)\n12.5% (1)\n\n\nRalston Purina\n100.0% (8)\n0.0% (0)\n\n\n\n\n\n\\(\\underline{\\textbf{Insights}}\\)\n\nOnly 1 manufacturer makes hots cereals: American Home\nOnly 2 manufacturers make both hot and cold cereal: Nabisco and Quaker Oats\nThe remaining 4 manufacturers only make cold cereals: General Mills, Kelloggs, Post, and Raiston Purina\n\n\n\nShelf Height and Marketing\nNow let’s look at the distribution of cereals across different shelf heights (1 = bottom, 2 = middle, and 3 = top). According to an article cited by Ted titled Cereal aisle psychology: All eyes on the consumer, researchers conducted a two-part study confirming that cereals targeting children are typically placed about 23 inches off the ground. They observed that cereals on the top shelves often feature characters staring straight ahead or slightly upward to make eye contact with adults, while those on lower shelves, adorned with cartoon characters with large inviting eyes, create eye contact with children.\nLets use crosstables to visualize how manufacturers distribute their cereals across shelves.\n\n\nShow code\ncereals |&gt;\n  # Create frequency table of manufacturers by shelf\n  janitor::tabyl(mfr, shelf) |&gt;\n  # Calculate percentages based on shelf total\n  janitor::adorn_percentages(denominator=\"row\") |&gt;\n  # Format percentages\n  janitor::adorn_pct_formatting() |&gt;\n  # Add totals\n  janitor::adorn_ns() |&gt;\n  # Nicely format table\n  knitr::kable()\n\n\n\n\n\nmfr\n1\n2\n3\n\n\n\n\nAmerican Home\n0.0% (0)\n100.0% (1)\n0.0% (0)\n\n\nGeneral Mills\n27.3% (6)\n31.8% (7)\n40.9% (9)\n\n\nKelloggs\n17.4% (4)\n30.4% (7)\n52.2% (12)\n\n\nNabisco\n50.0% (3)\n33.3% (2)\n16.7% (1)\n\n\nPost\n22.2% (2)\n11.1% (1)\n66.7% (6)\n\n\nQuaker Oats\n12.5% (1)\n37.5% (3)\n50.0% (4)\n\n\nRalston Purina\n50.0% (4)\n0.0% (0)\n50.0% (4)\n\n\n\n\n\n\\(\\underline{\\textbf{Insights}}\\)\n\nAmerican Home exclusively markets its cereals on the second shelf.\nNabisco primarily markets its cereals on the bottom shelf, presumably targeting children.\nGeneral Mills, Kelloggs, Post, and Quaker Oats mainly target adults, as they primarily market their cereals on the top shelf..\nRalston Purina evenly splits its marketing efforts between the top and bottom shelves.\n\n\n\nWhich shelves have cereal mascots?\nNext, let’s investigate which shelves feature cereal mascots, assuming that mascots often indicate cereals marketed towards children. Using data scraped by Ted from the article “268 Cereal Mascots,” we can identify cereals with mascots and their distribution across shelves.\nBelow we clean and join our data sets, and then check the dimmensions are correct.\n\n\nShow code\n# Load Data\nm_c &lt;- utils::read.csv(\"../../../../data/mascots.csv\",\n                       row.names = NULL)\n# Clean cereal names\nm_c &lt;- m_c |&gt;\nmutate(name=stringr::str_replace(name,\"Cap’n Crunch cereals\", \"Cap'n'Crunch\")) |&gt;\n  mutate(name=stringr::str_replace(name, \"Count Chocula cereal\", \"Count Chocula\")) |&gt;\n  mutate(name=stringr::str_replace(name, \"Honey Smacks\", \"Smacks\")) |&gt;\n  mutate(name=stringr::str_replace(name, \"Mini-Wheats\", \"Frosted Mini-Wheats\")) \n# join with cereals data\nmascot_count &lt;- cereals |&gt;\n  left_join(y=m_c, by=\"name\") |&gt;\n  mutate(has_mascot = ifelse(is.na(mascot), \"No\", \"Yes\")) \n# check dimension\ndim(mascot_count)\n\n\n[1] 80 19\n\n\nLet’s do a Quick Check!, and look at the second shelf to see any obvious errors in the data.\n\n\nShow code\nmascot_count |&gt;\n  # Filter to second shelf\n  dplyr::filter(shelf==2) |&gt;\n  # Select only name, manufacturer, mascot, and has_mascot\n  select(name, mfr, mascot, has_mascot) |&gt;\n  # Arrange by mascot \n  arrange(mascot) |&gt;\n  # Nicely format table\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nname\nmfr\nmascot\nhas_mascot\n\n\n\n\nCocoa Puffs\nGeneral Mills\nBuzz the Bee\nYes\n\n\nCap’n’Crunch\nQuaker Oats\nCap’n Crunch (Horatio Magellan Crunch)\nYes\n\n\nCocoa Puffs\nGeneral Mills\nCocoa Puffs’ Sheik of Shake\nYes\n\n\nCorn Pops\nKelloggs\nCornelius (Corny) the Corn\nYes\n\n\nCount Chocula\nGeneral Mills\nCount Chocula\nYes\n\n\nSmacks\nKelloggs\nDig’em Frog\nYes\n\n\nFruity Pebbles\nPost\nFred Flintstone\nYes\n\n\nLucky Charms\nGeneral Mills\nLucky the Leprechaun\nYes\n\n\nFrosted Mini-Wheats\nKelloggs\nMr. Mini-Wheats\nYes\n\n\nCocoa Puffs\nGeneral Mills\nSonny the Cuckoo Bird\nYes\n\n\nRaisin Bran\nKelloggs\nThe Raisin Bran Sun\nYes\n\n\nFroot Loops\nKelloggs\nToucan Sam\nYes\n\n\nTrix\nGeneral Mills\nTrix Rabbit\nYes\n\n\nCinnamon Toast Crunch\nGeneral Mills\nWendell\nYes\n\n\nApple Jacks\nKelloggs\nNA\nNo\n\n\nCream of Wheat (Quick)\nNabisco\nNA\nNo\n\n\nGolden Grahams\nGeneral Mills\nNA\nNo\n\n\nHoney Graham Ohs\nQuaker Oats\nNA\nNo\n\n\nKix\nGeneral Mills\nNA\nNo\n\n\nLife\nQuaker Oats\nNA\nNo\n\n\nMaypo\nAmerican Home\nNA\nNo\n\n\nNut&Honey Crunch\nKelloggs\nNA\nNo\n\n\nStrawberry Fruit Wheats\nNabisco\nNA\nNo\n\n\n\n\n\nTed notes that there are duplicate rows for cereals, because mascots can change over time, or have multiple mascots. This will be problematic for our crosstable which works on counts, so we do a little extra cleaning in the code below.\n\n\nShow code\nmascot_count |&gt;\n  # Select name, shelf, and has_mascot columns\n  select(name, shelf, has_mascot) |&gt;\n  # Remove the duplicate rows\n  distinct() |&gt;\n  # Create crosstable of shelfs by has_mascot\n  janitor::tabyl(shelf, has_mascot) |&gt;\n  # Calculate percentage based on Yes or No to having a mascot\n  janitor::adorn_percentages() |&gt;\n  # Format percentages\n  janitor::adorn_pct_formatting() |&gt;\n  # Add totals\n  janitor::adorn_ns() |&gt;\n  # Nicely format table\n  knitr::kable()\n\n\n\n\n\nshelf\nNo\nYes\n\n\n\n\n1\n60.0% (12)\n40.0% (8)\n\n\n2\n42.9% (9)\n57.1% (12)\n\n\n3\n100.0% (36)\n0.0% (0)\n\n\n\n\n\n\\(\\underline{\\textbf{Insights}}\\)\n\nCereals with mascots are absent from the top shelf, suggesting that cereal manufacturers predominately target children on the first and second shelves.\n\nFinally, let’s identify which manufacturer boasts the highest number of cereals with mascots:\n\n\nShow code\nmascot_count |&gt;\n  # Select name, manufacturer, and has_mascot\n  select(name, mfr, has_mascot) |&gt;\n  # Remove the duplicate rows\n  distinct() |&gt;\n  # Create crosstable of shelfs by has_mascot\n  janitor::tabyl(mfr, has_mascot) |&gt;\n  # Calculate percentages based on column totals\n  janitor::adorn_percentages(denominator = \"col\") |&gt;\n  # Format percentages\n  janitor::adorn_pct_formatting() |&gt;\n  # Add totals\n  janitor::adorn_ns() |&gt;\n  # Nicely format table\n  knitr::kable()\n\n\n\n\n\nmfr\nNo\nYes\n\n\n\n\nAmerican Home\n1.8% (1)\n0.0% (0)\n\n\nGeneral Mills\n24.6% (14)\n40.0% (8)\n\n\nKelloggs\n26.3% (15)\n40.0% (8)\n\n\nNabisco\n10.5% (6)\n0.0% (0)\n\n\nPost\n12.3% (7)\n10.0% (2)\n\n\nQuaker Oats\n12.3% (7)\n5.0% (1)\n\n\nRalston Purina\n12.3% (7)\n5.0% (1)\n\n\n\n\n\n\\(\\underline{\\textbf{Insights}}\\)\n\nGeneral Mills and Kelloggs lead with the most cereals featuring mascots.\nAmerican Home and Nabisco have no cereals with mascots.\n\n\n\nOverall\nCrosstables serves as a powerful tool for exploring relationships within datasets. Through these analyses, we gain valuable insights into how cereal manufacturers strategically position their products, offering a glimpse into the fascinating world of consumer psychology and marketing dynamics.\n\n\nW.E.B. Dubois’ Visualizations (Black History Month)\nBorn in 1868, Du Bois was not only a scholar and activist but also an innovative thinker in the realm of data representation. As the first Black American to earn a doctorate from Harvard, he embarked on a journey through Europe before returning to the United States to focus on social sciences.\nDriven by a fervent desire to elevate the rights and livelihoods of Black people, Du Bois recognized the power of compelling evidence in effecting societal change. However, he understood that mere statistical data alone could not dismantle generations of racial oppression.\nOne of his seminal works emerged during the 1900 Paris Exposition, where Du Bois seized the opportunity to narrate the story of Black Americans through a novel medium: charts. His collection of 63 data visualizations titled “The Exhibit of American Negroes” was divided into two sections. The first, “A Series of Statistical Charts Illustrating the Condition of the Descendants of Former African Slaves Now in Residence in the United States of America,” offered a broad view of the data at national and international levels. The companion piece, “The Georgia Negro,” focused on a singular state, providing a localized perspective.\nDu Bois meticulously organized his charts into three distinct viewpoints—international, national, and local—thereby weaving a comprehensive narrative encompassing African American history, education, business development, and property ownership. Through interconnected data points, he crafted an elaborate story that evolved seamlessly from the local to the international arena, reinforcing the narrative of Black empowerment and resilience.\n\\(\\underline{\\textbf{Articles}}\\)\n\nHow W.E.B. Du Bois used data visualization to confront prejudice in the early 20th century by Jason Forrest\nW.E.B. Du Bois Portrait Gallary by Chimdi Niwosu"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20210830-R4DS-RforDataScienceCh3/index.html",
    "href": "blog/Technical-Blog/posts/20210830-R4DS-RforDataScienceCh3/index.html",
    "title": "R for Data Science - Ch.3: Data Visualisations",
    "section": "",
    "text": "These are examples and exercises from Chapter 3 of R for Data Science, by Hadley Wickham and Garret Grolemund.\n\n1. Set Up\nThis first chunk will remove warning messages from all chunks in this file. To hide this chunk use include=FALSE within the {} brackets.\n\n\nShow code\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \n\n\nThis second chunk calls two packages:\n\ntidyverse: to tidy data and create visuals with ggplot2.\ngridExtra: to arrange data in a grid\n\n\n\nShow code\nlibrary(tidyverse)\nlibrary(gridExtra)\n\n\nThis chapter analyzes the mpg data so I’m using the head() function from utils to view the first five rows in the mpg data set.\n\n\nShow code\nutils::head(mpg, 5)\n\n\n# A tibble: 5 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n\n\n\n\n2. Visuals\n\\(\\underline{\\text{Question 1}}\\): Do cars with big engines use more fuel than cars with small engines?\nTo answer this question I will focus on two columns:\ndispl : a cars engine size in litres\nhwy : a car’s fuel efficiency on the highway in mpg.\n\n\nShow code\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy))\n\n\n\n\n\n\n\n\n\nNote:\n\nThere is a negative relation between engine size and fuel efficiency.\nThe mapping argument is always paired with aes(), and the x and y arguments of aes() specify which variables to map to the x and y axes.\n\n\n\n3. 3.2.4 Exercises\n\nThe following code chunk creates an empty square.\n\n\n\nShow code\nggplot2::ggplot(data = mpg)\n\n\n\n\n\n\n\n\n\n\nThe mpg data set has 234 rows and 11 columns.\nThe drv variable is the type of drive the car has such as f = front wheel, r = rear wheel, and 4 = 4 wheel drive.\nThe following plot shows hwy vs. cyl.\n\n\n\nShow code\nggplot2::ggplot(mpg) +\n  ggplot2::geom_point(ggplot2::aes(x = cyl, y = hwy))\n\n\n\n\n\n\n\n\n\nNote: This isn’t very useful because it is obvious that as the number of cylinders increases the miles per gallon decreases.\n\nThe following plot shows class vs. drv.\n\n\n\nShow code\nggplot2::ggplot(mpg) +\n  ggplot2::geom_point(ggplot2::aes(x = drv, y = class))\n\n\n\n\n\n\n\n\n\nNote: This plot isn’t useful because there are no obvious trends. Categorical variables usually have a small number of values they are limited to, so it only seems like there are 12 observed values.\n\n\n4. Aesthetics\nWithin the aes() function when specifying that color is equal to a column variable then ggplot will add a color key to these variables, as shown below.\n\n\nShow code\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, color = class))\n\n\n\n\n\n\n\n\n\nWhen defining color outside aes() then color is equal to a specific color (such as red or blue), and ggplot will make all points that one color, as shown below.\n\n\nShow code\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy), color = \"blue\")\n\n\n\n\n\n\n\n\n\nsize:\n\n\nShow code\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, size = class))\n\n\n\n\n\n\n\n\n\n(Warning: using size for a discrete variable is not advised.)\nalpha: (transparency)\n\n\nShow code\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, alpha = class))\n\n\n\n\n\n\n\n\n\nshape:\n\n\nShow code\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, shape = class))\n\n\n\n\n\n\n\n\n\n\n\n\nshapes built into R\n\n\n\n\n5. 3.3.1 Exercises\n\nThe following code is incorrect because color is inside aes(), which is labeling all the points as “blue”.\n\n\n\nShow code\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, color = \"blue\"))\n\n\n\n\n\n\n\n\n\n\nCategorical : manufacturer, model name, trans, drv, fl, and class  Continuous : displ, cty, year of manufacture, number of cylinders, and hwy\n\n\nNotice in the printed data frame the categorical variables are usually character  values, where continuous variables are numeric values such as  or .\n\n\nWhen mapping a continuous variable to an aes() such as color then then there the key also becomes continuos as shown below.\n\n\n\nShow code\n# Categorical\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = manufacturer, color = trans))\n\n\n\n\n\n\n\n\n\nShow code\n# Continuous\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = manufacturer, color = hwy))\n\n\n\n\n\n\n\n\n\n\nWhen mapping the same variable to multiple aesthetics then multiple keys are added as shown below.\n\n\n\nShow code\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, size = hwy, color = displ))\n\n\n\n\n\n\n\n\n\n\nStroke adjusts the thickness of the boarder (for shapes 21-25) as shown below.\n\n\n\nShow code\nggplot2::ggplot(mtcars, ggplot2::aes(wt, mpg)) +\n  ggplot2::geom_point(shape = 21, colour = \"black\", fill = \"pink\", size = 5, stroke = 5)\n\n\n\n\n\n\n\n\n\n\nWhen defining something like color to be displ &lt; 5, it sets up a true or false argument for this, and applies one color (blue) to true values less than 5 and red for false values greater than 5.\n\n\n\nShow code\nggplot2::ggplot(data = mpg) + \n  ggplot2::geom_point(mapping = ggplot2::aes(x = displ, y = hwy, color = displ &lt; 5))\n\n\n\n\n\n\n\n\n\n\n\n6. Facets\nfacet_wrap() should be used for discrete values as shown below:\n\n\nShow code\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n\n\n\n\n\nTo facet on a combination of variables use facet_grid() as shown below:\n\n\nShow code\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\n\n\nUse + facet_grid(.~cyl) to not facet rows.\n\n\nShow code\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_grid(.~ cyl)\n\n\n\n\n\n\n\n\n\n\n\n7. 3.5.1 Exercises\n\nWhen you facet a continuous variable you make A LOT of graphs.\n\n\n\nShow code\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ hwy)\n\n\n\n\n\n\n\n\n\n\nThe empty cells in the facet_grid(drv ~ cyl) plot above are showing the empty points in the graph below. For example cars with four wheel drive only have an even number of cylinders so the plot of 4 wheel drive with 5 cylinders is empty because it does not exist.\n\n\n\nShow code\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = drv, y = cyl))\n\n\n\n\n\n\n\n\n\n\nOne of the below plots is shown in rows and the other in columns. The period says not to facet the rows or the columns.\n\n\n\nShow code\n# rows\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\n\n\n\n\n\n\n\n\nShow code\n#columns\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(. ~ cyl)\n\n\n\n\n\n\n\n\n\n\nThe advantages of facet wrap allow for data with various classes or types to be analyzed by such. Additionally it’s difficult for humans to visualize a large amount of color so it is easier to digest the variety of date spread out. The disadvantage of this could be that spreading the data out would make it difficult to compare observations between different categories.\n\n\n\nShow code\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nnrow and ncol define the number of rows and columns in the facet wrap.There is also scales, shrink, labeler, as.table, switch, drop, dir, and stip.position. Facet grid doesn’t have these because it is specified in the function instead.\nVariables with more unique levels should be in columns when using facet_grid() because there is more space for columns if the plot is laid out horizontally.\n\n\n\n8. Geometric Objects\nThe side by side graphs below show the same data. The left graph uses the geometric object geom_point() which shows all the points, and the right graphs uses geom_smooth() which creates a best fit line with the data’s standard error without all the data points.\n\n\nShow code\n# left graph: geom_point()\na &lt;- ggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\n# right graph: geom_smooth()\nb &lt;- ggplot(data = mpg) + \n  geom_smooth(mapping = aes(x = displ, y = hwy))\n# both together\ngrid.arrange(a,b, nrow = 1)\n\n\n\n\n\n\n\n\n\nFor different line “shapes” geom_smooth() can be used with different linetypes within aes() as shown below.\n\n\nShow code\nggplot(data = mpg) + \n  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))\n\n\n\n\n\n\n\n\n\nFor the following geoms, you can set the group aesthetic to a categorical variable to draw multiple objects.\n\n\nShow code\nc &lt;- ggplot(data = mpg) +\n  geom_smooth(mapping = aes(x = displ, y = hwy))\n              \nd &lt;- ggplot(data = mpg) +\n  geom_smooth(mapping = aes(x = displ, y = hwy, group = drv))\n    \ne &lt;- ggplot(data = mpg) +\n  geom_smooth(\n    mapping = aes(x = displ, y = hwy, color = drv),\n    show.legend = FALSE)\ngrid.arrange(c,d,e, nrow = 1)\n\n\n\n\n\n\n\n\n\nBelow multiple geometric objects are added to one plot.\n\n\nShow code\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  geom_smooth(mapping = aes(x = displ, y = hwy))\n\n\n\n\n\n\n\n\n\nDefining the mapping aes() helps reduce repetion, as shown below.\n\n\nShow code\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_smooth()\n\n\n\n\n\n\n\n\n\nGlobal Mapping\n\n\nShow code\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth()\n\n\n\n\n\n\n\n\n\nSubcompact (subset) mapping\n\n\nShow code\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth(data = filter(mpg, class == \"subcompact\"), se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n9. 3.6.1 Exercises\n\nline chart: geom_line()  boxplot: geom_boxplot()  histogram: geom_histogram()  area chart: geom_area()\nPrediction: the below code will show the various points and lines for drv without any standard error.\n\n\n\nShow code\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + \n  geom_point() + \n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\n\nshow.legend = FALSE hides the legend box, and was used earlier in this chapter because it changes the size of the graphs, which would make it more difficult to compare to the other graphs."
  },
  {
    "objectID": "blog/Personal-Blog/posts/202401-Home-MLKStorm/index.html",
    "href": "blog/Personal-Blog/posts/202401-Home-MLKStorm/index.html",
    "title": "2024 January Winter Storm",
    "section": "",
    "text": "In January 2024, Oregon was battered by a formidable storm, leaving a lasting imprint on our community. Months later, the scars of destruction still linger in our neighborhood, evident in the tarps draped over homes awaiting repair from the trees that crashed through them."
  },
  {
    "objectID": "blog/Personal-Blog/posts/202401-Home-MLKStorm/index.html#timeline-of-events",
    "href": "blog/Personal-Blog/posts/202401-Home-MLKStorm/index.html#timeline-of-events",
    "title": "2024 January Winter Storm",
    "section": "Timeline of Events",
    "text": "Timeline of Events\nFriday, Jan. 12th\nA winter storm watch for the weekend warned of cold weather and snow. Despite no snowfall, we erred on the side of caution, rescheduling plans for my partner Tanner’s family birthday dinner.\nSaturday Jan. 13th\nThe day started calmly but turned tumultuous as winds picked up in the afternoon, leading to a power outage by 3 PM. Seeking refuge at our local watering hole, we waited for an update from PGE that wouldn’t come for a few days. Enduring the biting cold again, we walked home, and huddled under blankets with our beloved cat, Lulu (as seen in the picture above).\nSunday Jan. 14th\nWith no updates from the power company, our day was spent navigating the cold reality of a powerless home. Concern for Lulu’s well-being prompted makeshift measures to retain heat, including burning old furniture and fortifying our living room against the chill. The fire went out after midnight, but the room still retained heat more than the other rooms in the house.\nMonday Jan. 15th\nAs another storm loomed on the horizon, we made the decision to seek shelter at Tanner’s mom’s house, braving treacherous roads with our faithful companion, Lulu. The drive marked the first communication from the power company for power to return 10PM Tuesday.\nTuesday Jan. 16th\nDespite the icy roads and uncertainty, we received a belated message of restored power at midnight. A modest birthday gift for Tanner amidst the challenges.\nWednesday Jan. 17th\nEager to return home, we cautiously navigated icy roads, only to be greeted by the aftermath of a flooded bathroom upon arrival. Despite intermittent power, we resolved to stay put, improvising with limited resources. PGE says power should be back on tomorrow night at 10PM.\nThursday Jan. 18th\nTanner ventured out in search of firewood checking multiple locations, only to find some at Lowes. Power was out all day, and did not return until 8pm.\nFriday Jan. 19th\nPower was on for most of the day, but went out twice in the later afternoon. Roads are icy, and un-drivable.\nSaturday - Sunday\nPower stayed on, but roads and sidewalks were still icy."
  },
  {
    "objectID": "blog/Personal-Blog/posts/202401-Home-MLKStorm/index.html#prepared-checklist",
    "href": "blog/Personal-Blog/posts/202401-Home-MLKStorm/index.html#prepared-checklist",
    "title": "2024 January Winter Storm",
    "section": "Prepared Checklist",
    "text": "Prepared Checklist\n\nTurn off water lines.\nCheck travel chargers are fully charged.\nEssential electronics are fully charged.\nSpare batteries.\nSpare candles.\nRemove water from glass.\nPrepare overnight bag for quick exits."
  },
  {
    "objectID": "blog/Personal-Blog/posts/202401-Home-MLKStorm/index.html#bonus-items",
    "href": "blog/Personal-Blog/posts/202401-Home-MLKStorm/index.html#bonus-items",
    "title": "2024 January Winter Storm",
    "section": "Bonus Items",
    "text": "Bonus Items\n\nCostco generator.\nBatteries for headlamps.\nSpare firewood."
  },
  {
    "objectID": "blog/Personal-Blog/posts/202401-Home-MLKStorm/index.html#additional-articles",
    "href": "blog/Personal-Blog/posts/202401-Home-MLKStorm/index.html#additional-articles",
    "title": "2024 January Winter Storm",
    "section": "Additional Articles",
    "text": "Additional Articles\n\nOEM - 2024 January winter Storm Spotlight\nTanner Heffner - lessons from the storm\nPortland.gov - Dedicated team, community support help Portland weather a winter storm to start 2024"
  },
  {
    "objectID": "blog/Personal-Blog/posts/202405-Garden-MayUpdate/index.html",
    "href": "blog/Personal-Blog/posts/202405-Garden-MayUpdate/index.html",
    "title": "Garden Update",
    "section": "",
    "text": "Here is a bunch of pictures from my garden, May 2024.\n\n\n\n\nHot Pink Rose\n\n\n\n\n\nRed Rose\n\n\n\n\n\nWhite Rose\n\n\n\n\n\nLight Pink Roses\n\n\n\n\n\nRed Roses\n\n\n\n\n\nYellow Rose\n\n\n\n\n\nCherries\n\n\n\n\n\nCalla Lilies\n\n\n\n\n\nSage and my shadow (for size?)\n\n\n\n\n\nSage\n\n\n\n\n\nKiwi\n\n\n\n\n\nCherry Tree by Cement Pad"
  },
  {
    "objectID": "blog/Personal-Blog/blog.html",
    "href": "blog/Personal-Blog/blog.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "Garden Update\n\n\n\n\n\n\nGarden\n\n\n\n\n\n\n\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n2024 January Winter Storm\n\n\n\n\n\n\nStorm Preparedness\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html",
    "title": "Book Review: The Only Good Indians",
    "section": "",
    "text": "Rating: 10/10  Overview: Gripping horror intertwining four Native American men’s lives with their chilling past and its haunting repercussions."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#introduction-to-horror",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#introduction-to-horror",
    "title": "Book Review: The Only Good Indians",
    "section": "1.1. Introduction to Horror",
    "text": "1.1. Introduction to Horror\nThis was my first foray into horror novels. I chose “The Only Good Indians” specifically because I was unfamiliar with it, hoping to be genuinely surprised by the story’s twists. And indeed, I was not disappointed.\nFrom the first chapter, the novel sets a brisk pace and a tone that blends the ordinary with a sense of impending dread. The graphic content, while occasionally shocking, serves to intensify the atmosphere without overshadowing the plot. There’s a rich layer of mystery throughout the book, which I found thoroughly engaging and well worth the venture into this new genre."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#cultural-identity-and-heritage",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#cultural-identity-and-heritage",
    "title": "Book Review: The Only Good Indians",
    "section": "1.2. Cultural Identity and Heritage",
    "text": "1.2. Cultural Identity and Heritage\nThe novel is not only a narrative about personal survival but also a profound exploration of the complexities of cultural identity. Jones showcases how traditions and ancient beliefs can persist in contemporary life, influencing the characters’ actions, their worldviews, and their interactions with both the spiritual and the physical worlds.\nThe heritage of the characters is not merely a backdrop but acts as a pivotal force in driving the plot and deepening the horror elements of the story. Through vivid descriptions and culturally specific details, Jones brings to life the distinct experiences of his characters, providing readers with a deeper understanding of their lives and the pressures they face from both within and outside their community.\nFurthermore, the book engages with the idea of legacy—what one generation passes down to the next and how these inheritances can shape lives in ways that are both visible and hidden. The reverence for traditional practices and the struggle with the ghosts of history are woven into the narrative, creating a rich tapestry that highlights the resilience and complexity of Native American life.\nBy embedding these themes in a horror context, Jones not only tells a thrilling story but also invites readers to reflect on the cultural struggles and triumphs of his characters, offering a poignant commentary on identity, community, and the indelible marks of heritage. This approach enriches the reader’s experience, providing not just suspense but a meaningful exploration of what cultural identity means in a modern world fraught with both past traumas and present challenges."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#suvivors-guilt-and-consequences-of-the-past",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#suvivors-guilt-and-consequences-of-the-past",
    "title": "Book Review: The Only Good Indians",
    "section": "1.3. Suvivor’s Guilt and Consequences of the Past",
    "text": "1.3. Suvivor’s Guilt and Consequences of the Past\nThe narrative is heavily centered on the concept of survivor’s guilt, where the characters are not only dealing with the immediate threats that emerge around them but also grappling with the emotional and psychological consequences of events that occurred many years prior. This guilt shapes their decisions, relationships, and perceptions of self, creating a tension that is both internal and external.\nThe consequences of past actions are portrayed as inescapable and lingering, suggesting that the past is never truly past but continues to influence the present in profound and often horrifying ways. The characters find themselves haunted not just by a supernatural presence but by the weight of their own memories and the choices they made. This intersection of personal history and supernatural consequence creates a compelling narrative that questions the possibility of ever truly escaping one’s actions.\nJones uses these themes to explore deeper societal issues as well, reflecting on how historical injustices and personal traumas are interlinked and how they continue to affect individuals and communities. The novel suggests that the horrors of the past are not easily forgotten and can resurface, sometimes monstrously, shaping the lives of those involved for years to come."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#nature-and-supernatural",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#nature-and-supernatural",
    "title": "Book Review: The Only Good Indians",
    "section": "1.4. Nature and Supernatural",
    "text": "1.4. Nature and Supernatural\nJones uses a blend of nature and the supernatural to explore themes of respect, desecration, and reconciliation. The natural world in “The Only Good Indians” is not just a passive setting but a dynamic force that interacts with the characters, influencing their lives and driving the narrative forward. This approach heightens the horror elements of the story, as the environment and its spirits become sources of tension and terror that the characters must navigate.\nThe supernatural occurrences in the novel are deeply tied to these natural settings, suggesting that the land itself holds memories and has the power to manifest them in ways that are both protective and punitive. This connection emphasizes the characters’ respect for and symbiosis with their environment, as well as the consequences when this balance is disrupted."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#socail-commentary",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#socail-commentary",
    "title": "Book Review: The Only Good Indians",
    "section": "1.5. Socail Commentary",
    "text": "1.5. Socail Commentary\nJones’s narrative is not just a horror story but also a poignant examination of the social issues that affect Native American people. Through the lives of his characters, the novel addresses themes such as systemic oppression, cultural erasure, and the struggles with identity that many indigenous people face. These elements are woven into the fabric of the story, reflecting real-world concerns in a manner that is both engaging and enlightening.\nThe horror elements of the book often serve as metaphors for the larger, more insidious fears that these communities contend with—such as the loss of cultural identity and the ongoing effects of historical traumas. The supernatural occurrences in the novel can be seen as manifestations of these larger societal issues, making the horror feel both immediate and deeply symbolic.\nJones also touches on resilience and the importance of community solidarity in facing these challenges. The characters’ responses to the supernatural threats are informed by their cultural heritage and collective experiences, showcasing the strength found in shared histories and traditions."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#the-horror-of-everyday-life",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#the-horror-of-everyday-life",
    "title": "Book Review: The Only Good Indians",
    "section": "1.6. The Horror of Everyday Life",
    "text": "1.6. The Horror of Everyday Life\nJones cleverly elevates everyday scenarios and settings to create vessels of horror. This technique not only blurs the lines between the ordinary and the supernatural but also heightens the tension and suspense throughout the novel. The familiar becomes sinister, turning the characters’ routine lives into a landscape where horror can emerge at any moment.\nThis theme serves as a powerful metaphor for the internal and external battles the characters face. The everyday struggles of the characters, who deal with issues like identity, cultural heritage, and personal demons, are amplified by the horror elements, suggesting that sometimes the real terror lies in the challenges we encounter daily.\nJones uses this setting to reflect on how the past continually shadows the present, making even simple decisions or encounters fraught with deeper meanings and potential dangers. The ordinary moments are depicted as being just as capable of producing fear and anxiety as the extraordinary ones, which makes the horror in the novel more relatable and unnerving."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#narrative-techniques",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#narrative-techniques",
    "title": "Book Review: The Only Good Indians",
    "section": "1.7. Narrative Techniques",
    "text": "1.7. Narrative Techniques\nListening to “The Only Good Indians” on audiobook provided a uniquely immersive experience, thanks largely to the narrator’s masterful delivery. The narrator’s voice modulation was exceptional, adeptly building suspense and accentuating the story’s eerie atmosphere. Each character was brought to life with distinct vocal nuances, making the narrative easy to follow and deeply engaging. The pacing was particularly effective—swift during climactic scenes to heighten tension, and slower during more reflective moments, allowing the story’s deeper themes to resonate. Furthermore, the narrator’s sensitivity to the cultural elements of the book added a layer of authenticity that enriched my understanding and appreciation of the narrative. Overall, the audiobook version added a compelling auditory dimension to the novel, making my experience both memorable and haunting."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#title-significance",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#title-significance",
    "title": "Book Review: The Only Good Indians",
    "section": "2.1. Title Significance",
    "text": "2.1. Title Significance\nThe title of Stephen Graham Jones’ novel, “The Only Good Indians,” carries profound thematic and symbolic weight, particularly as it echoes the historically violent phrase, “The only good Indian is a dead Indian.” This saying, which became popular in the late 19th century, reflects the brutal attitudes and genocidal sentiments directed toward Native American populations during the westward expansion in the United States. By choosing this title, Jones engages in a subversive act of linguistic reclamation, turning a phrase that epitomizes racial hatred into a critique of those very notions.\nThe novel itself mirrors the violence of this old saying, dealing with both the literal and metaphorical hauntings of the characters by their past actions and the broader historical injustices inflicted upon their ancestors. This reflection is not only about survival but also about the struggles of cultural identity in contemporary society, as the characters grapple with the implications of being considered a “good” Indian.\nThe irony of using such a historically derogatory statement as the title adds a layer of critique; through the stories of the characters, Jones highlights the resilience and humanity of Native American individuals, countering the dehumanizing undertones of the original phrase. The title also prompts readers to consider deeper moral and ethical questions about identity, redemption, and the nature of good and evil, challenging entrenched stereotypes and inviting a reexamination of the complexities that these judgments entail. Through this charged title, Jones ensures that the themes of historical context and contemporary reality are palpable, enriching the narrative to transcend horror and become a poignant commentary on society."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#character-dynamics-and-background",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#character-dynamics-and-background",
    "title": "Book Review: The Only Good Indians",
    "section": "2.2 Character Dynamics and Background",
    "text": "2.2 Character Dynamics and Background\nThis book offers a rich tapestry of character complexity and moral ambiguity, particularly through characters like Gabe and Victor Yellow Tail. These characters are depicted with depth and nuance, navigating a world where choices are often constrained by external forces and internal conflicts. This portrayal effectively showcases their roles as anti-heroes, individuals who grapple with their flaws and the moral complexities of their decisions.\nGabe, one of the four friends central to the story, exemplifies the anti-hero archetype. His life is marked by a blend of good intentions and flawed actions, creating a character that is both relatable and tragic. Gabe’s decisions are often influenced by the immediate needs and pressures of his environment, reflecting the limited choices available to him. His struggles with alcohol, his role as a father, and his attempts to maintain a semblance of normalcy in the face of haunting past actions all contribute to a portrait of a man caught between the desire for redemption and the pull of his circumstances. Gabe’s moral ambiguity is highlighted by his participation in the elk hunt that sets the tragic events of the novel in motion, an act driven by youth and rebellion that later returns to haunt him.\nVictor Yellow Tail, on the other hand, serves as a foil to the main group of friends. As a tribal police officer, Victor represents an uncomfortable intersection of community and authority who is seen as someone who has ostensibly made ‘better’ choices than his peers. His role as a law enforcer is doubly contentious as he is seen not just as upholding the law, but specifically as enforcing “the white man’s law” on reservation land. This position places him in a delicate balance between his duties and his cultural identity, making him a figure of both respect and resentment within the community. Victor’s interactions with the community, and his ultimate fate in the story, serve to underline the themes of justice and retribution that are central to the novel. His character raises questions about the nature of authority and righteousness within a community still grappling with its history and the ongoing challenges of its present.\nBoth Gabe and Victor, in their respective roles, navigate the world with a complex mixture of heroism and villainy. Their stories highlight the inherent challenges in defining morality within a community where historical injustices and personal failures intertwine. Through these characters, Jones explores the idea that morality is not a fixed state but a fluid dynamic, influenced by past actions, personal motivations, and the broader social and cultural forces at play."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#cultural-and-tribal-identities",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#cultural-and-tribal-identities",
    "title": "Book Review: The Only Good Indians",
    "section": "2.3. Cultural and Tribal Identities",
    "text": "2.3. Cultural and Tribal Identities\nAt the heart of the novel, the tensions between Crow and Blackfeet cultures are embodied in the characters and their backgrounds, subtly informing their interactions and conflicts. Jones does not explicitly detail the historical conflicts between the Crow and Blackfeet in the novel, but the weight of history is palpable in the characters’ lives. Historically, the Crow and Blackfeet were often in conflict over territory and resources, a backdrop that enriches the narrative’s tension. This history adds layers of meaning to the characters’ struggles with belonging and loyalty, highlighting how historical inter-tribal dynamics continue to affect contemporary relationships and self-perception.\nMoreover, the novel touches on the internalized prejudices and stereotypes that can prevail within and between Native American communities. Through the characters’ interactions and internal monologues, Jones subtly addresses how these historical animosities can lead to internalized racism and self-hatred, complicating the characters’ abilities to form a cohesive identity. The struggle for identity and acceptance is portrayed not just as an internal battle but as a challenge shaped by the long shadows of inter-tribal histories.\nIn tying these personal and inter-tribal tensions to the broader themes of the novel, Jones skillfully uses the horror genre as a metaphorical landscape to discuss cultural disintegration and identity conflicts."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#symbolism-and-mirroring",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#symbolism-and-mirroring",
    "title": "Book Review: The Only Good Indians",
    "section": "2.4. Symbolism and Mirroring",
    "text": "2.4. Symbolism and Mirroring\nThe illegal elk hunt that the characters partake in their youth serves as a critical symbol in the novel. This act of killing a pregnant elk in a restricted area is not only a breach of legal and cultural norms but also a violation of sacred life, which mirrors the historical violence inflicted upon Native American communities. This act of violence against nature symbolically represents the broader violence against Native Americans—both are acts of invasion and disrespect against beings seen as lesser by an oppressive force. The elk, particularly because it was pregnant, symbolizes not just life but potential life, reflecting the deep cuts made into the fabric of Native communities, where not only lives were taken but potential futures were destroyed.\nThe mirroring effect is a powerful tool used by Jones to draw parallels between individual actions and historical events. By aligning the slaughter of the elk with the genocidal history faced by Native Americans, the novel invites readers to reflect on the cyclical nature of violence and the ways in which historical injustices continue to resonate in contemporary settings. The characters’ personal guilt and the supernatural retribution they face are not just about their actions but also about their roles within these larger historical cycles. They become stand-ins for broader cultural narratives, living out themes of retribution and guilt that have been part of their heritage."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#the-elk-headed-woman",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#the-elk-headed-woman",
    "title": "Book Review: The Only Good Indians",
    "section": "2.5. The Elk-Headed Woman",
    "text": "2.5. The Elk-Headed Woman\nAs a figure of horror, the Elk-Headed Woman is terrifying and relentless. Her appearances are marked by a chilling blend of the natural and the supernatural, as she combines elements of a vengeful spirit and a wronged animal. Her pursuit of the main characters, who were responsible for the death of her elk form and her unborn calf during an illegal hunt, is both brutal and ghostly, imbuing the story with a palpable sense of dread and inevitability. Her actions are gruesome and her presence suffuses the novel with a tension that is typical of horror but with an added layer of psychological depth. This depth comes from her motivations, which unfold as the story progresses, reshaping the reader’s understanding of her role in the narrative.\nThe revelation of the Elk-Headed Woman’s desire for her lost baby introduces profound themes of motherhood and loss, adding layers to her character that extend beyond mere vengeance. This aspect of her motivation reveals her actions not just as blind revenge but as a mother’s grief-stricken response to the violent loss of her child. In this light, her relentless pursuit of the men reflects the depth of her loss and the lengths to which she will go for what she sees as reconciliation or retribution. This revelation complicates the reader’s perception of her as merely a villain, inviting a more empathetic understanding of her fury and actions.\nMoreover, the Elk-Headed Woman’s presence and motivations are deeply intertwined with the characters’ feelings of guilt and the symbolic weight of their past actions. She embodies the consequences of their youthful indiscretions, transforming their guilt into a literal haunting that stalks them throughout their lives. This connection emphasizes the theme that the past is never truly past when its wounds remain unaddressed. The Elk-Headed Woman, therefore, stands as a constant reminder of the characters’ responsibility for their actions and the pain they have caused."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#the-final-girl",
    "href": "blog/Book-Reviews/posts/20240501-BookReview-TheOnlyGoodIndians/index.html#the-final-girl",
    "title": "Book Review: The Only Good Indians",
    "section": "2.6. The Final Girl",
    "text": "2.6. The Final Girl\nIn the novel, Denora is positioned as the “final girl.” Unlike the typical final girl trope where the character often begins as somewhat innocent or unassuming, Denora’s role is steeped in cultural identity and personal resilience from the outset. She is not only a survivor but also a representation of a new generation grappling with both the legacy of the past and the pressures of the present.\nDenora’s confrontation with the Elk Head Woman during the climactic scenes of the book redefines the trope. Here, the antagonist is not merely a slasher or a faceless monster but a manifestation of cultural trauma and historical violence. The Elk Head Woman is a symbol of the consequences that come from the characters’ disconnection from their traditions and their community.\nDenora’s showdown with this spirit is significant because it is not just about physical survival but about cultural survival and identity. Her role as the final girl underscores a thematic focus on healing and reconciliation. It highlights the possibility of overcoming generational traumas through understanding and facing the horrors of the past directly.\nThis confrontation also shifts the perspective on how strength and vulnerability are portrayed in horror narratives. Denora’s strength comes from her deep connection to her culture and her ability to face the truth of her community’s and her ancestors’ actions. This makes her survival not just a personal victory but a communal hope for renewal and change.\nBy placing Denora at the center of this final confrontation, Jones not only subverts the traditional final girl trope but enriches it, making it a vehicle for deeper commentary on resilience, identity, and redemption within Native American communities. Her survival is symbolic of the broader survival and adaptation of her culture, offering a powerful message about the endurance of heritage and the strength found in facing one’s history."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240401-BookReview-HowToWinFriendsAndInfluencePeople/index.html",
    "href": "blog/Book-Reviews/posts/20240401-BookReview-HowToWinFriendsAndInfluencePeople/index.html",
    "title": "Book Review: How to Win Friends & Influence People",
    "section": "",
    "text": "Rating: 3/10  Overview: Offers basic insights from 1936 with a tone of its era.\n\n\nIntroduction\nIn the landscape of personal development and interpersonal skills, few books have stood the test of time as prominently as Dale Carnegie’s “How to Win Friends and Influence People.” Despite its age, the core principles Carnegie presents remain remarkably relevant, addressing the timeless human desire for connection and understanding. However, it’s not without its controversies; some readers may find its techniques manipulative, and some of Carnegie’s idols (such as Robert E. Lee and John D. Rockefeller) and sexitst pro-capitalist ideals problematic.\nIn the following, I share my notes - a compilation of quotes and insights that I found particularly striking. These reflections are intended to capture the essence of Carnegie’s advice through my lens, underscoring its potential wisdom.\n\n\nPart One: Fundamental Techniques in Handling People\nThis section covers the basic principles of interacting with others, emphasizing the importance of avoiding criticism, condemnation, or complaints.\n\nBy criticizing, we do not make lasting changes and often incur resentment.\nDon’t criticize them, they are doing just as you would under similar circumstances.\nAny fool can criticize, complain, and condemn—and most fools do. But it takes character and self-control to be understanding and forgiving.\nTell me what makes you feel important and I’ll tell you the type of person you are.\n\n\n\nI learned thirty years ago that it is foolish to scold. I have enough trouble overcoming my own limitations without fretting over the fact that God has not seen fit to distribute evenly the gift of intelligence.\n\nJohn Wanamaker\n\n\n\n\n\nJudge not, that ye be not judeged. For with what judgment ye judge, ye shall be judged: and with what measure ye mete, it shall be measured to you again. And why beholdest thou the mote that is in thy brother’s eye, but considerest not the beam that is in thine own eye?\n\nMatthew 7:1-3\n\n\n\n\n\nI will speak ill of no one. I will speak all the good I know of everybody.\n\nBenjamin Franklin\n\n\n\n\n\nTo understand all is to forgive all.\n\nEvelyn Waugh\n\n\n\n\n\nI shall not pass this way again, so when I do it let me be kind.\n\nEtienne de Grellet\n\n\n\n\n\nPart Two: Six Ways to Make People Like You\nHere, Carnegie presents simple strategies to become more likable and build stronger, more positive relationships with others.\n\nBecome genuinely interested in other people.\n\nYou can make more friends in two months by becoming interested in other people that in 2 years by trying to get other people interested in you.\nIt is the individual who is not interested in their fellow man that has the greatest difficulties in life and provides the greatest injury to others. It’s from those individuals that all human failures spring.\nIf the author doesn’t like people, than people won’t like their stories.\nWe all like people who admire us.\n“I love my audience.”\n\n\n\nEvery man I meet is my superior in some way, and in that I learn from him.\n\nRalph Waldo Emerson\n\n\n\nSmile.\n\nEncouragement is more effective teaching device than punishment.\nStop talking about what you want, and see things from other peoples sides.\nAct and speak as if cheerfulness is already there.\nHappiness depends on inside conditions. It is what you think about it.\nTo think rightly is to create.\n\n\n\nThere is nothing good or bad, but thinking makes it so.\n\nShakespeare\n\n\n\n\n\nWe become like that on which our hearts are fixed. Carry your chin in and the crown of your head high. We are gods in the chrysalis.\n\nJeffrey J. Denning\n\n\n\nRemember that people like hearing their name.\nBe a good listener. Encourage others to talk about themselves.\nTalk in terms of the other person’s interests.\nMake the other person feel important and do it sincerely.\n\nOil the cogs with little things such as please, thank you, if you don’t mind, and so on.\nRecognize other people’s importance and recognize it sincerely.\nTalk to people about themselves and they will listen for hours.\n\n\n\nThe deepest urge in human nature is the desire to be important.\n\nJohn Dewey\n\n\n\n\n\nThe deepest principle in human nature is the craving to be appreciated.\n\nWilliam James\n\n\n\n\n\nDo unto others how you would have others do unto you.\n\nMathew 7:12\n\n\n\n\n\nBut man, proud man, Drest in a little brief authority, Most ignorant of what he’s most assur’d; His glassy essence, like an angry ape, Plays such fantastic tricks before high heaven, As make the angels weep.\n\nShakespeare\n\n\n\n\n\n\nPart Three: How to Win People to Your Way of Thinking\nThis part offers tactics for persuading others to your point of view without causing offense or arousing resentment.\n\nThe best way to win an argument is to avoid it.\n\nYou can’t win an argument.\nDistrust your first impression.\nWelcome the disagreement.\nWhen two partners always agree then one of them is not necessary.\nPerhaps there is something you haven’t thought about. Be thankful it’s been brought to your attention.\nWatch out for your first reaction. It may be you at your worst, not your best.\nYou can measure the size of a person by what makes them angry.\nBuild bridges of understanding, not barriers of misunderstanding.\nDwell on areas where you agree with others.\nPromise to think over their ideas and mean it. It’s easier to agree to think about their points.\nThank your opponents sincerely for their interests. Anyone who takes the time to disagree with you is interested in the same things you are.\nThink of your opponents as people who want to help you. Think of your opponents as your friends.\nWhen one person is yelling the other should be listening because when two people yell then no one is listening.\n\nQuestions to Ask Yourself\n\nCould my opponents be right or partially right?\nIs there truth or merit in their position or argument?\nIs my reaction one that will relieve the problem, or just relieve my frustration?\nWill my reaction draw my opponents further away or closer to me?\nWill my reaction demonstrate the estimation of good will people have of me?\nWill I win or lose? What price will I have to pay if I win?\nIf I am quiet about it will the disagreement blow over?\nIs this difficult situation an opportunity for me?\n\n\n\nIf you argue you will never have a person’s good will.\n\nBenjamin Franklin\n\n\n\n\n\nHere lies the body of William Jay, Who died maintaining his right of way— He was right, dead right, as he sped along, But he’s just as dead as if he were wrong.\n\nCarnege\n\n\n\n\n\nHatred is never ended by hatered, but by love.\n\nBuddah\n\n\n\n\n\nYield larger things to which you can show no more than equal right; and yield lesser ones, though clearly your own. Better give your path to a dog than be bitten by him in contesting for the right. Even killing the dog would not cure the bite.\n\nAbraham Lincoln\n\n\n\nShow respect for the other person’s opinions. Never say “You’re wrong”.\n\nIf you are going to prove anything, don’t make anybody know it.\nBe wiser than other people if you can; but do not tell them so.\nWhen you argue you are not persuading.\nRidicule and abuse never make someone agree with you.\nBenjamin Franklin never spoke in certainties.\nReply to your opposition stating that in some cases you could see their perspective being right. You could also see some cases in which that is not the case.\nConversations will go more pleasantly and a modest suggestion will be better received.\nYou will have less mortification when found to be in the wrong, and more people to agree with you when you are right.\nMake it a point not to disagree or tell the other person they are wrong.\nBeing diplomatic will help you gain your point.\n\n\n\nMen must be taught as if you taught them not, and thing proposed as things forgot.\n\nAlexander Pope\n\n\n\n\n\nYou cannot teach a man anything, only help him find it within himself.\n\nGalileo\n\n\n\n\n\nWe sometimes find ourselves changing our minds without any resistance or heavy emotion, but if we are told that we are wrong we resent the imputation and harden our hearts. We are incredibly heedless in the formation of our beliefs, but find ourselves filled with an illicit passion for them when anyone proposes to rob us of their companionship. It is obviously not the ideas themselves that are dear to us, but our self-esteem which is threatened.\n\nJames harvey Robinson, The Mind in the Making\n\n\n\n\n\nBen you are impossible. Your opinions have a slap in them for everyone who differs with you. They have become so offensive that nobody cares for them. Your friends find that they enjoy themselves better when you are not around. You know so much that no one can tell you anything. Indeed no one is going to try for the act and effort would only lead to discomfort and hard work. Therefore you are likely never to know anymore than you do now, which is very little.\n\nQuote from Benjamin Franklins Autobiography\n\n\n\n\n\nI judge people by their principles not by my own.\n\nMLK\n\n\n\n\n\nAgree with thyn adversary quickly.\n\nJesus\n\n\n\nIf you are wrong admit it quickly and emphatically.\n\nWhen you say the other person’s criticism of yourself first, then the criticism is coming from your mouth, and the blow will be much softer.\nAny fool can defend their mistakes, but it takes a large person to discuss failure.\nBy fighting you never get enough, but by yielding you get more than you expected.\n\nBegin in a friendly way.\n\nGentleness and friendliness are stronger than brute force.\n\n\n\nIf you come at me with your fists doubled, I think I can promise you that mine will double as fast as yours; but if you come to me and say, ‘Let us sit down and take counsel together, and, if we differ from each other, understand why it is that we differ, just what the points at issue are,’ we will presently find that we are not so far apart after all, that the points on which we differ are few and the points on which we agree are many, and that if we only have the patience and the candor and the desire to get together, we will get together.\n\nWoodrow Wilson\n\n\n\n\n\nA drop of honey catches more flies than a gallon of gall.\n\nAbraham Lincoln\n\n\n\nGet the other person saying “yes yes” immediately.\n\nBegin and keep on emphasizing the things on which you agree. You are both striving for the same end, but your only difference is of method not of purpose.\nA no response is a most difficult handicap to overcome. When you have said no, all your pride and personality demands that you remain consistent with yourself.\nIt takes more force to go in the opposite direction.\nWhen someone says no they are doing more than that. The body is doing more than that. When a person says yes, no physical or mental withdraw happens.\nA skillful speaker gets a yes response.\nThe more yeses we can receive, the more likely we are to succeed at capturing our proposal.\nThe Socratic method is based on getting a yes response.\nAsk gentle questions to get a yes response.\nHe who treads softly goes far.\n\nLet the other person do a great deal of the talking.\n\nLet the other person talk themselves out.\n\nLet the other person feel the idea is theirs.\nTry honestly to see things from the other person’s point of view.\n\nSuccess when dealing with people depends on a sympathetic grasp of the other person’s viewpoint.\nCooperativeness in conversation is achieved when you show that you consider the other person’s ideas and feelings as important as your own.\nThink always in terms of the other person’s point of view.\n\nBe sympathetic to the other person’s ideas and desires.\n\nI don’t blame you for feeling how you do, and if I were you I would feel as you do.\n\n\nThere but for the grace of god go I.\nProverb\n\nAppeal to the nobler motives.\n\nBasically there is a real reason why someone does something, and then the reason they tell people (the nobler reason). Appeal to these reasons.\nStory about a guy who didn’t want a specific picture of him in the paper, but wrote a letter saying that it was his mother who did not like this picture. Appealing to the nobler motive of respecting a mothers wishes.\n\nDramatize your ideas.\n\nShow your point, don’t just explain it.\n\nThrow down a challenge.\n\nMotivate with friendly competition.\n\n\n\n\nPart Four: Be a Leader: How to Change People Without Giving Offense or Arrousing Resentment\nThe final section focuses on leadership and how to encourage others to change their behavior or beliefs in a positive way.\n\nBeing a leader often involves changing people’s behaviors and attitudes.\n\n\nBegin with praise and honest appreciation.\nCall attention to people’s mistakes indirectly.\nTalk about your own mistakes before criticizing others.\nAsk questions instead of giving direct orders.\nLet the other person save face.\nPraise the slightest improvement, and praise every improvement. Be hearty in your approbation and lavish in your praise.\nGive the other person a fine reputation to live up to.\n\n\nAssume a virtue if you have it not.\n\nShakespeare\n\n\n\nUse encouragement. Make the fault seem easy to correct.\nMake the other person happy about doing the thing that you suggest.\n“Playbook”\n\nBe sincere, and do not promise anything.\nKnow what you want the other person to do.\nBe empathetic, and ask what the other person wants.\nWhat are the benefits the other person will get doing what you want?\nMatch those benefits to the things that they want.\nWhen you make your request, put it in a form that they personally will benefit from doing this.\n\n\n\n\nConclusion\nDale Carnegie’s “How to Win Friends and Influence People” provides a foundational perspective on the art of interaction and persuasion. While its age-old advice resonates with timeless truths about human nature, the modern reader must navigate its principles with a critical eye."
  },
  {
    "objectID": "links/github.html",
    "href": "links/github.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "Continue to ~ Github ~\nThanks for stopping by!\n\n\n\nNamba Park, Osaka, Japan"
  },
  {
    "objectID": "links/letterboxd.html",
    "href": "links/letterboxd.html",
    "title": "Randi Bolt",
    "section": "",
    "text": "Continue to ~ Letterboxd ~\nThanks for stopping by!\n\n\n\nNamba Park, Osaka, Japan"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html",
    "href": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html",
    "title": "Code to Content Lighting Talk 2",
    "section": "",
    "text": "In this post are the materials for my Cascadia R Conference lightning talk called: Code to Content."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-1-code-to-content",
    "href": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-1-code-to-content",
    "title": "Code to Content Lighting Talk 2",
    "section": "Slide 1: Code to Content",
    "text": "Slide 1: Code to Content\nHello, my name is Randi Bolt. Since rendering my first R markdown file to HTML in 2021, I have been enhancing my technical skills by creating web pages and blogs. Today in my talk, “Code to Content,” I will be discussing the benefits and process of data blogging."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-2-agenda",
    "href": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-2-agenda",
    "title": "Code to Content Lighting Talk 2",
    "section": "Slide 2: Agenda",
    "text": "Slide 2: Agenda\nToday I will be covering four main points:\n\nWhy you should start a blog, where I’ll share key reasons and benefits.\nHow to create a blog, where I’ll outline the high-level steps to get you started.\nWhat to consider, where I’ll provide important tips and advice.\nWhere to find more information, where I’ll give you valuable resources for further exploration.\n\nThe images I’ve included are examples of my past related projects, from R Markdown on GitHub to blogdown with Hugo templates, and now Quarto.\nLet’s get into it!"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-3-why",
    "href": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-3-why",
    "title": "Code to Content Lighting Talk 2",
    "section": "Slide 3: Why",
    "text": "Slide 3: Why\nTo begin, let’s discuss some of the reasons why someone might embark on their blogging journey. There is joy in expression and self discovery. This can help cultivate a richer process of understanding, confidence, and purpose. Own your narrative, and find yourself more empowered speaking to the work you have done and own. Continuous Growth and Learning, which is crucial in today’s job market. Reveal fresh perspectives, and find you’re able to solve problems more efficiently. Data advocacy and empowerment, which are crucial elements driving informed decision-making, fostering transparency, and allow more people the opportunity to leverage this knowledge and these tools. Lastly it is embedded in R culture, and the principles of collaborative learning.\nThese blog posts illustrate my journey, from humble drawings and cheat sheets to web scraping and package building. Blogging provides a low-stakes way to share work and remember key techniques.\nContinuing on,"
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-4-how",
    "href": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-4-how",
    "title": "Code to Content Lighting Talk 2",
    "section": "Slide 4: How",
    "text": "Slide 4: How\nWhen it comes to how to create a blog there are a variety of approaches. At a high level start by gathering the necessary knowledge, brainstorm your blog’s focus, and plan your content. Then set up your blog, connect it to a version control system like Git, and deploy it on a platform such as Netlify. From there configure your blog with a .yam file, personalize it with unique pages, and publish your first post. Then style the aesthetics of your blog with an .scss file, take ownership by securing a custom URL, and use Google Analytics to track and analyze user engagement.\nNow that you have a high level understanding of the process let discuss what to focus on and away from."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-5-what",
    "href": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-5-what",
    "title": "Code to Content Lighting Talk 2",
    "section": "Slide 5: What",
    "text": "Slide 5: What\nWhen it comes to creating a blog, it is not always easy to know what to prioritize. First and foremost, prioritize accessibility to ensure your content is available to everyone. Select the right tool for the job to suit your specific needs and skills. Avoid duplicating other profiles by creating unique, original content. Finally, always consider your audience to make sure your blog resonates with them and meets their needs.\nThe images on this slide provide examples of these priorities in action. They illustrate how I’ve used different tools, ensured accessibility, organized content for my audience, and maintained originality without duplicating other profiles.\nNow that we’ve covered what to focus on, let’s look at some resources to help you get started."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-6-where-12",
    "href": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-6-where-12",
    "title": "Code to Content Lighting Talk 2",
    "section": "Slide 6: Where (1/2)",
    "text": "Slide 6: Where (1/2)\nTo start, read the Quarto documents for starting a blog. This is a one page scrollable document that should be a fairly quick read. From there I’d check out “Quarto/RMarkdown - What’s Different” by Ted Laderas to not only understand the difference between these two tools, but also learn a lot of the extra capabilities of Quarto. When it comes to creating your first Quarto blog, Bea Milz’s post, “Creating a blog with Quarto in 10 steps,’ is an essential guide to the process."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-7-where-22",
    "href": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-7-where-22",
    "title": "Code to Content Lighting Talk 2",
    "section": "Slide 7: Where (2/2)",
    "text": "Slide 7: Where (2/2)\nOnce you’ve started wrapping your head around blogging with quarto I’d check out the video “Create Your Data Science Portfolio with Quarto” by Deepsha Menghani. This is a fun and hands-on tutorial where Deepsha guides you through creating a data science portfolio for the TV character Wednesday Adams. Lastly, I’d check out Albert Rapp’s video “Style your Quarto Blog without Knowing a lot of HTML/CSS”. This is an uncomplicated tutorial that will empower you with the knowledge to add extra polish to the look of your blog."
  },
  {
    "objectID": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-8-thank-you",
    "href": "blog/Technical-Blog/posts/20240612-Original-CodetoContentSlides2/index.html#slide-8-thank-you",
    "title": "Code to Content Lighting Talk 2",
    "section": "Slide 8: Thank You!",
    "text": "Slide 8: Thank You!\nAnd there you have it! I have briefly talked you through the why, how, what, and where of creating a blog. Thank you for your time, and I look forward to seeing more R users transforming their code into content\nThank you!"
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240601-BookReview-AnxiousPeople/index.html",
    "href": "blog/Book-Reviews/posts/20240601-BookReview-AnxiousPeople/index.html",
    "title": "Book Review: Anxious People",
    "section": "",
    "text": "Rating: 2/10  Overview: A group of strangers connected by a heist navigate their anxieties and personal struggles in a tedious narrative with a few redeeming moments."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240601-BookReview-AnxiousPeople/index.html#structure",
    "href": "blog/Book-Reviews/posts/20240601-BookReview-AnxiousPeople/index.html#structure",
    "title": "Book Review: Anxious People",
    "section": "Structure",
    "text": "Structure\nI found the structure of the book tedious due to its numerous short chapters that jump between characters and time periods. While this doesn’t make the story difficult to follow, it does make it hard to care about the characters. Most of the characters are written to be annoying, and although the intent is to eventually make the reader empathize with them, this payoff didn’t work for me. We learn why people are the way they are, but it doesn’t feel earned.\nThe constant shifts in perspective and time can disrupt the narrative flow, making it challenging to become fully invested in any single storyline. This fragmented approach might have been intended to mirror the characters’ anxieties and chaotic lives, but for me, it resulted in a lack of emotional connection and depth. The interviews, meant to provide insight into the characters’ backgrounds and motivations, often felt repetitive and dragged on, further diminishing my engagement with the story."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240601-BookReview-AnxiousPeople/index.html#class-divide",
    "href": "blog/Book-Reviews/posts/20240601-BookReview-AnxiousPeople/index.html#class-divide",
    "title": "Book Review: Anxious People",
    "section": "Class Divide",
    "text": "Class Divide\nA significant portion of this novel takes place during an apartment viewing, featuring a variety of characters from different financial backgrounds. For instance, there’s a bank robber struggling to make ends meet and a banker who attends apartment viewings merely to judge the apartments and the people viewing them. This setup effectively highlights the contrasts between the lives of those with financial stability and those facing financial struggles.\nThe novel underscores the idea that the poor get poorer and the rich get richer, emphasizing that the real class divide is between those who can borrow money and those who can’t. This economic disparity profoundly affects the characters’ opportunities, relationships, and mental health.\nAdditionally, the novel critiques the notion that homes should be viewed as investments rather than places to live. It delves into the emotional and psychological toll of treating housing as a commodity, where the value of a home is measured in financial terms rather than as a sanctuary or a space for personal growth. This perspective shifts focus from creating a nurturing living environment to maximizing profit, often leading to a sense of instability and insecurity for those who are unable to keep up with the market’s demands. The story suggests that when homes are seen merely as financial assets, it exacerbates economic divides and diminishes the quality of life for many, ultimately questioning the ethics and sustainability of such an approach to housing."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240601-BookReview-AnxiousPeople/index.html#second-chances",
    "href": "blog/Book-Reviews/posts/20240601-BookReview-AnxiousPeople/index.html#second-chances",
    "title": "Book Review: Anxious People",
    "section": "Second Chances",
    "text": "Second Chances\nDespite facing significant personal failures, tragedies, and societal stigmas, these characters find redemption through the compassion and solidarity they offer each other. The story emphasizes how their lives, once intertwined, create a network of empathy and understanding that fosters personal growth and healing. Through their shared experiences, the characters learn that even the most challenging circumstances can lead to new beginnings and the chance for a better future."
  },
  {
    "objectID": "blog/Book-Reviews/posts/20240601-BookReview-AnxiousPeople/index.html#additional-themes",
    "href": "blog/Book-Reviews/posts/20240601-BookReview-AnxiousPeople/index.html#additional-themes",
    "title": "Book Review: Anxious People",
    "section": "Additional Themes",
    "text": "Additional Themes\n\nSuicide\nStockholmers\nParent-Child Relationships"
  }
]